Optimization
==============

Univariate function optimization
-------------------------------------------------------------------------------

Example function to be maximized
++++++++++++++++++++++++++++++++++++++++


.. code-block:: python

    import numpy as np
    import matplotlib.pyplot as plt
    import math as m
    from scipy import stats as st
    from scipy import optimize
    #
    import time  # Imports system time module to time your script
    
    plt.close('all')  # close all open figures
    



Here we want to optimize a univariate function: ``f1``


.. code-block:: python

    def f1simple(x):
        # gamma(2,3) density
        if (x < 0):
            return (0)
        if (x == 0):
            return (np.nan)
        y = np.exp(-2*x)
        return (4 * x**2 * y)
    



Next we define the same function but return :math:`f(x)`, :math:`f'(x)`, and
:math:`f''(x)`.


.. code-block:: python

    def f1(x):
        # gamma(2,3) density
        if (x < 0):
            return np.array([0, 0, 0])
        if (x == 0):
            return np.array([0, 0, np.nan])
        y = np.exp(-2.0*x)
        return np.array([4.0 * x**2.0 * y, 8.0 * x*(1.0-x)*y, 8.0*(1.0 -
    2.0 * x**2.0)*y])
    



Some algorithms that we'll encounter later will minimize a function. So if we
want to maximize our function we have to define it as a negate function, that
is: :math:`g(x) = -f(x)` then :math:`min(g(x))` is the same as
:math:`max(f(x))`.


.. code-block:: python

    def f1simpleNeg(x):
        # gamma(2,3) density
        if (x < 0):
            return (0)
        if (x == 0):
            return (np.nan)
        y = np.exp(-2*x)
        return (-(4 * x**2 * y))
    



Plotting the function is always a good idea!


.. code-block:: python

    xmin = 0.0
    xmax = 6.0
    xv = np.arange(xmin, xmax, (xmax - xmin)/200.0)
    fx = np.zeros(len(xv),float) # define column vector
    for i in range(len(xv)):
        fx[i] = f1(xv[i])[0]
    
    print("fx= ", fx)
    
    fig, ax = plt.subplots()
    ax.plot(xv, fx)
    ax.plot(xv, np.zeros(len(xv)))
    plt.show()
    

::

    fx=  [ 0.          0.00339035  0.01277165  0.02706275  0.04530976
    0.06667364
      0.09041885  0.11590306  0.14256769  0.16992939  0.19757219
    0.22514044
      0.25233237  0.27889422  0.30461495  0.32932142  0.35287408
    0.37516298
      0.39610422  0.4156368   0.43371967  0.45032913  0.46545655
    0.4791062
      0.4912934   0.50204286  0.51138714  0.51936535  0.52602191
    0.53140556
      0.5355684   0.53856503  0.54045191  0.54128664  0.54112748
    0.54003285
      0.53806088  0.53526913  0.53171422  0.52745162  0.52253541
    0.51701815
      0.51095069  0.50438208  0.4973595   0.48992819  0.48213137
    0.4740103
      0.46560418  0.45695021  0.44808362  0.43903762  0.42984354
    0.42053078
      0.41112691  0.40165769  0.39214718  0.38261772  0.37309007
    0.36358341
      0.35411544  0.34470245  0.33535934  0.32609974  0.31693605
    0.30787947
      0.29894013  0.29012709  0.28144844  0.27291131  0.26452198
    0.2562859
      0.24820775  0.24029149  0.23254041  0.22495718  0.21754387
    0.21030202
      0.20323267  0.19633641  0.18961337  0.18306333  0.17668568
    0.1704795
      0.16444357  0.15857638  0.1528762   0.14734107  0.14196882
    0.13675713
      0.1317035   0.1268053   0.12205979  0.11746411  0.11301532
    0.10871042
      0.10454632  0.10051991  0.09662802  0.09286748  0.08923508
    0.0857276
      0.08234185  0.07907461  0.07592269  0.07288294  0.06995219
    0.06712735
      0.06440532  0.06178308  0.05925763  0.05682602  0.05448534
    0.05223274
      0.05006543  0.04798066  0.04597576  0.04404808  0.04219506
    0.04041418
      0.03870301  0.03705914  0.03548024  0.03396405  0.03250835  0.031111
      0.0297699   0.02848302  0.02724839  0.02606409  0.02492828
    0.02383913
      0.02279492  0.02179394  0.02083457  0.0199152   0.01903432
    0.01819043
      0.0173821   0.01660795  0.01586664  0.01515687  0.01447739  0.013827
      0.01320454  0.01260888  0.01203895  0.0114937   0.01097213
    0.01047327
      0.00999619  0.00954     0.00910383  0.00868684  0.00828825
    0.00790728
      0.0075432   0.00719528  0.00686286  0.00654527  0.00624188
    0.00595209
      0.00567532  0.005411    0.00515861  0.00491762  0.00468755
    0.00446791
      0.00425827  0.00405817  0.0038672   0.00368496  0.00351108
    0.00334517
      0.00318689  0.00303589  0.00289187  0.00275449  0.00262348
    0.00249854
      0.0023794   0.0022658   0.00215749  0.00205424  0.00195581
    0.00186199
      0.00177256  0.00168734  0.00160611  0.00152872  0.00145497
    0.0013847
      0.00131775  0.00125397  0.00119321  0.00113534  0.00108022
    0.00102772
      0.00097772  0.00093011]
    
    

.. image:: _static/Slides_Optimization_figure5_1.*
   :width: 15 cm



Optimization methods
-------------------------------------------------------------------------------

Newton's method
+++++++++++++++++++++++++

In order to implement the Newton method we basically look for the root of a
first derivative so that :math:`f'(x) = 0`. We then use the root finding
algorithm from the previous section to find this point, or: :math:`x(n+1) =
x(n) - \frac{f'(x(x))}{f''(x(n))}`


.. code-block:: python

    def newton(f3, x0, tol = 1e-9, nmax = 100):
        # Newton's method for optimization, starting at x0
        # f3 is a function that given x returns the vector
        # (f(x), f'(x), f''(x)), for some f
        x = x0
        f3x = f3(x)
        n = 0
        while ((abs(f3x[1]) > tol) and (n < nmax)):
            x = x - f3x[1]/f3x[2]
            f3x = f3(x)
            n = n + 1
        if (n == nmax):
            print("newton failed to converge")
        else:
            return(x)
    



Golden section method
+++++++++++++++++++++++++++++++

The golden-section method works in one dimension only, but does not need the derivatives of the function.
This method is very similar to the bisection method (root bracketing) from the previous section.
The algorithm proceeds in the following way:
Start with :math:`x_l<x_m<x_r` such that :math:`f(x_l) \le (x_m)` and :math:`f(x_r) \le f(x_m)` and :math:`\rho = \frac{1+\sqrt{5}}{2}`.

1. if :math:`x_r - x_l \le \epsilon` then stop
2. if :math:`x_r-x_m > x_m-x_l` then do :math:`(a)` otherwise do :math:`(b)`

  * (a) let :math:`y=x_m+(x_r-x_m)/(1+\rho)` if :math:`f(y) \ge  f(x_m)` then put :math:`x_l=x_m` and :math:`x_m=y` otherwise put :math:`x_r=y`
  * (b) let :math:`y=x_m+(x_m-x_l)/(1+\rho)` if :math:`f(y) \ge  f(x_m)` then put :math:`x_r=x_m` and :math:`x_m=y` otherwise put :math:`x_l=y`

3. go back to step 1


.. code-block:: python

    def gsection(ftn, xl, xr, xm, tol = 1e-9):
        # applies the golden-section algorithm to maximise ftn
        # we assume that ftn is a function of a single variable
        # and that x.l < x.m < x.r and ftn(x.l), ftn(x.r) <= ftn(x.m)
        #
        # the algorithm iteratively refines x.l, x.r, and x.m and
    terminates
        # when x.r - x.l <= tol, then returns x.m
        # golden ratio plus one
        gr1 = 1 + (1 + np.sqrt(5))/2
        #
        # successively refine x.l, x.r, and x.m
        fl = ftn(xl)
        fr = ftn(xr)
        fm = ftn(xm)
        while ((xr - xl) > tol):
            if ((xr - xm) > (xm - xl)):
                y = xm + (xr - xm)/gr1
                fy = ftn(y)
                if (fy >= fm):
                    xl = xm
                    fl = fm
                    xm = y
                    fm = fy
                else:
                    xr = y
                    fr = fy
            else:
                y = xm - (xm - xl)/gr1
                fy = ftn(y)
                if (fy >= fm):
                    xr = xm
                    fr = fm
                    xm = y
                    fm = fy
                else:
                    xl = y
                    fl = fy
        return(xm)
    



Built in 'optimize.fmin' function
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The function 'optimize.fmin' is in scipy.optimize as optimize

Maximize function: f1
++++++++++++++++++++++++++++

Maximizing using the Newton method
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. code-block:: python

    print(" -----------------------------------")
    print(" Newton results ")
    print(" -----------------------------------")
    print(newton(f1, 0.25))
    print(newton(f1, 0.5))
    print(newton(f1, 0.75))
    print(newton(f1, 1.75))
    

::

     -----------------------------------
     Newton results
     -----------------------------------
    1.97865578347e-12
    0.0
    1.00000000001
    1.0
    
    



Maximizing using the Secant method
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


.. code-block:: python

    print(" -----------------------------------")
    print(" Golden section results ")
    print(" -----------------------------------")
    print(gsection(f1simple, 0.1, 0.25, 1.3))
    print(gsection(f1simple, 0.25, 0.5, 1.7))
    print(gsection(f1simple, 0.6, 0.75, 1.8))
    print(gsection(f1simple, 0.0, 2.75, 5.0))
    

::

     -----------------------------------
     Golden section results
     -----------------------------------
    0.99999999879
    0.999999992411
    0.999999997487
    0.999999999283
    
    



Maximizing using the built in optimize function
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. code-block:: python

    print(" -----------------------------------")
    print(" optimize.fmin ")
    print(" -----------------------------------")
    print(optimize.fmin(f1simpleNeg, 0.25))
    print(optimize.fmin(f1simpleNeg, 0.5))
    print(optimize.fmin(f1simpleNeg, 0.75))
    print(optimize.fmin(f1simpleNeg, 1.75))
    

::

     -----------------------------------
     optimize.fmin
     -----------------------------------
    Optimization terminated successfully.
             Current function value: -0.541341
             Iterations: 18
             Function evaluations: 36
    [ 1.]
    Optimization terminated successfully.
             Current function value: -0.541341
             Iterations: 16
             Function evaluations: 32
    [ 1.]
    Optimization terminated successfully.
             Current function value: -0.541341
             Iterations: 14
             Function evaluations: 28
    [ 0.99997559]
    Optimization terminated successfully.
             Current function value: -0.541341
             Iterations: 16
             Function evaluations: 32
    [ 1.00001221]
    
    



Multivariate optimization
-------------------------------------------------------------------------------

Define multivariate (i.e. bivariate) example functions
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Here we want to optimize the following functions: f3, f4

Function f3
~~~~~~~~~~~~~~~~~~~~~~


.. code-block:: python

    def f3simple(x):
        a = x[0]**2/2.0 - x[1]**2/4.0
        b = 2*x[0] - np.exp(x[1])
        f = np.sin(a)*np.cos(b)
        return(f)
    



Its negative version:


.. code-block:: python

    def f3simpleNeg(x):
        a = x[0]**2/2.0 - x[1]**2/4.0
        b = 2*x[0] - np.exp(x[1])
        f = -np.sin(a)*np.cos(b)
        return(f)
    



And the version that returns :math:`f(x)`, :math:`f'(x)` (i.e. the gradient),
and :math:`f''(x)` (i.e. the Hessian):


.. code-block:: python

    def f3(x):
        a = x[0]**2/2.0 - x[1]**2/4.0
        b = 2*x[0] - np.exp(x[1])
        f = np.sin(a)*np.cos(b)
        f1 = np.cos(a)*np.cos(b)*x[0] - np.sin(a)*np.sin(b)*2
        f2 = -np.cos(a)*np.cos(b)*x[1]/2 +
    np.sin(a)*np.sin(b)*np.exp(x[1])
        f11 = -np.sin(a)*np.cos(b)*(4 + x[0]**2) + np.cos(a)*np.cos(b) \
            - np.cos(a)*np.sin(b)*4*x[0]
        f12 = np.sin(a)*np.cos(b)*(x[0]*x[1]/2.0 + 2*np.exp(x[1])) \
            + np.cos(a)*np.sin(b)*(x[0]*np.exp(x[1]) + x[1])
        f22 = -np.sin(a)*np.cos(b)*(x[1]**2/4.0 + np.exp(2*x[1])) \
            - np.cos(a)*np.cos(b)/2.0 -
    np.cos(a)*np.sin(b)*x[1]*np.exp(x[1]) \
            + np.sin(a)*np.sin(b)*np.exp(x[1])
        # Function f3 returns: f(x), f'(x), and f''(x)
        return (f, np.array([f1, f2]), np.array([[f11, f12], [f12, f22]]))
    



Plot function f3:


.. code-block:: python

    from mpl_toolkits.mplot3d import Axes3D
    
    fig = plt.figure(figsize=(14, 16))
    ax = plt.gca(projection='3d')
    
    X = np.arange(-3, 3, .1)
    Y = np.arange(-3, 3, .1)
    X, Y = np.meshgrid(X, Y)
    
    Z = np.zeros((len(X),len(Y)),float)
    for i in range(len(X)):
        for j in range(len(Y)):
            Z[i][j] = f3simple([X[i][j],Y[i][j]])
    
    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.jet,
    \
           linewidth=0, antialiased=False)
    plt.show()
    

.. image:: _static/Slides_Optimization_Fig_Surface_1.*
   :width: 12 cm




Function f4
~~~~~~~~~~~~~~~~~~~~~~

This is the same function, so we won't repeat it here.


Multivariate optimization methods
++++++++++++++++++++++++++++++++++++++++

2.2.1 Newton
~~~~~~~~~~~~~~~~


.. code-block:: python

    def newtonMult(f3, x0, tol = 1e-9, nmax = 100):
        # Newton's method for optimisation, starting at x0
        # f3 is a function that given x returns the list
        # {f(x), grad f(x), Hessian f(x)}, for some f
        x = x0
        f3x = f3(x)
        n = 0
        while ((max(abs(f3x[1])) > tol) and (n < nmax)):
            x = x - np.linalg.solve(f3x[2], f3x[1])
            f3x = f3(x)
            n = n + 1
        if (n == nmax):
            print("newton failed to converge")
        else:
            return(x)
    



Built in 'fmin_tnc' in scipy.optimize.tnc
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Maximize multivariate function
+++++++++++++++++++++++++++++++++++++

We use various starting values to see whether we can find more than one optimum.


.. code-block:: python

    for x0 in np.arange(1.4, 1.6, 0.1):
        for y0 in np.arange(0.4, 0.7, 0.1):
            # This algorithm requires f(x), f'(x), and f''(x)
            print("Newton: f3  " + str([x0,y0]) + ' --> ' +
    str(newtonMult(f3, \
                np. array([x0,y0]))))
            print("optimize.fmin: f3 " + str([x0,y0]) + ' --> ' \
                + str(optimize.fmin(f3simpleNeg, np.array([x0,y0]))))
            print(" ----------------------------------------- ")
    

::

    Newton: f3  [1.3999999999999999, 0.40000000000000002] --> [ 0.04074437
    -2.50729047]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 47
             Function evaluations: 89
    optimize.fmin: f3 [1.3999999999999999, 0.40000000000000002] --> [
    2.0307334   1.40155445]
     -----------------------------------------
    Newton: f3  [1.3999999999999999, 0.5] --> [ 0.11797341  3.34466147]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 50
             Function evaluations: 93
    optimize.fmin: f3 [1.3999999999999999, 0.5] --> [ 2.03072555
    1.40154756]
     -----------------------------------------
    Newton: f3  [1.3999999999999999, 0.59999999999999998] --> [-1.5531627
    6.0200129]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 43
             Function evaluations: 82
    optimize.fmin: f3 [1.3999999999999999, 0.59999999999999998] --> [
    2.03068816  1.40151998]
     -----------------------------------------
    Newton: f3  [1.5, 0.40000000000000002] --> [ 2.83714224  5.35398196]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 48
             Function evaluations: 90
    optimize.fmin: f3 [1.5, 0.40000000000000002] --> [ 2.03067611
    1.40149298]
     -----------------------------------------
    Newton: f3  [1.5, 0.5] --> [ 0.04074437 -2.50729047]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 42
             Function evaluations: 82
    optimize.fmin: f3 [1.5, 0.5] --> [ 2.03071509  1.40155165]
     -----------------------------------------
    Newton: f3  [1.5, 0.59999999999999998] --> [  9.89908350e-10
    1.36639196e-09]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 43
             Function evaluations: 82
    optimize.fmin: f3 [1.5, 0.59999999999999998] --> [ 2.0307244
    1.40153761]
     -----------------------------------------
    Newton: f3  [1.6000000000000001, 0.40000000000000002] --> [-0.55841026
    -0.78971136]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 47
             Function evaluations: 88
    optimize.fmin: f3 [1.6000000000000001, 0.40000000000000002] --> [
    2.0307159   1.40150964]
     -----------------------------------------
    Newton: f3  [1.6000000000000001, 0.5] --> [-0.29022131 -0.23047994]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 44
             Function evaluations: 80
    optimize.fmin: f3 [1.6000000000000001, 0.5] --> [ 2.03074135
    1.40151521]
     -----------------------------------------
    Newton: f3  [1.6000000000000001, 0.59999999999999998] --> [-1.55294692
    -3.33263763]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 42
             Function evaluations: 80
    optimize.fmin: f3 [1.6000000000000001, 0.59999999999999998] --> [
    2.03069759  1.40155333]
     -----------------------------------------
    
    



Homework
-------------------------------------------------------------------------------

:doc:`./Lecture_Optimization/Homework/Homework_Optimization`

