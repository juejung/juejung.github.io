<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Tips for Successful Projects</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="pandoc.css" />
</head>
<body>
<header id="title-block-header">
<h1 class="title">Tips for Successful Projects</h1>
</header>
<p>Undergraduate research projects typically fall within three categories:</p>
<blockquote>
<ol type="1">
<li>Data analysis of existing survey data</li>
<li>Web scraping with subsequent data analysis</li>
<li>Simulation models that require optimization</li>
</ol>
</blockquote>
<p>I will make some suggestions and provide links for each one of the project types next.</p>
<p>When you work towards your first presentation, I strongly suggest you use and economics research relevant search engine like: <a href="https://econpapers.repec.org/scripts/search.pf?ft=">EconPapers</a> in order to find relevant literature.</p>
<h1 id="data-analysis-projects-and-machine-learning-projects">1. Data Analysis Projects (and Machine Learning Projects)</h1>
<p>Since you are working with data that is already available in some form, your focus will be on data cleaning and the subsequent econometric/statistical analysis. I expect you to become very familiar with the <a href="http://pandas.pydata.org/">Pandas</a> library as it is the core library for doing data analysis in Python. For graphs I suggest you also check out <a href="http://ggplot.yhathq.com/">ggplot</a> or the <a href="http://bokeh.pydata.org/en/latest/">Bokeh</a> library which is newer and allows for somewhat interactive graphs that could be embedded into websites. You may also want to check out the <a href="http://pydata.org/">pydata</a> website.</p>
<p>I expect you to make professional looking summary tables and graphs of your data for starters. This should then be followed by regression analysis (OLS or related models) similar to what you have been introduced in Econ 205 and Econ 306.</p>
<p>The structure of your paper (about 20 pages) will roughly follow this outline:</p>
<blockquote>
<ol type="1">
<li>Introduction (with literature review)</li>
<li>Model (Econometric model, e.g., <span class="math inline"><em>y</em> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>X</em> + <em>ϵ</em></span>)</li>
<li>Data (Description of data survey and summary stats)</li>
<li>Results (Regression analysis etc.)</li>
<li>Conclusion</li>
</ol>
<ul>
<li>References</li>
<li>Appendix: Tables</li>
<li>Appendix: Figures</li>
</ul>
</blockquote>
<p><strong>Developed Expertise:</strong> General data analysis, data wrangling, advanced graphing libraries, econometric modeling</p>
<h1 id="web-scraping-projects">2. Web Scraping Projects</h1>
<p>If you work on a web scraping project you need to familiarize yourself with one of the many scraping libraries out there. One of the more basic ones is <a href="https://pypi.python.org/pypi/beautifulsoup4/">Beautiful Soup</a>. We go over at least one simple example in class where we scrape data from <a href="https://www.youtube.com/">YouTube</a>.</p>
<p>A more powerful web scraping framework is <a href="http://scrapy.org/">scrapy</a>. According to <a href="http://stackoverflow.com/questions/19687421/difference-between-beautifulsoup-and-scrapy-crawler">Stack Overflow</a> the difference between the two is as follows:</p>
<pre><code>Scrapy is a Web-spider or web scraper framework. You give Scrapy a root URL
to start crawling, then you can specify constraints on how many number of
URLs you want to crawl and fetch,etc., It is a complete framework for
Web-scrapping or crawling. [...]

BeautifulSoup is a parsing library which also does pretty job of fetching
contents from URL and allows you to parse certain parts of them without any
hassle. It only fetches the contents of the URL that you give and stops. It
does not crawl unless you manually put it inside a infinite loop with
certain criteria.

In simple words, with Beautiful Soup you can build something similar to
Scrapy.  Beautiful Soup is a library while Scrapy is a complete framework.</code></pre>
<p>In order to scrape data successfully you also need to learn/understand the basics of the hypertext markup language or <strong>html</strong>. There are many short introductions available on the net, so you need to work through some of those <a href="http://www.google.com/search?q=intro+to+html">intros to html</a> in order to get up to speed.</p>
<p>Your expertise will therefore be developed in text processing using lists, string manipulation, and scraping frameworks. If you feel very brave you may also want to check out the basics of <a href="https://docs.python.org/3/howto/regex.html">regular expressions</a> which is a pattern matching concept that will allow you to pretty much grab anything from a website.</p>
<p>Since you will spend a considerable amount of time trying to acquire the data, I will be less demanding when it comes to the data analysis part of your project. Still, the library that you probably want to use for the data analysis part is again <a href="http://pandas.pydata.org/">Pandas</a> for organizing and cleaning the data and possibly <a href="http://matplotlib.org/">matplotlib</a> for making graphs.</p>
<p>The structure of your paper (about 20 pages) will roughly follow this outline:</p>
<blockquote>
<ol type="1">
<li>Introduction (with literature review)</li>
<li>Data (Description of source and how acquired)</li>
<li>Results (Summary statistics, graphs, etc.)</li>
<li>(Optional: Regression analysis)</li>
<li>Conclusion</li>
</ol>
<ul>
<li>References</li>
<li>Appendix: Tables</li>
<li>Appendix: Figures</li>
</ul>
</blockquote>
<p><strong>Developed Expertise:</strong> Web scraping basics, basics in data wrangling, text parsing and string methods, basics in html and regular expressions (pattern matching)</p>
<h1 id="simulation-projects">3. Simulation Projects</h1>
<p>If you are working on a simulation project then you have to familiarize yourself with <a href="http://www.numpy.org/">numpy</a> for basic numerical coding objects like arrays, <a href="http://docs.scipy.org/doc/">scipy</a> for optimization libraries and <a href="http://matplotlib.org/">matplotlib</a> for graphing your optimization or simulation results.</p>
<p>You can read ahead in the lecture notes to see how a simple 2-period optimization problem is set up in Python. You will also want to read up on how to define <a href="http://www.google.com/search?q=functions+in+python">functions in Python</a>.</p>
<p>For a more general simulation approach you may also want to dig into <a href="http://www.google.com/search?q=python+object+oriented+programming">object-oriented-programming (OOP)</a> a slightly different programming paradigm that allows you to keep data objects and functional methods that can be applied on them neatly together. Check out the lecture on OOP as well to get a rough idea.</p>
<p>The structure of your paper (about 20 pages) will roughly follow this outline:</p>
<blockquote>
<ol type="1">
<li>Introduction (with literature review)</li>
<li>Model</li>
<li>Equilibrium</li>
<li>Results (Simulation of policy experiments, etc.)</li>
<li>Conclusion</li>
</ol>
<ul>
<li>References</li>
<li>Appendix: Tables</li>
<li>Appendix: Figures</li>
</ul>
</blockquote>
<p><strong>Developed Expertise:</strong> Optimization methods, functional programming, object oriented programming, economic modeling</p>
</body>
</html>
