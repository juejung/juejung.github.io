
# A Import data and standard statistics functionality in R


# 1 Read in small data set from a comma separated (.csv) file

We use the ```read.csv``` command to read in the data file **Lecture_4_excel_a.csv**. This command is part of the library ```foreign```. So we have to load this library first.

```{r R1}
rm(list=ls())       # Remove almost everything in the memory
library(foreign)    # Load foreign library
setwd("C:/Dropbox/Towson/Teaching/3_ComputationalEconomics/Lectures/Lecture4/Slides/")
#
mydata   = read.csv("Lecture_4_Excel_a.csv", header=T)  # Data has headers, so we set this to 'true'
View(mydata)        # Let's look at our data set
```

The data is currently saved in a so called **data frame**. This is a very convenient R specific concept. Our data consists of 3 variables total: ```Area``` and ```Frequency```.
We can access each one of these variables by adding the name of the data frame separated by the dollar sign. So if you'd like to manipulate variable Frequency you could do so by writing: ```mydata$Frequency```. Try

```{r R2}
sum(mydata$Frequency)
```
We next generate a new variable called ```X``` that contains the relative frequencies.

```{r R3}
mydata$X = mydata$Frequency/sum(mydata$Frequency)       # Generate a new variable in mydata call
View(mydata)
```

# 2 Making simple graphs from our data

## 2.1 Bar chart

We first make a bar chart of the absolute frequencies.

```{r R4}
barplot(mydata$Frequency, main = "Barplot", xlab = "Categories", 
        ylab = "Absolute frequencies" , ylim = c(0, 100), names.arg = mydata$Area)
```
This simple command plots the barchart into a window. As you can see we are able to add titles etc. Next we want to save the graph as a **.pdf** file so that we can copy it into our reports. We save it into a subfolder to the current directly called ```Graphs```. Don't forget to first make this subfolder, otherwise ```R``` will throw an error when it can't find the saving location.

```{R5}
# write this graps into a .pdf file and save it in a subfolder
   pdf(file="./R/Graphs/Fig1_R.pdf")
   barplot(mydata$Frequency, main = "Barplot", xlab = "Categories", 
        ylab = "Absolute frequencies" , ylim = c(0, 100), names.arg = mydata$Area)
   dev.off()
```
## 2.2 Pie chart
We next make a pie chart using the relative frequencies stored in variable ```X```.

```{r R6}
lbls = paste(mydata$Area, round(mydata$X,4)*100)  # add percent to labels 
lbls = paste(lbls,"%",sep="")                     # add % to labels
pie(mydata$X, main = "Pie chart", labels = lbls)
```
## 2.3 Histogram
Next we use a new data file called **Lecture_4_Excel_b.csv**. This data file contains data on height age and other variables. We first make a histogram of the continuous variable Height.

```{r R7}
setwd("C:/Dropbox/Towson/Teaching/3_ComputationalEconomics/Lectures/Lecture4")
mydata <- read.csv("Lecture_4_Excel_b.csv", header=T)
View(mydata)
hist(mydata$Height,main="Histogram of Height")
```

## 2.4 Boxplots
A boxplot of height is made as follows:

```{r R8}
boxplot(mydata$Height,ylab="Height")
```
-----------------------------------------------------------
# 3. Summary statistics

We next go through some basic summary statistics.

## 3.1 Measures of central tendency

We first calculate the mean, median and mode.

```{r R9}
n = length(mydata$Height)
sum(mydata$Height)/n           # average
# or simply
mean(mydata$Height)

# Median
median(mydata$Height)

# Mode (value with highest frequency)
mode(mydata$Height)
```

We can also just summarize a variable in our data frame with the ```summary``` command.

```{r R10}
summary(mydata$Height)
```

## 3.2 Measures of dispersion

We now calculate the range, variance and standard deviations. Remember that for variance and standard deviation there is a distinction between population and sample.

```{r R11}
range(mydata$Height)           # returns smallest a largest element
diff(range(mydata$Height))     # returns (largest-smallest)

sum((mydata$Height-mean(mydata$Height))^2)/n      # population variance
sum((mydata$Height-mean(mydata$Height))^2)/(n-1)  # sample variance
var(mydata$Height)
sqrt(var(mydata$Height))
sd(mydata$Height)
```

The coefficient of variation is a measure of risk. You can calculate it as a normalized standard deviation: $$ CV = \frac{\sigma}{\mu} $$

```{r R12}
sd(mydata$Height)/mean(mydata$Height)             # CV ... coefficient of variation
```


## 3.3 Measures of relative standing

Percentiles are calculated as follows:

```{r R13}
quantile(mydata$Height,0.25)
quantile(mydata$Height,c(0.25,0.5,0.75))
quantile(mydata$Height)
IQR(mydata$Height)             # Inter quartile rante: Q3-Q1 or P_75-P_25
```


---------------------------------------------------------
# 4 Measures of linear relationship

## 4.1 Covariance

```{r R14}
n = length(mydata$Age)
x = mydata$Age
y = mydata$Height

sum((x-mean(x))*(y-mean(y)))/n           # Population covariance
sum((x-mean(x))*(y-mean(y)))/(n-1)       # sample covariance
# or simply
cov(x,y)
```

## 4.2 Correlation coefficient

```{r R15}
cov(x,y)/(sd(x)*sd(y))
# or simply
cor(x,y)
```

## 4.3 Regression line

### Example 1: Simple example

We first generate some data. A variable ```x``` and a variable ```y```.

```{r R16}
x = (1:8)
y = c(6,1,9,5,17,12,14,15)
```
We then run a regression of the form: $$y = \beta_0 + \beta_1 * x + \epsilon$$. The command for this OLS regression is: ```lm(y~x)``` where ```y``` is the dependent variable and ```x``` is the independent variable.

```{r R17}
res = lm(y~x)  # runs regression
```
We fist make a **scatterplot** with least squares trend line.

```{r R18}
plot(x, y,main="Regression: y = b_0 + b_1*x")
abline(res)
```
We next calculate the  coefficient of determination $$R^2$$ by summarizing the residuals of our regression.

```{r R19}
summary(res)
```
Finally, we use the model to make a prediction of ```y``` when ```x=8.5```.

```{r R20}
# Prediction: size of 8.5 will produce a math score of ...
betas = coef(res)
cat("Prediction for y of x=8.5 is:", sum(betas * c(1,8.5)))
```
-------------------------
### Example 2: OLS with categorical (dummy variables)

The next example is a bit more involved as we increase the number of explanatory variables. In addition, some explanatory variables are categorical variables. In order to use them in our OLS regression we first have to make so called **dummy** variables (i.e. indicator variables that are either 0 or 1). This is very easy to do in **R** as we simply use the ```as.factor()``` command inside the linear regression command ```lm()```.

```{r R21}
res = lm(mydata$AverageMathSAT ~ mydata$Height + mydata$Age + as.factor(mydata$Female) + as.factor(mydata$Race))   # type res to see output table

# R^2: Coefficient of determination
summary(res)

# Prediction: size of 2.5, 22 years of age, female, race=other will produce a math score of ...
betas = coef(res)
sum(betas * c(1,2.5,22,1,0,0,1,0))
```
