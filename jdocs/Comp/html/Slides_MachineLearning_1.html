

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>19. Machine Learning I: Introduction to Machine Learning &mdash; Computational Economics 1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/graphviz.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="20. Machine Learning II: Categorization Algorithm" href="Slides_MachineLearning_2.html" />
    <link rel="prev" title="18. Constrained Optimization" href="Slides_Cake.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Computational Economics
          

          
          </a>

          
            
            
              <div class="version">
                1.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Slides_CourseAdministration.html">1. Course Administration</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Basics.html">2. First Steps in Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_DataStructures.html">3. Python Data Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Loop.html">4. Basic Programming Techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Debugging.html">5. Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Arrays.html">6. Vectors and Matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Plot.html">7. Plotting using <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Functions.html">8. Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_OOP.html">9. Object Oriented Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Data_1.html">10. Working with Data I: Data Cleaning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Data_2.html">11. Working with Data II: Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_WebData.html">12. Working with Data from the Web I</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_WebData_2.html">13. Working with Data from the Web II</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_RegularExpressions.html">14. Regular Expressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Random.html">15. Random Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Root.html">16. Root Finding</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Optimization.html">17. Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Cake.html">18. Constrained Optimization</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">19. Machine Learning I: Introduction to Machine Learning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#different-types-of-machine-learning-algorithms">19.1. Different Types of Machine Learning Algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="#a-first-regression-model-example">19.2. A First Regression Model Example</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#downloading-the-data">19.2.1. Downloading the Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inspecting-the-data">19.2.2. Inspecting the Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#splitting-the-data">19.2.3. Splitting the Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#visualize-data-to-gain-insight">19.2.4. Visualize data to gain insight</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training-a-regression-model">19.2.5. Training a Regression Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#key-concepts-and-summary">19.3. Key Concepts and Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="#self-check-questions">19.4. Self-Check Questions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Slides_MachineLearning_2.html">20. Machine Learning II: Categorization Algorithm</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_OLG_I.html">21. A Simple OLG Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_OLG_II.html">22. An OLG Model with Labor-Leisure Choice</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Growth.html">23. Growth Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Slides_Assignments.html">24. Assignments</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Computational Economics</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li><span class="section-number">19. </span>Machine Learning I: Introduction to Machine Learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/Slides_MachineLearning_1.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="machine-learning-i-introduction-to-machine-learning">
<h1><span class="section-number">19. </span>Machine Learning I: Introduction to Machine Learning<a class="headerlink" href="#machine-learning-i-introduction-to-machine-learning" title="Permalink to this headline">Â¶</a></h1>
<div class="sidebar" id="machinelearningi">
<p class="sidebar-title">Section Learning Objectives</p>
<blockquote>
<div><ul class="simple">
<li><p>Introduction to machine learning</p></li>
<li><p>Simple example</p></li>
</ul>
</div></blockquote>
</div>
<p id="index-0">This chapter and the next are heavily built on two chapters in <a class="reference internal" href="#geron2019" id="id1"><span>[Geron2019]</span></a>
Hands-On Machine Learning with Scikit-Learn, Kera &amp; TensorFlow.
You can find a link to the GitHub page of this textbook at <a class="reference external" href="https://github.com/ageron/handson-ml/">Geron GitHub</a></p>
<p>Machine learning is ubiquitous. Machine learning algorithm guide your daily
google searches, determine the way Netflix presents its offerings to you, guide
your selections when shopping on sites such as Amazon, translate your spoken
words into code that your Phone or any other of the many voice assistants can
process further into meaningful services for you, drive Teslas semi-autonomous,
or simply recognize your face on a photo you upload onto Facebook. These are
just a few of the many many examples where Machine Learning has entered your
life, whether you are aware of it or not.</p>
<div class="figure align-center" id="fig0-ml-timeline">
<a class="reference internal image-reference" href="_images/MachineLearningTimeLine.jpeg"><img alt="_images/MachineLearningTimeLine.jpeg" src="_images/MachineLearningTimeLine.jpeg" style="width: 540.0px; height: 290.7px;" /></a>
<p class="caption"><span class="caption-number">Fig. 19.1 </span><span class="caption-text">Brief History of Machine Learning</span><a class="headerlink" href="#fig0-ml-timeline" title="Permalink to this image">Â¶</a></p>
</div>
<p>One of the earliest examples of a Machine Learning algorithm that you are
familiar with is the <strong>Spam Filter</strong>. We will be using this example to further
explain what machine learning does and how different machine learning
algorithms can be classified.</p>
<div class="section" id="different-types-of-machine-learning-algorithms">
<h2><span class="section-number">19.1. </span>Different Types of Machine Learning Algorithms<a class="headerlink" href="#different-types-of-machine-learning-algorithms" title="Permalink to this headline">Â¶</a></h2>
<p>Machine learning algorithms can be classified according to the following
criteria:</p>
<blockquote>
<div><ol class="arabic simple">
<li><dl class="simple">
<dt>Supervised vs. unsupervised vs. reinforcement learning</dt><dd><p>Are they trained (estimated) with human supervision, without supervision,
or do they reinforce actions based on rewards and penalties.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Online vs. batch learning</dt><dd><p>Do they learn incrementally as data becomes available or do they require
âall of the dataâ at once</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Instance-based vs. model-based learning</dt><dd><p>Do they compare new data points to known data points or do they detect
patterns building a predictive model (based on parameters)</p>
</dd>
</dl>
</li>
</ol>
</div></blockquote>
<p>Letâs discuss this classification in some more detail.
In <strong>supervised learning</strong>, the training set (i.e., data) you feed to the
algorithm includes the desired outcome or solution, called label (i.e., the
dependent or outcome variable).  <strong>Unsupervised learning</strong> deals with unlabeled
data.</p>
<p><a class="reference internal" href="#fig1-ml-classification"><span class="std std-numref">Fig. 19.2</span></a> summarizes the different types of</p>
<blockquote>
<div><ol class="lowerroman simple">
<li><p>Supervised Learning,</p></li>
<li><p>Unsupervised Learning, and</p></li>
<li><p>Reinforcement Learning.</p></li>
</ol>
</div></blockquote>
<div class="figure align-center" id="fig1-ml-classification">
<a class="reference internal image-reference" href="_images/MLclassification.png"><img alt="_images/MLclassification.png" src="_images/MLclassification.png" style="width: 699.0px; height: 500.0px;" /></a>
<p class="caption"><span class="caption-number">Fig. 19.2 </span><span class="caption-text">Classification of ML algorithms.</span><a class="headerlink" href="#fig1-ml-classification" title="Permalink to this image">Â¶</a></p>
</div>
<p><a class="reference internal" href="#tab-metricsvsml"><span class="std std-numref">Table 19.1</span></a> contrasts the language we use in Econometrics with the
language commonly used in Machine Learning.</p>
<table class="docutils align-default" id="tab-metricsvsml">
<caption><span class="caption-number">Table 19.1 </span><span class="caption-text">The Language of Econometrics and Machine Learning</span><a class="headerlink" href="#tab-metricsvsml" title="Permalink to this table">Â¶</a></caption>
<colgroup>
<col style="width: 19%" />
<col style="width: 35%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Item</p></th>
<th class="head"><p>Econometrics</p></th>
<th class="head"><p>Machine Learning2011</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Data</p></td>
<td><p>Data/Obs.</p></td>
<td><p>Training data/set</p></td>
</tr>
<tr class="row-odd"><td><p>y</p></td>
<td><p>Dependent var</p></td>
<td><p>Label</p></td>
</tr>
<tr class="row-even"><td><p>x or X</p></td>
<td><p>Independent var</p></td>
<td><p>Feature/predictor</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>Estimation</p></td>
<td><p>Training algo/model</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="a-first-regression-model-example">
<h2><span class="section-number">19.2. </span>A First Regression Model Example<a class="headerlink" href="#a-first-regression-model-example" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="downloading-the-data">
<h3><span class="section-number">19.2.1. </span>Downloading the Data<a class="headerlink" href="#downloading-the-data" title="Permalink to this headline">Â¶</a></h3>
<p>We again start by first importing some libraries.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># https://github.com/ageron/handson-ml/</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">tarfile</span>
<span class="kn">from</span> <span class="nn">six.moves</span> <span class="kn">import</span> <span class="n">urllib</span>
</pre></div>
</div>
<p>We first need to download the data from GitHub:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DOWNLOAD_ROOT</span> <span class="o">=</span> <span class="s2">&quot;https://raw.githubusercontent.com/ageron/handson-ml/master/&quot;</span>
<span class="n">HOUSING_URL</span> <span class="o">=</span> <span class="n">DOWNLOAD_ROOT</span> <span class="o">+</span> <span class="s2">&quot;datasets/housing/housing.tgz&quot;</span>

<span class="k">def</span> <span class="nf">fetch_housing_data</span><span class="p">(</span><span class="n">housing_url</span><span class="o">=</span><span class="n">HOUSING_URL</span><span class="p">):</span>
    <span class="n">tgz_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="s2">&quot;housing.tgz&quot;</span><span class="p">)</span>
    <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">housing_url</span><span class="p">,</span> <span class="n">tgz_path</span><span class="p">)</span>
    <span class="n">housing_tgz</span> <span class="o">=</span> <span class="n">tarfile</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">tgz_path</span><span class="p">)</span>
    <span class="n">housing_tgz</span><span class="o">.</span><span class="n">extractall</span><span class="p">()</span>
    <span class="n">housing_tgz</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

<span class="n">fetch_housing_data</span><span class="p">()</span>
</pre></div>
</div>
<p>Once that is done, we store the
data on our local drive. You want to avoid downloading the data each time you
run the script file. So download it once, store it and then comment our the
download section of your code and just read the data from your local drive
using  the <code class="docutils literal notranslate"><span class="pre">pd.read_csv()</span></code> function from the <code class="docutils literal notranslate"><span class="pre">numpy</span></code> library.
We then can use a few commands to look at our data which is organized as a
Pandas Dataframe.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">housing</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;Lecture_MachineLearning_1/housing.csv&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">housing</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>   <span class="n">longitude</span>  <span class="n">latitude</span>  <span class="n">housing_median_age</span>  <span class="n">total_rooms</span>
<span class="n">total_bedrooms</span>  \
<span class="mi">0</span>    <span class="o">-</span><span class="mf">122.23</span>     <span class="mf">37.88</span>                <span class="mf">41.0</span>        <span class="mf">880.0</span>
<span class="mf">129.0</span>
<span class="mi">1</span>    <span class="o">-</span><span class="mf">122.22</span>     <span class="mf">37.86</span>                <span class="mf">21.0</span>       <span class="mf">7099.0</span>
<span class="mf">1106.0</span>
<span class="mi">2</span>    <span class="o">-</span><span class="mf">122.24</span>     <span class="mf">37.85</span>                <span class="mf">52.0</span>       <span class="mf">1467.0</span>
<span class="mf">190.0</span>
<span class="mi">3</span>    <span class="o">-</span><span class="mf">122.25</span>     <span class="mf">37.85</span>                <span class="mf">52.0</span>       <span class="mf">1274.0</span>
<span class="mf">235.0</span>
<span class="mi">4</span>    <span class="o">-</span><span class="mf">122.25</span>     <span class="mf">37.85</span>                <span class="mf">52.0</span>       <span class="mf">1627.0</span>
<span class="mf">280.0</span>

   <span class="n">population</span>  <span class="n">households</span>  <span class="n">median_income</span>  <span class="n">median_house_value</span>
<span class="n">ocean_proximity</span>
<span class="mi">0</span>       <span class="mf">322.0</span>       <span class="mf">126.0</span>         <span class="mf">8.3252</span>            <span class="mf">452600.0</span>
<span class="n">NEAR</span> <span class="n">BAY</span>
<span class="mi">1</span>      <span class="mf">2401.0</span>      <span class="mf">1138.0</span>         <span class="mf">8.3014</span>            <span class="mf">358500.0</span>
<span class="n">NEAR</span> <span class="n">BAY</span>
<span class="mi">2</span>       <span class="mf">496.0</span>       <span class="mf">177.0</span>         <span class="mf">7.2574</span>            <span class="mf">352100.0</span>
<span class="n">NEAR</span> <span class="n">BAY</span>
<span class="mi">3</span>       <span class="mf">558.0</span>       <span class="mf">219.0</span>         <span class="mf">5.6431</span>            <span class="mf">341300.0</span>
<span class="n">NEAR</span> <span class="n">BAY</span>
<span class="mi">4</span>       <span class="mf">565.0</span>       <span class="mf">259.0</span>         <span class="mf">3.8462</span>            <span class="mf">342200.0</span>
<span class="n">NEAR</span> <span class="n">BAY</span>
</pre></div>
</div>
<p>We can get a quick summary of our data using the <code class="docutils literal notranslate"><span class="pre">.info()</span></code> function on the
dataframe.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">housing</span><span class="o">.</span><span class="n">info</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 10 columns):
 #   Column              Non-Null Count  Dtype
---  ------              --------------  -----
 0   longitude           20640 non-null  float64
 1   latitude            20640 non-null  float64
 2   housing_median_age  20640 non-null  float64
 3   total_rooms         20640 non-null  float64
 4   total_bedrooms      20433 non-null  float64
 5   population          20640 non-null  float64
 6   households          20640 non-null  float64
 7   median_income       20640 non-null  float64
 8   median_house_value  20640 non-null  float64
 9   ocean_proximity     20640 non-null  object
dtypes: float64(9), object(1)
memory usage: 1.6+ MB
None
</pre></div>
</div>
</div>
<div class="section" id="inspecting-the-data">
<h3><span class="section-number">19.2.2. </span>Inspecting the Data<a class="headerlink" href="#inspecting-the-data" title="Permalink to this headline">Â¶</a></h3>
<p>We can also get some summary statistic for specific feature variables (in
econometrics we refer to these as independent variables). We can check the
variable <code class="docutils literal notranslate"><span class="pre">ocean_proximity</span></code> for instance. It is a categorical variable. We
cannot do any numerical statistics with this but we can get a feel for this
variable by counting how many observation by category we have.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;ocean_proximity&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>     <span class="mi">9136</span>
<span class="n">INLAND</span>        <span class="mi">6551</span>
<span class="n">NEAR</span> <span class="n">OCEAN</span>    <span class="mi">2658</span>
<span class="n">NEAR</span> <span class="n">BAY</span>      <span class="mi">2290</span>
<span class="n">ISLAND</span>           <span class="mi">5</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">ocean_proximity</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
</pre></div>
</div>
<p>Finally, we can use the <code class="docutils literal notranslate"><span class="pre">.describe()</span></code> dataframe method (or function) to get
certain summary statistics for each data column.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">housing</span><span class="o">.</span><span class="n">describe</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>          <span class="n">longitude</span>      <span class="n">latitude</span>  <span class="n">housing_median_age</span>   <span class="n">total_rooms</span>  \
<span class="n">count</span>  <span class="mf">20640.000000</span>  <span class="mf">20640.000000</span>        <span class="mf">20640.000000</span>  <span class="mf">20640.000000</span>
<span class="n">mean</span>    <span class="o">-</span><span class="mf">119.569704</span>     <span class="mf">35.631861</span>           <span class="mf">28.639486</span>   <span class="mf">2635.763081</span>
<span class="n">std</span>        <span class="mf">2.003532</span>      <span class="mf">2.135952</span>           <span class="mf">12.585558</span>   <span class="mf">2181.615252</span>
<span class="nb">min</span>     <span class="o">-</span><span class="mf">124.350000</span>     <span class="mf">32.540000</span>            <span class="mf">1.000000</span>      <span class="mf">2.000000</span>
<span class="mi">25</span><span class="o">%</span>     <span class="o">-</span><span class="mf">121.800000</span>     <span class="mf">33.930000</span>           <span class="mf">18.000000</span>   <span class="mf">1447.750000</span>
<span class="mi">50</span><span class="o">%</span>     <span class="o">-</span><span class="mf">118.490000</span>     <span class="mf">34.260000</span>           <span class="mf">29.000000</span>   <span class="mf">2127.000000</span>
<span class="mi">75</span><span class="o">%</span>     <span class="o">-</span><span class="mf">118.010000</span>     <span class="mf">37.710000</span>           <span class="mf">37.000000</span>   <span class="mf">3148.000000</span>
<span class="nb">max</span>     <span class="o">-</span><span class="mf">114.310000</span>     <span class="mf">41.950000</span>           <span class="mf">52.000000</span>  <span class="mf">39320.000000</span>

       <span class="n">total_bedrooms</span>    <span class="n">population</span>    <span class="n">households</span>  <span class="n">median_income</span>  \
<span class="n">count</span>    <span class="mf">20433.000000</span>  <span class="mf">20640.000000</span>  <span class="mf">20640.000000</span>   <span class="mf">20640.000000</span>
<span class="n">mean</span>       <span class="mf">537.870553</span>   <span class="mf">1425.476744</span>    <span class="mf">499.539680</span>       <span class="mf">3.870671</span>
<span class="n">std</span>        <span class="mf">421.385070</span>   <span class="mf">1132.462122</span>    <span class="mf">382.329753</span>       <span class="mf">1.899822</span>
<span class="nb">min</span>          <span class="mf">1.000000</span>      <span class="mf">3.000000</span>      <span class="mf">1.000000</span>       <span class="mf">0.499900</span>
<span class="mi">25</span><span class="o">%</span>        <span class="mf">296.000000</span>    <span class="mf">787.000000</span>    <span class="mf">280.000000</span>       <span class="mf">2.563400</span>
<span class="mi">50</span><span class="o">%</span>        <span class="mf">435.000000</span>   <span class="mf">1166.000000</span>    <span class="mf">409.000000</span>       <span class="mf">3.534800</span>
<span class="mi">75</span><span class="o">%</span>        <span class="mf">647.000000</span>   <span class="mf">1725.000000</span>    <span class="mf">605.000000</span>       <span class="mf">4.743250</span>
<span class="nb">max</span>       <span class="mf">6445.000000</span>  <span class="mf">35682.000000</span>   <span class="mf">6082.000000</span>      <span class="mf">15.000100</span>

       <span class="n">median_house_value</span>
<span class="n">count</span>        <span class="mf">20640.000000</span>
<span class="n">mean</span>        <span class="mf">206855.816909</span>
<span class="n">std</span>         <span class="mf">115395.615874</span>
<span class="nb">min</span>          <span class="mf">14999.000000</span>
<span class="mi">25</span><span class="o">%</span>         <span class="mf">119600.000000</span>
<span class="mi">50</span><span class="o">%</span>         <span class="mf">179700.000000</span>
<span class="mi">75</span><span class="o">%</span>         <span class="mf">264725.000000</span>
<span class="nb">max</span>         <span class="mf">500001.000000</span>
</pre></div>
</div>
<p>We next start with analyzing our data using graphs and simple statistics.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># %matplotlib inline</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">housing</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="c1"># save_fig(&quot;attribute_histogram_plots&quot;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/Slides_MachineLearning_1_figure6_1.png"><img alt="_images/Slides_MachineLearning_1_figure6_1.png" src="_images/Slides_MachineLearning_1_figure6_1.png" style="width: 15cm;" /></a>
</div>
<div class="section" id="splitting-the-data">
<h3><span class="section-number">19.2.3. </span>Splitting the Data<a class="headerlink" href="#splitting-the-data" title="Permalink to this headline">Â¶</a></h3>
<p>In machine learning it is important to be able to assess how well your trained
model can predict outcome variables (or label variables in the case of
supervised machine learning). In order to do this we need to split the sample
into a <strong>training sample</strong> that you use to train (estimate) the model with and into
a <strong>test sample</strong> that you can then use to verify how well your model makes out
of sample predictions.</p>
<p>It is important that you randomly split the sample and that both samples
maintain the features of the overall sample. For instance if your main sample
contains 45 percent men, then you want to split the sample in such a way that
the training sample has roughly 45 percent men in it and the test sample has
roughly 45 percent men in it.</p>
<p>In order to accomplish this we use the built in command <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code>
from the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> machine learning library. When you call this function you
need to specify how large you want the test sample to be. Below we choose a
split that maintains 80 percent of the observations in the raw data for
the training sample and 20 percent for test sample. We set the random seed by
hand, so that our results become reproducible, i.e., the sample is split in
exactly the same way each time we run the script file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># to make this notebook&#39;s output identical at every run</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">train_set</span><span class="p">,</span> <span class="n">test_set</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">housing</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p>We can now inspect the test sample.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">test_set</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>       <span class="n">longitude</span>  <span class="n">latitude</span>  <span class="n">housing_median_age</span>  <span class="n">total_rooms</span>
<span class="n">total_bedrooms</span>  \
<span class="mi">20046</span>    <span class="o">-</span><span class="mf">119.01</span>     <span class="mf">36.06</span>                <span class="mf">25.0</span>       <span class="mf">1505.0</span>
<span class="n">NaN</span>
<span class="mi">3024</span>     <span class="o">-</span><span class="mf">119.46</span>     <span class="mf">35.14</span>                <span class="mf">30.0</span>       <span class="mf">2943.0</span>
<span class="n">NaN</span>
<span class="mi">15663</span>    <span class="o">-</span><span class="mf">122.44</span>     <span class="mf">37.80</span>                <span class="mf">52.0</span>       <span class="mf">3830.0</span>
<span class="n">NaN</span>
<span class="mi">20484</span>    <span class="o">-</span><span class="mf">118.72</span>     <span class="mf">34.28</span>                <span class="mf">17.0</span>       <span class="mf">3051.0</span>
<span class="n">NaN</span>
<span class="mi">9814</span>     <span class="o">-</span><span class="mf">121.93</span>     <span class="mf">36.62</span>                <span class="mf">34.0</span>       <span class="mf">2351.0</span>
<span class="n">NaN</span>

       <span class="n">population</span>  <span class="n">households</span>  <span class="n">median_income</span>  <span class="n">median_house_value</span>  \
<span class="mi">20046</span>      <span class="mf">1392.0</span>       <span class="mf">359.0</span>         <span class="mf">1.6812</span>             <span class="mf">47700.0</span>
<span class="mi">3024</span>       <span class="mf">1565.0</span>       <span class="mf">584.0</span>         <span class="mf">2.5313</span>             <span class="mf">45800.0</span>
<span class="mi">15663</span>      <span class="mf">1310.0</span>       <span class="mf">963.0</span>         <span class="mf">3.4801</span>            <span class="mf">500001.0</span>
<span class="mi">20484</span>      <span class="mf">1705.0</span>       <span class="mf">495.0</span>         <span class="mf">5.7376</span>            <span class="mf">218600.0</span>
<span class="mi">9814</span>       <span class="mf">1063.0</span>       <span class="mf">428.0</span>         <span class="mf">3.7250</span>            <span class="mf">278000.0</span>

      <span class="n">ocean_proximity</span>
<span class="mi">20046</span>          <span class="n">INLAND</span>
<span class="mi">3024</span>           <span class="n">INLAND</span>
<span class="mi">15663</span>        <span class="n">NEAR</span> <span class="n">BAY</span>
<span class="mi">20484</span>       <span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>
<span class="mi">9814</span>       <span class="n">NEAR</span> <span class="n">OCEAN</span>
</pre></div>
</div>
<p>One thing that is important when splitting the data is to maintain the
composition of the data in the subsamples, so that subsamples correctly
represent the large sample. Let us look at an example.</p>
<p>If we analyze income using the histogram plot we have the following
distribution of income.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;median_income&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">_subplots</span><span class="o">.</span><span class="n">AxesSubplot</span> <span class="n">at</span> <span class="mh">0x7fa6105707d0</span><span class="o">&gt;</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/Slides_MachineLearning_1_figure9_1.png"><img alt="_images/Slides_MachineLearning_1_figure9_1.png" src="_images/Slides_MachineLearning_1_figure9_1.png" style="width: 15cm;" /></a>
<p>We can also make a categorical variable out of income using the following
command.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;income_cat&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;median_income&quot;</span><span class="p">],</span>
                               <span class="n">bins</span><span class="o">=</span><span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">],</span>
                               <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
<p>In order to do that we need to define the bins and assign labels to each bin.
Where label <code class="docutils literal notranslate"><span class="pre">1</span></code> indicates the low income group, label <code class="docutils literal notranslate"><span class="pre">2</span></code> indicates income
between zero and 1.5, etc. We can then plot histograms of this new categorical
variable.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;income_cat&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>

<span class="n">housing</span><span class="p">[</span><span class="s2">&quot;income_cat&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">3</span>    <span class="mi">7236</span>
<span class="mi">2</span>    <span class="mi">6581</span>
<span class="mi">4</span>    <span class="mi">3639</span>
<span class="mi">5</span>    <span class="mi">2362</span>
<span class="mi">1</span>     <span class="mi">822</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">income_cat</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">int64</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">_subplots</span><span class="o">.</span><span class="n">AxesSubplot</span> <span class="n">at</span> <span class="mh">0x7fa61048eed0</span><span class="o">&gt;</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/Slides_MachineLearning_1_figure11_1.png"><img alt="_images/Slides_MachineLearning_1_figure11_1.png" src="_images/Slides_MachineLearning_1_figure11_1.png" style="width: 15cm;" /></a>
<p>When we split the sample into a training set and a test set we need to make
sure that the distribution of the different income groups is maintained in the
subsamples. We want to avoid as situation where we have, letâs say richer
households in the training sample and relatively poorer households in the
testing sample. We can accomplish this with stratified sampling. The built in
command <code class="docutils literal notranslate"><span class="pre">STratifiedShuffleSplit</span></code> will split the data ensuring that the
distribution is maintained.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedShuffleSplit</span>

<span class="n">split</span> <span class="o">=</span> <span class="n">StratifiedShuffleSplit</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">split</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">housing</span><span class="p">,</span> <span class="n">housing</span><span class="p">[</span><span class="s2">&quot;income_cat&quot;</span><span class="p">]):</span>
    <span class="n">strat_train_set</span> <span class="o">=</span> <span class="n">housing</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">train_index</span><span class="p">]</span>
    <span class="n">strat_test_set</span> <span class="o">=</span> <span class="n">housing</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">test_index</span><span class="p">]</span>
</pre></div>
</div>
<p>We can check this by comparing the histogram we made earlier for the entire
data with the histogram for the income variables of the stratified test sample.
You see that the proportions of the different income groups are maintained
because we split the sample according to the income strata in the above code
block.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">strat_test_set</span><span class="p">[</span><span class="s2">&quot;income_cat&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">strat_test_set</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;income_cat&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">housing</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">3</span>    <span class="mf">0.350581</span>
<span class="mi">2</span>    <span class="mf">0.318847</span>
<span class="mi">4</span>    <span class="mf">0.176308</span>
<span class="mi">5</span>    <span class="mf">0.114438</span>
<span class="mi">1</span>    <span class="mf">0.039826</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">income_cat</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>
</pre></div>
</div>
<p>We next do it more systematic across the full data, the stratified test set,
and the non-stratified test set. We then plot the proportions of the income
groups for each data set. You want the proportions in the test set be very
close to the income proportions in the full data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">income_cat_proportions</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;income_cat&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">train_set</span><span class="p">,</span> <span class="n">test_set</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">housing</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">compare_props</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s2">&quot;Overall&quot;</span><span class="p">:</span> <span class="n">income_cat_proportions</span><span class="p">(</span><span class="n">housing</span><span class="p">),</span>
    <span class="s2">&quot;Stratified&quot;</span><span class="p">:</span> <span class="n">income_cat_proportions</span><span class="p">(</span><span class="n">strat_test_set</span><span class="p">),</span>
    <span class="s2">&quot;Random&quot;</span><span class="p">:</span> <span class="n">income_cat_proportions</span><span class="p">(</span><span class="n">test_set</span><span class="p">),</span>
<span class="p">})</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span>
<span class="n">compare_props</span><span class="p">[</span><span class="s2">&quot;Rand. </span><span class="si">%e</span><span class="s2">rror&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">compare_props</span><span class="p">[</span><span class="s2">&quot;Random&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">compare_props</span><span class="p">[</span><span class="s2">&quot;Overall&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">100</span>
<span class="n">compare_props</span><span class="p">[</span><span class="s2">&quot;Strat. </span><span class="si">%e</span><span class="s2">rror&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">compare_props</span><span class="p">[</span><span class="s2">&quot;Stratified&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">compare_props</span><span class="p">[</span><span class="s2">&quot;Overall&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="mi">100</span>
</pre></div>
</div>
<p>Before we move on, we drop the income category variable because we do not use
this categorical variable in our âforecastingâ model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">set_</span> <span class="ow">in</span> <span class="p">(</span><span class="n">strat_train_set</span><span class="p">,</span> <span class="n">strat_test_set</span><span class="p">):</span>
    <span class="n">set_</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;income_cat&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="visualize-data-to-gain-insight">
<h3><span class="section-number">19.2.4. </span>Visualize data to gain insight<a class="headerlink" href="#visualize-data-to-gain-insight" title="Permalink to this headline">Â¶</a></h3>
<p>We next copy the stratified training data set and assign it a shorter name.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">housing</span> <span class="o">=</span> <span class="n">strat_train_set</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</pre></div>
</div>
<p>We next make a scatter splot to get a feel for the geographic location of the
house observations. Once we plot it, we roughly see the outline of the State of
California.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">housing</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;scatter&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;latitude&quot;</span><span class="p">)</span>
<span class="c1"># save_fig(&quot;bad_visualization_plot&quot;)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">_subplots</span><span class="o">.</span><span class="n">AxesSubplot</span> <span class="n">at</span> <span class="mh">0x7fa60c335490</span><span class="o">&gt;</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/Slides_MachineLearning_1_figure17_1.png"><img alt="_images/Slides_MachineLearning_1_figure17_1.png" src="_images/Slides_MachineLearning_1_figure17_1.png" style="width: 15cm;" /></a>
<p>We can do a little better by making the data points in the graph a bit opaque.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">housing</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;scatter&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="c1"># save_fig(&quot;better_visualization_plot&quot;)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">axes</span><span class="o">.</span><span class="n">_subplots</span><span class="o">.</span><span class="n">AxesSubplot</span> <span class="n">at</span> <span class="mh">0x7fa60c318210</span><span class="o">&gt;</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/Slides_MachineLearning_1_figure18_1.png"><img alt="_images/Slides_MachineLearning_1_figure18_1.png" src="_images/Slides_MachineLearning_1_figure18_1.png" style="width: 15cm;" /></a>
<p>We can also adjust the data point size using a population measure. So a bigger
point represents a housing observation from a place with a higher population
density.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">housing</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;scatter&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;population&quot;</span><span class="p">]</span><span class="o">/</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;population&quot;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">7</span><span class="p">),</span>
    <span class="n">c</span><span class="o">=</span><span class="s2">&quot;median_house_value&quot;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">get_cmap</span><span class="p">(</span><span class="s2">&quot;jet&quot;</span><span class="p">),</span> <span class="n">colorbar</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">sharex</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="c1"># save_fig(&quot;housing_prices_scatterplot&quot;)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">legend</span><span class="o">.</span><span class="n">Legend</span> <span class="n">at</span> <span class="mh">0x7fa60c26a210</span><span class="o">&gt;</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/Slides_MachineLearning_1_figure19_1.png"><img alt="_images/Slides_MachineLearning_1_figure19_1.png" src="_images/Slides_MachineLearning_1_figure19_1.png" style="width: 15cm;" /></a>
<p>We next investigate correlations between the value of the house and the other
variables in our data in order to get a feel for what a good forecasting model
should factor in.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">housing</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">[</span><span class="s2">&quot;median_house_value&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">median_house_value</span>    <span class="mf">1.000000</span>
<span class="n">median_income</span>         <span class="mf">0.687160</span>
<span class="n">total_rooms</span>           <span class="mf">0.135097</span>
<span class="n">housing_median_age</span>    <span class="mf">0.114110</span>
<span class="n">households</span>            <span class="mf">0.064506</span>
<span class="n">total_bedrooms</span>        <span class="mf">0.047689</span>
<span class="n">population</span>           <span class="o">-</span><span class="mf">0.026920</span>
<span class="n">longitude</span>            <span class="o">-</span><span class="mf">0.047432</span>
<span class="n">latitude</span>             <span class="o">-</span><span class="mf">0.142724</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">median_house_value</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Pandas</span></code> library has a very powerful command that allows you to draw
scatter plots for all variable combinations. This is a visual method to inspect
the correlation between variables.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># from pandas.tools.plotting import scatter_matrix # For older versions of Pandas</span>
<span class="kn">from</span> <span class="nn">pandas.plotting</span> <span class="kn">import</span> <span class="n">scatter_matrix</span>

<span class="n">attributes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;median_house_value&quot;</span><span class="p">,</span> <span class="s2">&quot;median_income&quot;</span><span class="p">,</span> <span class="s2">&quot;total_rooms&quot;</span><span class="p">,</span>
              <span class="s2">&quot;housing_median_age&quot;</span><span class="p">]</span>
<span class="n">scatter_matrix</span><span class="p">(</span><span class="n">housing</span><span class="p">[</span><span class="n">attributes</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="c1"># save_fig(&quot;scatter_matrix_plot&quot;)</span>

<span class="n">housing</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">kind</span><span class="o">=</span><span class="s2">&quot;scatter&quot;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;median_income&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;median_house_value&quot;</span><span class="p">,</span>
             <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">550000</span><span class="p">])</span>
<span class="c1"># save_fig(&quot;income_vs_house_value_scatterplot&quot;)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">550000</span><span class="p">]</span>
</pre></div>
</div>
<a class="reference internal image-reference" href="_images/Slides_MachineLearning_1_figure21_1.png"><img alt="_images/Slides_MachineLearning_1_figure21_1.png" src="_images/Slides_MachineLearning_1_figure21_1.png" style="width: 15cm;" /></a>
<a class="reference internal image-reference" href="_images/Slides_MachineLearning_1_figure21_2.png"><img alt="_images/Slides_MachineLearning_1_figure21_2.png" src="_images/Slides_MachineLearning_1_figure21_2.png" style="width: 15cm;" /></a>
<p>Finally we can generate some additional variables that are combinations of
variables in our original data set.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;rooms_per_household&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">housing</span><span class="p">[</span><span class="s2">&quot;total_rooms&quot;</span><span class="p">]</span><span class="o">/</span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;households&quot;</span><span class="p">]</span>
<span class="n">housing</span><span class="p">[</span><span class="s2">&quot;bedrooms_per_room&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">housing</span><span class="p">[</span><span class="s2">&quot;total_bedrooms&quot;</span><span class="p">]</span><span class="o">/</span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;total_rooms&quot;</span><span class="p">]</span>
<span class="n">housing</span><span class="p">[</span><span class="s2">&quot;population_per_household&quot;</span><span class="p">]</span><span class="o">=</span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;population&quot;</span><span class="p">]</span><span class="o">/</span><span class="n">housing</span><span class="p">[</span><span class="s2">&quot;households&quot;</span><span class="p">]</span>

<span class="c1"># Note: there was a bug in the previous cell, in the definition of the rooms_per_household attribute. This explains why the correlation value below differs slightly from the value in the book (unless you are reading the latest version).</span>

<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">housing</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">[</span><span class="s2">&quot;median_house_value&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">median_house_value</span>          <span class="mf">1.000000</span>
<span class="n">median_income</span>               <span class="mf">0.687160</span>
<span class="n">rooms_per_household</span>         <span class="mf">0.146285</span>
<span class="n">total_rooms</span>                 <span class="mf">0.135097</span>
<span class="n">housing_median_age</span>          <span class="mf">0.114110</span>
<span class="n">households</span>                  <span class="mf">0.064506</span>
<span class="n">total_bedrooms</span>              <span class="mf">0.047689</span>
<span class="n">population_per_household</span>   <span class="o">-</span><span class="mf">0.021985</span>
<span class="n">population</span>                 <span class="o">-</span><span class="mf">0.026920</span>
<span class="n">longitude</span>                  <span class="o">-</span><span class="mf">0.047432</span>
<span class="n">latitude</span>                   <span class="o">-</span><span class="mf">0.142724</span>
<span class="n">bedrooms_per_room</span>          <span class="o">-</span><span class="mf">0.259984</span>
<span class="n">Name</span><span class="p">:</span> <span class="n">median_house_value</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">float64</span>
</pre></div>
</div>
</div>
<div class="section" id="training-a-regression-model">
<h3><span class="section-number">19.2.5. </span>Training a Regression Model<a class="headerlink" href="#training-a-regression-model" title="Permalink to this headline">Â¶</a></h3>
<p>Before training (estimating) the model we need to prepare the data for the
built in Machine Learning algorithms. We need to make sure that the data only
contains numeric data and not strings, lists, etc.</p>
<p>We first put the label variable and the rest of the data (the X variables) into
separate dataframes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">housing</span> <span class="o">=</span> <span class="n">strat_train_set</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s2">&quot;median_house_value&quot;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># drop labels for training set</span>
<span class="n">housing_labels</span> <span class="o">=</span> <span class="n">strat_train_set</span><span class="p">[</span><span class="s2">&quot;median_house_value&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
</pre></div>
</div>
<p>We then check how many observations with incomplete (i.e., missing)
observations we have.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sample_incomplete_rows</span> <span class="o">=</span> <span class="n">housing</span><span class="p">[</span><span class="n">housing</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample_incomplete_rows</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>       <span class="n">longitude</span>  <span class="n">latitude</span>  <span class="n">housing_median_age</span>  <span class="n">total_rooms</span>
<span class="n">total_bedrooms</span>  \
<span class="mi">4629</span>     <span class="o">-</span><span class="mf">118.30</span>     <span class="mf">34.07</span>                <span class="mf">18.0</span>       <span class="mf">3759.0</span>
<span class="n">NaN</span>
<span class="mi">6068</span>     <span class="o">-</span><span class="mf">117.86</span>     <span class="mf">34.01</span>                <span class="mf">16.0</span>       <span class="mf">4632.0</span>
<span class="n">NaN</span>
<span class="mi">17923</span>    <span class="o">-</span><span class="mf">121.97</span>     <span class="mf">37.35</span>                <span class="mf">30.0</span>       <span class="mf">1955.0</span>
<span class="n">NaN</span>
<span class="mi">13656</span>    <span class="o">-</span><span class="mf">117.30</span>     <span class="mf">34.05</span>                 <span class="mf">6.0</span>       <span class="mf">2155.0</span>
<span class="n">NaN</span>
<span class="mi">19252</span>    <span class="o">-</span><span class="mf">122.79</span>     <span class="mf">38.48</span>                 <span class="mf">7.0</span>       <span class="mf">6837.0</span>
<span class="n">NaN</span>

       <span class="n">population</span>  <span class="n">households</span>  <span class="n">median_income</span> <span class="n">ocean_proximity</span>
<span class="mi">4629</span>       <span class="mf">3296.0</span>      <span class="mf">1462.0</span>         <span class="mf">2.2708</span>       <span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>
<span class="mi">6068</span>       <span class="mf">3038.0</span>       <span class="mf">727.0</span>         <span class="mf">5.1762</span>       <span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>
<span class="mi">17923</span>       <span class="mf">999.0</span>       <span class="mf">386.0</span>         <span class="mf">4.6328</span>       <span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>
<span class="mi">13656</span>      <span class="mf">1039.0</span>       <span class="mf">391.0</span>         <span class="mf">1.6675</span>          <span class="n">INLAND</span>
<span class="mi">19252</span>      <span class="mf">3468.0</span>      <span class="mf">1405.0</span>         <span class="mf">3.1662</span>       <span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>
</pre></div>
</div>
<p>We next run a built in function over our data that attempts to impute missing
data with the median values of similar observations.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Warning: Since Scikit-Learn 0.20, the sklearn.preprocessing.Imputer class was replaced by the sklearn.impute.SimpleImputer class.</span>
<span class="k">try</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">sklearn.impute</span> <span class="kn">import</span> <span class="n">SimpleImputer</span> <span class="c1"># Scikit-Learn 0.20+</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
    <span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">Imputer</span> <span class="k">as</span> <span class="n">SimpleImputer</span>

<span class="n">imputer</span> <span class="o">=</span> <span class="n">SimpleImputer</span><span class="p">(</span><span class="n">strategy</span><span class="o">=</span><span class="s2">&quot;median&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Before we run the imputing algorithm we need to remove variables from the
dataframe that are not numeric such as categorical variables.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Remove the text attribute because median can only be calculated on numerical attributes:</span>

<span class="n">housing_num</span> <span class="o">=</span> <span class="n">housing</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;ocean_proximity&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># alternatively: housing_num = housing.select_dtypes(include=[np.number])</span>
<span class="n">imputer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">housing_num</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">imputer</span><span class="o">.</span><span class="n">statistics_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">housing_num</span><span class="o">.</span><span class="n">median</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">imputer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">housing_num</span><span class="p">)</span>

<span class="n">housing_tr</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">housing_num</span><span class="o">.</span><span class="n">columns</span><span class="p">,</span>
                          <span class="n">index</span><span class="o">=</span><span class="n">housing</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="o">-</span><span class="mf">118.51</span>     <span class="mf">34.26</span>     <span class="mf">29.</span>     <span class="mf">2119.5</span>     <span class="mf">433.</span>     <span class="mf">1164.</span>      <span class="mf">408.</span>
    <span class="mf">3.5409</span><span class="p">]</span>
<span class="p">[</span><span class="o">-</span><span class="mf">118.51</span>     <span class="mf">34.26</span>     <span class="mf">29.</span>     <span class="mf">2119.5</span>     <span class="mf">433.</span>     <span class="mf">1164.</span>      <span class="mf">408.</span>
    <span class="mf">3.5409</span><span class="p">]</span>
</pre></div>
</div>
<p>We have now created a complete dataset with no more missing observations that
only contains numeric variables.</p>
<p>We next need to handle the categorical variable of ocean proximity of a housing
unit. Let us inspect this variable first.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">housing_cat</span> <span class="o">=</span> <span class="n">housing</span><span class="p">[[</span><span class="s1">&#39;ocean_proximity&#39;</span><span class="p">]]</span>
<span class="n">housing_cat</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>      <span class="n">ocean_proximity</span>
<span class="mi">17606</span>       <span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>
<span class="mi">18632</span>       <span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>
<span class="mi">14650</span>      <span class="n">NEAR</span> <span class="n">OCEAN</span>
<span class="mi">3230</span>           <span class="n">INLAND</span>
<span class="mi">3555</span>        <span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>
<span class="mi">19480</span>          <span class="n">INLAND</span>
<span class="mi">8879</span>        <span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>
<span class="mi">13685</span>          <span class="n">INLAND</span>
<span class="mi">4937</span>        <span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>
<span class="mi">4861</span>        <span class="o">&lt;</span><span class="mi">1</span><span class="n">H</span> <span class="n">OCEAN</span>
</pre></div>
</div>
<p>We next use a label encoder function which is part of the <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> toolkit.
This is basically a function that will generate dummy variables out of
categorical variables. Dummy variables are variables that are either 0 or 1 and
and it can be used to encode categorical (i.e., non numerical attributes) such
as gender. The dummy variable could be called <code class="docutils literal notranslate"><span class="pre">d_female</span></code>. It is equal to
1 if a person is female and 0 if a person is not female.</p>
<p>The ocean proximity variable has multiple categories in it, not just two as in
the gender example. For each category of the ocean proximity variable we have
to create a dummy variable. We could accomplish this separately for each dummy
variable using <code class="docutils literal notranslate"><span class="pre">if</span></code> statements or we can use the built in <code class="docutils literal notranslate"><span class="pre">OrdinalEncoder</span></code>
to do it in one line of code.</p>
<p># Warning: earlier versions of the book used the LabelEncoder class or Pandasâ Series.factorize() method to encode string categorical attributes as integers. However, the OrdinalEncoder class that was introduced in Scikit-Learn 0.20 (see PR #10521) is preferable since it is designed for input features (X instead of labels y) and it plays well with pipelines (introduced later in this notebook). If you are using an older version of Scikit-Learn (&lt;0.20), then you can import it from future_encoders.py instead.
try:</p>
<blockquote>
<div><p>from sklearn.preprocessing import OrdinalEncoder</p>
</div></blockquote>
<dl class="simple">
<dt>except ImportError:</dt><dd><p>from future_encoders import OrdinalEncoder # Scikit-Learn &lt; 0.20</p>
</dd>
</dl>
<p>ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
housing_cat_encoded[:10]</p>
<p>print(<a href="#id3"><span class="problematic" id="id4">ordinal_encoder.categories_</span></a>)</p>
<p># Warning: earlier versions of the book used the LabelBinarizer or CategoricalEncoder classes to convert each categorical value to a one-hot vector. It is now preferable to use the OneHotEncoder class. Since Scikit-Learn 0.20 it can handle string categorical inputs (see PR #10521), not just integer categorical inputs. If you are using an older version of Scikit-Learn, you can import the new version from future_encoders.py:</p>
<dl class="simple">
<dt>try:</dt><dd><p>from sklearn.preprocessing import OrdinalEncoder # just to raise an ImportError if Scikit-Learn &lt; 0.20
from sklearn.preprocessing import OneHotEncoder</p>
</dd>
<dt>except ImportError:</dt><dd><p>from future_encoders import OneHotEncoder # Scikit-Learn &lt; 0.20</p>
</dd>
</dl>
<p>cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
housing_cat_1hot</p>
<p>print(<a href="#id5"><span class="problematic" id="id6">cat_encoder.categories_</span></a>)</p>
<p># get the right column indices: safer than hard-coding indices 3, 4, 5, 6
rooms_ix, bedrooms_ix, population_ix, household_ix = [</p>
<blockquote>
<div><p>list(housing.columns).index(col)
for col in (âtotal_roomsâ, âtotal_bedroomsâ, âpopulationâ, âhouseholdsâ)]</p>
</div></blockquote>
<p>from sklearn.preprocessing import FunctionTransformer</p>
<dl>
<dt>def add_extra_features(X, add_bedrooms_per_room=True):</dt><dd><p>rooms_per_household = X[:, rooms_ix] / X[:, household_ix]
population_per_household = X[:, population_ix] / X[:, household_ix]
if add_bedrooms_per_room:</p>
<blockquote>
<div><p>bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]
return np.c_[X, rooms_per_household, population_per_household,</p>
<blockquote>
<div><p>bedrooms_per_room]</p>
</div></blockquote>
</div></blockquote>
<dl class="simple">
<dt>else:</dt><dd><p>return np.c_[X, rooms_per_household, population_per_household]</p>
</dd>
</dl>
</dd>
<dt>attr_adder = FunctionTransformer(add_extra_features, validate=False,</dt><dd><p>kw_args={âadd_bedrooms_per_roomâ: False})</p>
</dd>
</dl>
<p>housing_extra_attribs = attr_adder.fit_transform(housing.values)</p>
<dl class="simple">
<dt>housing_extra_attribs = pd.DataFrame(</dt><dd><p>housing_extra_attribs,
columns=list(housing.columns)+[ârooms_per_householdâ, âpopulation_per_householdâ],
index=housing.index)</p>
</dd>
</dl>
<p>housing_extra_attribs.head()</p>
<p>from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler</p>
<dl>
<dt>num_pipeline = Pipeline([</dt><dd><blockquote>
<div><p>(âimputerâ, SimpleImputer(strategy=âmedianâ)),
(âattribs_adderâ, FunctionTransformer(add_extra_features, validate=False)),
(âstd_scalerâ, StandardScaler()),</p>
</div></blockquote>
<p>])</p>
</dd>
</dl>
<p>housing_num_tr = num_pipeline.fit_transform(housing_num)</p>
<dl class="simple">
<dt>try:</dt><dd><p>from sklearn.compose import ColumnTransformer</p>
</dd>
<dt>except ImportError:</dt><dd><p>from future_encoders import ColumnTransformer # Scikit-Learn &lt; 0.20</p>
</dd>
</dl>
<p>num_attribs = list(housing_num)
cat_attribs = [âocean_proximityâ]</p>
<dl>
<dt>full_pipeline = ColumnTransformer([</dt><dd><blockquote>
<div><p>(ânumâ, num_pipeline, num_attribs),
(âcatâ, OneHotEncoder(), cat_attribs),</p>
</div></blockquote>
<p>])</p>
</dd>
</dl>
<p>housing_prepared = full_pipeline.fit_transform(housing)</p>
<p>#%% Select and train a model</p>
<p>from sklearn.linear_model import LinearRegression</p>
<p>lin_reg = LinearRegression()
lin_reg.fit(housing_prepared, housing_labels)</p>
<p># letâs try the full preprocessing pipeline on a few training instances
some_data = housing.iloc[:5]
some_labels = housing_labels.iloc[:5]
some_data_prepared = full_pipeline.transform(some_data)</p>
<p>print(âPredictions:â, lin_reg.predict(some_data_prepared))
print(âLables:â, list(some_labels))</p>
<p>from sklearn.metrics import mean_squared_error</p>
<p>housing_predictions = lin_reg.predict(housing_prepared)
lin_mse = mean_squared_error(housing_labels, housing_predictions)
lin_rmse = np.sqrt(lin_mse)
print(lin_rmse)</p>
<p>from sklearn.metrics import mean_absolute_error</p>
<p>lin_mae = mean_absolute_error(housing_labels, housing_predictions)
print(lin_mae)</p>
<p>from sklearn.tree import DecisionTreeRegressor</p>
<p>tree_reg = DecisionTreeRegressor(random_state=42)
tree_reg.fit(housing_prepared, housing_labels)</p>
<dl class="simple">
<dt>DecisionTreeRegressor(criterion=âmseâ, max_depth=None, max_features=None,</dt><dd><p>max_leaf_nodes=None, min_impurity_decrease=0.0,
min_impurity_split=None, min_samples_leaf=1,
min_samples_split=2, min_weight_fraction_leaf=0.0,
presort=False, random_state=42, splitter=âbestâ)</p>
</dd>
</dl>
<p>housing_predictions = tree_reg.predict(housing_prepared)
tree_mse = mean_squared_error(housing_labels, housing_predictions)
tree_rmse = np.sqrt(tree_mse)
print(tree_rmse)</p>
<p>from sklearn.model_selection import cross_val_score</p>
<dl class="simple">
<dt>scores = cross_val_score(tree_reg, housing_prepared, housing_labels,</dt><dd><p>scoring=âneg_mean_squared_errorâ, cv=10)</p>
</dd>
</dl>
<p>tree_rmse_scores = np.sqrt(-scores)</p>
<dl class="simple">
<dt>def f_display_scores(scores):</dt><dd><p>print(âScores:â, scores)
print(âMean:â, scores.mean())
print(âStandard deviation:â, scores.std())</p>
</dd>
</dl>
<p>f_display_scores(tree_rmse_scores)</p>
<dl class="simple">
<dt>lin_scores = cross_val_score(lin_reg, housing_prepared, housing_labels,</dt><dd><p>scoring=âneg_mean_squared_errorâ, cv=10)</p>
</dd>
</dl>
<p>lin_rmse_scores = np.sqrt(-lin_scores)
f_display_scores(lin_rmse_scores)</p>
<p># Note: we specify n_estimators=10 to avoid a warning about the fact that the default value is going to change to 100 in Scikit-Learn 0.22.
from sklearn.ensemble import RandomForestRegressor</p>
<p>forest_reg = RandomForestRegressor(n_estimators=10, random_state=42)
forest_reg.fit(housing_prepared, housing_labels)</p>
<p>housing_predictions = forest_reg.predict(housing_prepared)
forest_mse = mean_squared_error(housing_labels, housing_predictions)
forest_rmse = np.sqrt(forest_mse)
print(forest_rmse)</p>
<p>from sklearn.model_selection import cross_val_score</p>
<dl class="simple">
<dt>forest_scores = cross_val_score(forest_reg, housing_prepared, housing_labels,</dt><dd><p>scoring=âneg_mean_squared_errorâ, cv=10)</p>
</dd>
</dl>
<p>forest_rmse_scores = np.sqrt(-forest_scores)
f_display_scores(forest_rmse_scores)</p>
<p>scores = cross_val_score(lin_reg, housing_prepared, housing_labels, scoring=âneg_mean_squared_errorâ, cv=10)
pd.Series(np.sqrt(-scores)).describe()</p>
<p>#%% Fine Tune Model
# ââââââââââââââââââââââââ
from sklearn.model_selection import GridSearchCV</p>
<dl>
<dt>param_grid = [</dt><dd><blockquote>
<div><p># try 12 (3Ã4) combinations of hyperparameters
{ân_estimatorsâ: [3, 10, 30], âmax_featuresâ: [2, 4, 6, 8]},
# then try 6 (2Ã3) combinations with bootstrap set as False
{âbootstrapâ: [False], ân_estimatorsâ: [3, 10], âmax_featuresâ: [2, 3, 4]},</p>
</div></blockquote>
<p>]</p>
</dd>
</dl>
<p>forest_reg = RandomForestRegressor(random_state=42)
# train across 5 folds, thatâs a total of (12+6)*5=90 rounds of training
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,</p>
<blockquote>
<div><p>scoring=âneg_mean_squared_errorâ, return_train_score=True)</p>
</div></blockquote>
<p>grid_search.fit(housing_prepared, housing_labels)</p>
<p>print(<a href="#id7"><span class="problematic" id="id8">grid_search.best_params_</span></a>)</p>
<p>print(<a href="#id9"><span class="problematic" id="id10">grid_search.best_estimator_</span></a>)</p>
<p># Letâs look at the score of each hyperparameter combination tested during the grid search:
cvres = <a href="#id11"><span class="problematic" id="id12">grid_search.cv_results_</span></a></p>
<dl class="simple">
<dt>for mean_score, params in zip(cvres[âmean_test_scoreâ], cvres[âparamsâ]):</dt><dd><p>print(np.sqrt(-mean_score), params)</p>
</dd>
</dl>
<p>feature_importances = <a href="#id13"><span class="problematic" id="id14">grid_search.best_estimator_</span></a>.feature_importances_
print(feature_importances)</p>
<p>extra_attribs = [ârooms_per_hholdâ, âpop_per_hholdâ, âbedrooms_per_roomâ]
#cat_encoder = cat_pipeline.named_steps[âcat_encoderâ] # old solution
cat_encoder = full_pipeline.named_transformers_[âcatâ]
cat_one_hot_attribs = list(cat_encoder.categories_[0])
attributes = num_attribs + extra_attribs + cat_one_hot_attribs
sorted(zip(feature_importances, attributes), reverse=True)</p>
<p>final_model = <a href="#id15"><span class="problematic" id="id16">grid_search.best_estimator_</span></a></p>
<p>X_test = strat_test_set.drop(âmedian_house_valueâ, axis=1)
y_test = strat_test_set[âmedian_house_valueâ].copy()</p>
<p>X_test_prepared = full_pipeline.transform(X_test)
final_predictions = final_model.predict(X_test_prepared)</p>
<p>final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)
print(final_rmse)</p>
<p>from scipy import stats</p>
<p>confidence = 0.95
squared_errors = (final_predictions - y_test) ** 2
mean = squared_errors.mean()
m = len(squared_errors)</p>
<dl class="simple">
<dt>CI = np.sqrt(stats.t.interval(confidence, m - 1,</dt><dd><p>loc=np.mean(squared_errors),
scale=stats.sem(squared_errors)))</p>
</dd>
</dl>
<p>print(âConfidence Intervalâ, CI)</p>
<p>&#64;</p>
</div>
</div>
<div class="section" id="key-concepts-and-summary">
<h2><span class="section-number">19.3. </span>Key Concepts and Summary<a class="headerlink" href="#key-concepts-and-summary" title="Permalink to this headline">Â¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Machine learning</p></li>
<li><p>The basic</p></li>
<li><p>Some central</p></li>
</ul>
</div>
</div>
<div class="section" id="self-check-questions">
<h2><span class="section-number">19.4. </span>Self-Check Questions<a class="headerlink" href="#self-check-questions" title="Permalink to this headline">Â¶</a></h2>
<div class="admonition-todo admonition" id="id2">
<p class="admonition-title">Todo</p>
<ul class="simple">
<li><p>Why is regression analysis machine learning. What type of machine
learning is it?</p></li>
</ul>
</div>
<dl class="citation">
<dt class="label" id="geron2019"><span class="brackets"><a class="fn-backref" href="#id1">Geron2019</a></span></dt>
<dd><p>Geron, Aurelien (2019), Hands-On Machine Learning with Scikit-Learn, Keras &amp; Tensorflow, OâReilly, 2nd edition.</p>
</dd>
</dl>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="Slides_MachineLearning_2.html" class="btn btn-neutral float-right" title="20. Machine Learning II: Categorization Algorithm" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Slides_Cake.html" class="btn btn-neutral float-left" title="18. Constrained Optimization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Juergen Jung

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>