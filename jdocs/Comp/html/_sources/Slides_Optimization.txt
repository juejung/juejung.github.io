Optimization
==============

Univariate function optimization
-------------------------------------------------------------------------------

Example function to be maximized
++++++++++++++++++++++++++++++++++++++++


.. code-block:: python

    import numpy as np
    import matplotlib.pyplot as plt
    import math as m
    import time  # Imports system time module to time your script
    
    plt.close('all')  # close all open figures
    



Here we want to optimize a univariate function:

.. math::

 f(x) = 4x^2e^{-2x}

We first define the function:


.. code-block:: python

    def f1simple(x):
        # gamma(2,3) density
        if (x < 0):
            return (0)
        if (x == 0):
            return (np.nan)
        y = np.exp(-2*x)
        return (4 * x**2 * y)
    



Next we define the same function but return :math:`f(x)`, :math:`f'(x)`, and
:math:`f''(x)`.

.. math::

 f'(x) = 4(2 \times xe^{-2x} +(-2)x^2e^{-2x}) = 8x(1-x)e^{-2x}

 f''(x) = 8e^{-2x}(1-4x+2x^2)


.. code-block:: python

    def f1(x):
        # gamma(2,3) density
        if (x < 0):
            return np.array([0, 0, 0])
        if (x == 0):
            return np.array([0, 0, np.nan])
        y = np.exp(-2.0*x)
        return np.array([4.0 * x**2.0 * y, \
          8.0 * x*(1.0-x)*y, \
          8.0*(1 - 4*x + 2 * x**2)*y])
    



Some algorithms that we'll encounter later will minimize a function. So if we
want to maximize our function we have to define it as a negated function, that
is:

.. math::
  g(x) = -f(x)

then

.. math::
    min \ g(x)

is the same as

.. math::
  max \ f(x).


.. code-block:: python

    def f1simpleNeg(x):
        # gamma(2,3) density
        if (x < 0):
            return (0)
        if (x == 0):
            return (np.nan)
        y = np.exp(-2*x)
        return (-(4 * x**2 * y))
    



Plotting the function is always a good idea!


.. code-block:: python

    xmin = 0.0
    xmax = 6.0
    xv = np.arange(xmin, xmax, (xmax - xmin)/200.0)
    fx = np.zeros(len(xv),float) # define column vector
    for i in range(len(xv)):
        fx[i] = f1(xv[i])[0]
    
    print("fx[0:10]= ", fx[0:10])
    
    fig, ax = plt.subplots()
    ax.plot(xv, fx)
    ax.plot(xv, np.zeros(len(xv)))
    plt.show()
    

::

    fx[0:10]=  [ 0.          0.00339035  0.01277165  0.02706275
    0.04530976  0.06667364
      0.09041885  0.11590306  0.14256769  0.16992939]
    
    

.. image:: _static/Slides_Optimization_figure5_1.*
   :width: 15 cm



Optimization methods
-------------------------------------------------------------------------------

Newton's method
+++++++++++++++++++++++++

In order to implement the Newton method we basically look for the root of a
first derivative so that :math:`f'(x) = 0`. We then use the root finding
algorithm from the previous section to find this point, or:

 .. math::
   x(n+1) = x(n) - \frac{f'(x(x))}{f''(x(n))}


.. code-block:: python

    def newton(f3, x0, tol = 1e-9, nmax = 100):
        # Newton's method for optimization, starting at x0
        # f3 is a function that given x returns the vector
        # (f(x), f'(x), f''(x)), for some f
        x = x0
        f3x = f3(x)
        n = 0
        while ((abs(f3x[1]) > tol) and (n < nmax)):
            x = x - f3x[1]/f3x[2]
            f3x = f3(x)
            n = n + 1
        if (n == nmax):
            print("newton failed to converge")
        else:
            return(x)
    



Golden section method
+++++++++++++++++++++++++++++++

The golden-section method works in one dimension only, but does not need the derivatives of the function.
However, the function still needs to be continuous.
In order to determine whether there is a local maximum we need three points:
if :math:`a<c<b` and :math:`f(a) \leq f(c)` and :math:`f(b) \leq f(c)` then there
must be a local maximum in the interval between :math:`[a,b].`

This method is very similar to the bisection method (root bracketing) from the previous section.
The algorithm proceeds in the following way:
Start with :math:`x_l<x_m<x_r` such that

 .. math::

   f(x_l) \le (x_m)

and

 .. math::

   f(x_r) \le f(x_m)

and

 .. math::
   \rho = \frac{1+\sqrt{5}}{2}.

Then check the following:

1. if :math:`x_r - x_l \le \epsilon` then stop
2. if :math:`x_r-x_m > x_m-x_l` then do :math:`(a)` otherwise do :math:`(b)`

  * (a) let :math:`y=x_m+(x_r-x_m)/(1+\rho)` if :math:`f(y) \ge  f(x_m)` then put :math:`x_l=x_m` and :math:`x_m=y` otherwise put :math:`x_r=y`
  * (b) let :math:`y=x_m+(x_m-x_l)/(1+\rho)` if :math:`f(y) \ge  f(x_m)` then put :math:`x_r=x_m` and :math:`x_m=y` otherwise put :math:`x_l=y`

3. go back to step 1


.. code-block:: python

    def gsection(ftn, xl, xm, xr, tol = 1e-9):
        # applies the golden-section algorithm to maximise ftn
        # we assume that ftn is a function of a single variable
        # and that x.l < x.m < x.r and ftn(x.l), ftn(x.r) <= ftn(x.m)
        #
        # the algorithm iteratively refines x.l, x.r, and x.m and
        # terminates when x.r - x.l <= tol, then returns x.m
        # golden ratio plus one
        gr1 = 1 + (1 + np.sqrt(5))/2
        #
        # successively refine x.l, x.r, and x.m
        fl = ftn(xl)
        fr = ftn(xr)
        fm = ftn(xm)
        while ((xr - xl) > tol):
            if ((xr - xm) > (xm - xl)):
                y = xm + (xr - xm)/gr1
                fy = ftn(y)
                if (fy >= fm):
                    xl = xm
                    fl = fm
                    xm = y
                    fm = fy
                else:
                    xr = y
                    fr = fy
            else:
                y = xm - (xm - xl)/gr1
                fy = ftn(y)
                if (fy >= fm):
                    xr = xm
                    fr = fm
                    xm = y
                    fm = fy
                else:
                    xl = y
                    fl = fy
        return(xm)
    



Maximize function: f
++++++++++++++++++++++++++++


.. code-block:: python

    print(" -----------------------------------")
    print(" Newton results ")
    print(" -----------------------------------")
    print(newton(f1, 0.25))
    print(newton(f1, 0.5))
    print(newton(f1, 0.75))
    print(newton(f1, 1.75))
    

::

     -----------------------------------
     Newton results
     -----------------------------------
    -1.25
    1.0
    0.999999999998
    14.4236788158
    
    




.. code-block:: python

    print(" -----------------------------------")
    print(" Golden section results ")
    print(" -----------------------------------")
    print(gsection(f1simple, 0.1, 0.25, 1.3))
    print(gsection(f1simple, 0.25, 0.5, 1.7))
    print(gsection(f1simple, 0.6, 0.75, 1.8))
    print(gsection(f1simple, 0.0, 2.75, 5.0))
    

::

     -----------------------------------
     Golden section results
     -----------------------------------
    1.00000001179
    1.00000001073
    0.999999992138
    1.00000000522
    
    



The function ``fmin`` is in the ``scipy.optimize`` library.


.. code-block:: python

    from scipy.optimize import fmin
    
    print(" -----------------------------------")
    print(" fmin results ")
    print(" -----------------------------------")
    print(fmin(f1simpleNeg, 0.25))
    print(fmin(f1simpleNeg, 0.5))
    print(fmin(f1simpleNeg, 0.75))
    print(fmin(f1simpleNeg, 1.75))
    

::

     -----------------------------------
     fmin results
     -----------------------------------
    Optimization terminated successfully.
             Current function value: -0.541341
             Iterations: 18
             Function evaluations: 36
    [ 1.]
    Optimization terminated successfully.
             Current function value: -0.541341
             Iterations: 16
             Function evaluations: 32
    [ 1.]
    Optimization terminated successfully.
             Current function value: -0.541341
             Iterations: 14
             Function evaluations: 28
    [ 0.99997559]
    Optimization terminated successfully.
             Current function value: -0.541341
             Iterations: 16
             Function evaluations: 32
    [ 1.00001221]
    
    




Finding the root of a function using minimization
-------------------------------------------------------------------------------

In a previous chapter on root searching of a function we used various methods
like the built in **fsolve** or **root** functions from the
`scipy.optimize <http://docs.scipy.org/doc/scipy/reference/optimize.html#module-scipy.optimize>`_
library.  Here's the example from the Root finding chapter again:

The example function for which we calculate the root, i.e., find :math:`x`
so that :math:`f(x) = 0` is defined as:


.. code-block:: python

    def func(x):
        s = np.log(x) - np.exp(-x)  # function: f(x)
        return s
    



We can solve for the zero (root) position with:


.. code-block:: python

    from scipy.optimize import root
    guess = 2
    result = root(func, guess) # starting from x = 2
    print(" ")
    print(" -------------- Root ------------")
    myroot = result.x  # Grab number from result dictionary
    print("The root of func is at {}".format(myroot))
    

::

    
     -------------- Root ------------
    The root of func is at [ 1.30979959]
    
    



In this section we set up the root finding problem as an optimization problem.
In order to do this we change the return value of the function slightly to

 .. math::
   \text{residual-error} = (f(x) - 0)^2.

So we are trying to find the value of
``x`` so that the residual error between :math:`f(x)` and its target value of
:math:`0` is as small as possible. The new function is defined as:


.. code-block:: python

    def func_root_min(x):
        s = np.log(x) - np.exp(-x)  # function: f(x)
        return (s**2)
    



We now invoke a minimizing algorithm to find this value of :math:`x`


.. code-block:: python

    from scipy.optimize import minimize
    guess = 2 # starting guess x = 2
    result = minimize(func_root_min, guess, method='Nelder-Mead')
    print(" ")
    print("-------------- Root ------------")
    myroot = result.x  # Grab number from result dictionary
    print("The root of func is at {}".format(myroot))
    

::

    
    -------------- Root ------------
    The root of func is at [ 1.30976562]
    
    




Multivariate optimization
-------------------------------------------------------------------------------

Here we want to optimize the following function **f3**


.. code-block:: python

    def f3simple(x):
        a = x[0]**2/2.0 - x[1]**2/4.0
        b = 2*x[0] - np.exp(x[1])
        f = np.sin(a)*np.cos(b)
        return(f)
    



Its negative version:


.. code-block:: python

    def f3simpleNeg(x):
        a = x[0]**2/2.0 - x[1]**2/4.0
        b = 2*x[0] - np.exp(x[1])
        f = -np.sin(a)*np.cos(b)
        return(f)
    



And the version that returns :math:`f(x)`, :math:`f'(x)` (i.e., the gradient),
and :math:`f''(x)` (i.e., the Hessian matrix):


.. code-block:: python

    def f3(x):
        a = x[0]**2/2.0 - x[1]**2/4.0
        b = 2*x[0] - np.exp(x[1])
        f = np.sin(a)*np.cos(b)
        f1 = np.cos(a)*np.cos(b)*x[0] - np.sin(a)*np.sin(b)*2
        f2 = -np.cos(a)*np.cos(b)*x[1]/2 +
    np.sin(a)*np.sin(b)*np.exp(x[1])
        f11 = -np.sin(a)*np.cos(b)*(4 + x[0]**2) + np.cos(a)*np.cos(b) \
            - np.cos(a)*np.sin(b)*4*x[0]
        f12 = np.sin(a)*np.cos(b)*(x[0]*x[1]/2.0 + 2*np.exp(x[1])) \
            + np.cos(a)*np.sin(b)*(x[0]*np.exp(x[1]) + x[1])
        f22 = -np.sin(a)*np.cos(b)*(x[1]**2/4.0 + np.exp(2*x[1])) \
            - np.cos(a)*np.cos(b)/2.0 -
    np.cos(a)*np.sin(b)*x[1]*np.exp(x[1]) \
            + np.sin(a)*np.sin(b)*np.exp(x[1])
        # Function f3 returns: f(x), f'(x), and f''(x)
        return (f, np.array([f1, f2]), np.array([[f11, f12], [f12, f22]]))
    



We next plot the function:


.. code-block:: python

    from mpl_toolkits.mplot3d import Axes3D
    
    fig = plt.figure(figsize=(14, 16))
    ax = plt.gca(projection='3d')
    
    X = np.arange(-3, 3, .1)
    Y = np.arange(-3, 3, .1)
    X, Y = np.meshgrid(X, Y)
    
    Z = np.zeros((len(X),len(Y)),float)
    for i in range(len(X)):
        for j in range(len(Y)):
            Z[i][j] = f3simple([X[i][j],Y[i][j]])
    
    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, \
        cmap=plt.cm.jet, linewidth=0, antialiased=False)
    plt.show()
    

.. image:: _static/Slides_Optimization_Fig_Surface_1.*
   :width: 12 cm




Multivariate Newton method
++++++++++++++++++++++++++++++++++++++++


.. code-block:: python

    def newtonMult(f3, x0, tol = 1e-9, nmax = 100):
        # Newton's method for optimisation, starting at x0
        # f3 is a function that given x returns the list
        # {f(x), grad f(x), Hessian f(x)}, for some f
        x = x0
        f3x = f3(x)
        n = 0
        while ((max(abs(f3x[1])) > tol) and (n < nmax)):
            x = x - np.linalg.solve(f3x[2], f3x[1])
            f3x = f3(x)
            n = n + 1
        if (n == nmax):
            print("newton failed to converge")
        else:
            return(x)
    



Compare the Newton method with the built in ``fmin`` method in
``scipy.optimize``.  We use various starting values to see whether we can find
more than one optimum.


.. code-block:: python

    from scipy.optimize import fmin
    
    for x0 in np.arange(1.4, 1.6, 0.1):
        for y0 in np.arange(0.4, 0.7, 0.1):
            # This algorithm requires f(x), f'(x), and f''(x)
            print("Newton: f3  " + str([x0,y0]) + ' --> ' +
    str(newtonMult(f3, \
                np. array([x0,y0]))))
    
            print("fmin: f3 " + str([x0,y0]) + ' --> ' \
                + str(fmin(f3simpleNeg, np.array([x0,y0]))))
    
            print(" ----------------------------------------- ")
    

::

    Newton: f3  [1.3999999999999999, 0.40000000000000002] --> [ 0.04074437
    -2.50729047]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 47
             Function evaluations: 89
    fmin: f3 [1.3999999999999999, 0.40000000000000002] --> [ 2.0307334
    1.40155445]
     -----------------------------------------
    Newton: f3  [1.3999999999999999, 0.5] --> [ 0.11797341  3.34466147]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 50
             Function evaluations: 93
    fmin: f3 [1.3999999999999999, 0.5] --> [ 2.03072555  1.40154756]
     -----------------------------------------
    Newton: f3  [1.3999999999999999, 0.59999999999999998] --> [-1.5531627
    6.0200129]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 43
             Function evaluations: 82
    fmin: f3 [1.3999999999999999, 0.59999999999999998] --> [ 2.03068816
    1.40151998]
     -----------------------------------------
    Newton: f3  [1.5, 0.40000000000000002] --> [ 2.83714224  5.35398196]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 48
             Function evaluations: 90
    fmin: f3 [1.5, 0.40000000000000002] --> [ 2.03067611  1.40149298]
     -----------------------------------------
    Newton: f3  [1.5, 0.5] --> [ 0.04074437 -2.50729047]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 42
             Function evaluations: 82
    fmin: f3 [1.5, 0.5] --> [ 2.03071509  1.40155165]
     -----------------------------------------
    Newton: f3  [1.5, 0.59999999999999998] --> [  9.89908350e-10
    1.36639196e-09]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 43
             Function evaluations: 82
    fmin: f3 [1.5, 0.59999999999999998] --> [ 2.0307244   1.40153761]
     -----------------------------------------
    Newton: f3  [1.6000000000000001, 0.40000000000000002] --> [-0.55841026
    -0.78971136]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 47
             Function evaluations: 88
    fmin: f3 [1.6000000000000001, 0.40000000000000002] --> [ 2.0307159
    1.40150964]
     -----------------------------------------
    Newton: f3  [1.6000000000000001, 0.5] --> [-0.29022131 -0.23047994]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 44
             Function evaluations: 80
    fmin: f3 [1.6000000000000001, 0.5] --> [ 2.03074135  1.40151521]
     -----------------------------------------
    Newton: f3  [1.6000000000000001, 0.59999999999999998] --> [-1.55294692
    -3.33263763]
    Optimization terminated successfully.
             Current function value: -1.000000
             Iterations: 42
             Function evaluations: 80
    fmin: f3 [1.6000000000000001, 0.59999999999999998] --> [ 2.03069759
    1.40155333]
     -----------------------------------------
    
    


