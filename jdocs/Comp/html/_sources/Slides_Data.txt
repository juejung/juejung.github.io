Working with data
===============================================================

Read in small data set from a comma separated (.csv) file
-------------------------------------------------------------

We use the ``Pandas`` package for data work. It allows easy organization of
data in the spirit of ``DataFrame`` concept in **R**.
We need to install ``Pandas`` using ``pip install pandas`` from command line.
We will then import the ``Pandas`` library using the ``import`` statement.

The first dataset that we are working with is a comma separated file, called
:download:`Lecture_Data_Excel_a.csv <Lecture_Data/Lecture_Data_Excel_a.csv>`.
We first import this file using the ``pd.read_csv`` command from the ``Pandas`` library.


.. code-block:: python

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from ggplot import *
    from scipy import stats as st
    import math as m
    import seaborn as sns
    import time  # Imports system time module to time your script
    
    plt.close('all')  # close all open figures
    




.. code-block:: python

    # Read in small data from .csv file
    # Filepath
    filepath = 'Lecture_Data/'
    # In windows you can also specify the absolute path to your data file
    # filepath =
    'C:/Dropbox/Towson/Teaching/3_ComputationalEconomics/Lectures/Lecture_Data/'
    
    # ------------- Load data --------------------
    df = pd.read_csv(filepath + 'Lecture_Data_Excel_a.csv',
    dtype={'Frequency': float})
    
    # Alternatively we could have used:
    # df = pd.read_table(filepath + 'Lecture_Data_Excel_a.csv'), sep=',')
    
    # Let's have a look at it, it's a nested list
    print(df)
    

::

                     Area  Frequency  Unnamed: 2  Unnamed: 3
    0          Accounting         73         NaN         NaN
    1             Finance         52         NaN         NaN
    2  General management         36         NaN         NaN
    3     Marketing sales         64         NaN         NaN
    4               other         28         NaN         NaN
    
    



Drop column 3 and 4 and let's have a look at the data again.


.. code-block:: python

    df = df.drop('Unnamed: 2', 1)
    df = df.drop('Unnamed: 3', 1)
    # Let's have a look at it, it's a nested list
    print(df)
    

::

                     Area  Frequency
    0          Accounting         73
    1             Finance         52
    2  General management         36
    3     Marketing sales         64
    4               other         28
    
    



We next generate a new variable called ``relFrequency`` that contains the relative
frequencies.


.. code-block:: python

    # Make new column with relative frequency
    df['relFrequency'] = df['Frequency']/df['Frequency'].sum()
    # Let's have a look at it, it's a nested list
    print(df)
    

::

                     Area  Frequency  relFrequency
    0          Accounting         73      0.288538
    1             Finance         52      0.205534
    2  General management         36      0.142292
    3     Marketing sales         64      0.252964
    4               other         28      0.110672
    
    



Now we grab the relative frequencies out of the DataFrame into a numpy array.


.. code-block:: python

    xv = df['relFrequency'].values
    



Making simple graphs from our data
--------------------------------------

Bar chart
++++++++++++++++

We first make a bar chart of the absolute frequencies.


.. code-block:: python

    fig, ax = plt.subplots()
    #
    ax.bar(1. + np.arange(len(xv)), xv, align='center')
    # Annotate with text
    ax.set_xticks(1. + np.arange(len(xv)))
    for i, val in enumerate(xv):
        ax.text(i+1, val/2, str(round(val, 2)*100)+'%', va='center',
    ha='center', color='black')
    ax.set_ylabel('%')
    ax.set_title('Relative Frequency Barchart')
    plt.show()
    # We can also save this graph as a pdf
    #savefig('./Graphs/fig1.pdf')
    

.. image:: _static/Slides_Data_Barchart_1.*
   :width: 12 cm



This simple command plots the barchart into a window and saves it as
**fig1.pdf** into a subfolder called ``Graphs``. Don't forget to first make
this subfolder, otherwise ``Python`` will throw an error when it can't find the
folder.

Pie chart
++++++++++++++++

We next make a pie chart using the relative frequencies stored in vector ``xv``.


.. code-block:: python

    fig, ax = plt.subplots()
    ax.pie(xv, labels = np.round(xv, 2)*100, shadow=True)
    # Annotate with text
    ax.set_title('Relative Frequency Pie Chart')
    plt.show()
    

.. image:: _static/Slides_Data_Piechart_1.*
   :width: 12 cm



Histogram
++++++++++++++++

Next we use a new data file called **Lecture_Data_Excel_b.csv**. This data file
contains data on height age and other variables. We first make a histogram of
the continuous variable Height.
The actual command for the histogram is ``hist``. It returns three variables
``prob``, ``bins``, and ``patches``. We use them in the following to add
information to the histogram.


.. code-block:: python

    df = pd.read_csv(filepath + 'Lecture_Data_Excel_b.csv')
    
    heightv = df['Height'].values
    
    # Initialize
    N = len(heightv) # number of obs.
    B = 8           # number of bins in histogram
    
    fig, ax = plt.subplots(2,2)
    prob, bins, patches = ax[0,0].hist(heightv, bins=B, align='mid' )
    # Annotate with text
    for i, p in enumerate(prob):
        percent = int(float(p)/N*100)
        # only annotate non-zero values
        if percent:
            ax[0,0].text(bins[i]*1.5, p/2.0, str(percent)+'%',
    rotation=45, va='bottom', ha='center')
    ax[0,0].set_xlabel('Height groups')
    ax[0,0].set_ylabel('Number of obs')
    ax[0,0].set_title('Histogram of Height')
    ax[0,0].set_xlim(min(heightv),max(heightv))
    #
    df['Height'].hist(bins = B, ax = ax[0,1], color = 'k', alpha = 0.3)
    #
    df['AverageMathSAT'].hist(bins = B, ax = ax[1,0], color = 'g', alpha =
    0.3)
    ax[1,0].set_title('Math SAT')
    #
    plt.subplots_adjust(wspace = 0.3, hspace = 0.5)
    plt.show()
    

.. image:: _static/Slides_Data_Histogram_1.*
   :width: 12 cm



Another way to graph a histogram is by using the powerful ``ggplot`` package
which is a translation of **R's** ``ggplot2`` package into **Python**. It
follows a completely different graphing philosophy based on the **grammar of
grapics**


.. code-block:: python

    # Using ggplot package
    print( ggplot(aes(x='Height'), data = df) + geom_histogram() +
    ggtitle("Histogram of Height using ggplot") + labs("Height", "Freq"))
    plt.show(1)
    

::

    <ggplot: (10340591)>
    
    

.. image:: _static/Slides_Data_Boxplot_1.*
   :width: 12 cm



Boxplots
++++++++

A boxplot of height is made as follows:


.. code-block:: python

    fig, ax = plt.subplots()
    ax.boxplot(heightv)
    # Annotate with text
    ax.set_title('Boxplot of Height')
    plt.show()
    

.. image:: _static/Slides_Data_Boxplot_1.*
   :width: 12 cm



Summary statistics
-----------------------------------------------------------

We next go through some basic summary statistics.

Measures of central tendency
+++++++++++++++++++++++++++++++++++

A quick way to produce summary statistics of our data set is to use ``panda's``
command ``describe``.


.. code-block:: python

    print(df.describe())
    

::

              Height   Response  AverageMathSAT        Age     Female
    Education
    count  60.000000  60.000000       60.000000  60.000000  60.000000
    60.000000
    mean    6.816667   3.966667      493.666667  20.933333   0.333333
    3.016667
    std     3.571743   0.662984       34.018274   1.176992   0.475383
    1.096863
    min     1.100000   3.000000      370.000000  20.000000   0.000000
    1.000000
    25%     3.950000   4.000000      478.500000  20.000000   0.000000
    2.000000
    50%     6.850000   4.000000      507.000000  20.000000   0.000000
    3.000000
    75%     9.650000   4.000000      509.250000  22.000000   1.000000
    4.000000
    max    12.700000   5.000000      542.000000  23.000000   1.000000
    5.000000
    
    



Or we can produce the same statistics by hand. We first calculate the mean,
median and mode. Note that ``mode`` is part of the ``stats`` package which was
imported as ``st`` using: ``from scipy import stats as st``. Now we have to add
``st`` to the mode command: ``st.mode(heightv)`` in order to call it. The other
commands are part of the ``numpy`` package so they have the prefix ``np.``


.. code-block:: python

    N = len(heightv)   # Number of observations (sample size)
    print(" -----------------------------------------")
    # Mean - Median - Mode
    print("Mean(height)= " + str(np.mean(heightv)))
    print("Median(height)= " + str(np.median(heightv)))
    print("Mode(height)= " + str(st.mode(heightv))) # Mode (value with
    highest frequency)
    

::

     -----------------------------------------
    Mean(height)= 6.81666666667
    Median(height)= 6.85
    Mode(height)= (array([ 1.1]), array([ 1.]))
    
    



We can also just summarize a variable using the ``st.describe`` command from
the ``stats`` package.


.. code-block:: python

    # Summary stats
    print("Summary stats: " + str(st.describe(heightv)))
    

::

    Summary stats: (60, (1.1000000000000001, 12.699999999999999),
    6.8166666666666655, 12.757344632768362, 0.01909961978127301,
    -1.1611486008105545)
    
    



Measures of dispersion
+++++++++++++++++++++++++++++

We now calculate the range, variance and standard deviations. Remember that for
variance and standard deviation there is a distinction between population and
sample.


.. code-block:: python

    print(" -----------------------------------------")
    print("Range= " + str(max(heightv)-min(heightv)))   # returns smallest
    a largest element
    print("Population variance = " + str(np.sum((heightv-
    np.mean(heightv))**2)/N))    # population variance
    print("Sample variance     = " + str(np.sum((heightv-
    np.mean(heightv))**2)/(N-1)))      # sample variance
    print("Pop.   standard dev = " + str(np.sqrt(np.sum((heightv-
    np.mean(heightv))**2)/N)))
    print("Sample standard dev = " + str(np.sqrt(np.sum((heightv-
    np.mean(heightv))**2)/(N-1))))
    #or simply
    print("Pop. standard dev = " + str(np.std(heightv)))
    print(" -----------------------------------------")
    

::

     -----------------------------------------
    Range= 11.6
    Population variance = 12.5447222222
    Sample variance     = 12.7573446328
    Pop.   standard dev = 3.54185293628
    Sample standard dev = 3.5717425205
    Pop. standard dev = 3.54185293628
     -----------------------------------------
    
    



Measures of relative standing
+++++++++++++++++++++++++++++

Percentiles are calculated as follows:


.. code-block:: python

    print("1 quartile (25th percentile) = " +
    str(st.scoreatpercentile(heightv, 25)))
    print("2 quartile (50th percentile) = " +
    str(st.scoreatpercentile(heightv, 50)))
    print("3 quartile (75th percentile) = " +
    str(st.scoreatpercentile(heightv, 75)))
    # Inter quartile rante: Q3-Q1 or P_75-P_25
    print("IQR = (P75-P25) = " + str(st.scoreatpercentile(heightv,
    75)-st.scoreatpercentile(heightv, 25)))
    print(" -----------------------------------------")
    

::

    1 quartile (25th percentile) = 3.95
    2 quartile (50th percentile) = 6.85
    3 quartile (75th percentile) = 9.65
    IQR = (P75-P25) = 5.7
     -----------------------------------------
    
    




Measures of linear relationship
---------------------------------------------------------

Covariance
+++++++++++++++++

We first grab two variables ``heightv`` and ``agev`` from the dataframe and
define them as ``numpy`` arrays.


.. code-block:: python

    xv = df['Age'].values
    yv = df['Height'].values
    n = len(xv)
    



We then go ahead and calculate the covariance.


.. code-block:: python

    print("Population covariance= " + str(np.sum((xv-np.mean(xv))*(yv-
    np.mean(yv)))/n))          # Population covariance
    print("Sample covariance= " + str(np.sum((xv-np.mean(xv))*(yv-
    np.mean(yv)))/(n-1)))       # sample covariance
    # or simply
    print("Sample covariance= " + str(df.Age.cov(df.Height)))
    

::

    Population covariance= -0.158888888889
    Sample covariance= -0.161581920904
    Sample covariance= -0.161581920904
    
    



Correlation coefficient
++++++++++++++++++++++++++++++

We can calculate the correlation coefficient by hand or using the ``pandas``
toolbox command.


.. code-block:: python

    print("Correlation coefficient= " + \
            str((np.sum((xv-np.mean(xv))*(yv-
    np.mean(yv)))/n)/(np.std(xv)*np.std(yv))))
    print("Correlation coefficient= " + str(df.Age.corr(df.Height)))
    

::

    Correlation coefficient= -0.0384360741963
    Correlation coefficient= -0.0384360741963
    
    



Regression line
++++++++++++++++++++++

Example 1: Simple example
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We first generate some data. A variable ``x`` and a variable ``y``.


.. code-block:: python

    # Define data. 2 vectors xv and yv
    xv = np.arange(1,9,1)
    yv = np.array([6,1,9,5,17,12,14,15])
    



We then "cast" these into a ``pandas`` DataFrame.


.. code-block:: python

    # Define data frame
    df = pd.DataFrame(np.array([xv, yv]).T, columns = ['x', 'y'])
    



We fist make a **scatterplot** with least squares trend line: `y = \beta_0 +
\beta_1 * x + \epsilon`.


.. code-block:: python

    p = np.polyfit(xv, yv, 1)
    print("p = " + str(p))
    
    # scatterplot
    fig, ax = plt.subplots()
    ax.set_title('Linear regression with polyfit()')
    ax.plot(xv, yv, 'o', label = 'data')
    ax.plot(xv, np.polyval(p,xv),'-', label = 'Linear regression')
    ax.legend(['Data', 'OLS'], loc='best')
    plt.show()
    

::

    p = [ 1.77380952  1.89285714]
    
    

.. image:: _static/Slides_Data_Scatter1_1.*
   :width: 12 cm



We then run the same regression using a more general method called ``ols``
which is part of ``pandas``. When calling the ``ols`` function
you need to add the module name (``pandas`` was imported as ``pd``)
in front of it: ``pd.ols()``. You then define the independent variable ``y``
and the dependent variables ``x's``.


.. code-block:: python

    # Run OLS regression
    res = pd.ols(y=df['y'], x=df['x'])
    # Show coefficient estimates
    print(res)
    print('Parameters = ' + str(res.beta))
    

::

    
    -------------------------Summary of Regression
    Analysis-------------------------
    
    Formula: Y ~ <x> + <intercept>
    
    Number of Observations:         8
    Number of Degrees of Freedom:   2
    
    R-squared:         0.6093
    Adj R-squared:     0.5442
    
    Rmse:              3.7578
    
    F-stat (1, 6):     9.3583, p-value:     0.0222
    
    Degrees of Freedom: model 1, resid 6
    
    -----------------------Summary of Estimated
    Coefficients------------------------
          Variable       Coef    Std Err     t-stat    p-value    CI 2.5%
    CI 97.5%
    --------------------------------------------------------------------------------
                 x     1.7738     0.5798       3.06     0.0222     0.6373
    2.9103
         intercept     1.8929     2.9281       0.65     0.5419    -3.8461
    7.6318
    ---------------------------------End of
    Summary---------------------------------
    
    Parameters = x            1.773810
    intercept    1.892857
    dtype: float64
    
    



Finally, we use the model to make a prediction of ``y`` when ``x=8.5``.


.. code-block:: python

    # Make prediction for x = 8.5
    betas = res.beta
    print("Prediction of y for x=8.5 is: " + str(np.sum(betas *
    np.array([1,8.5]))))
    

::

    Prediction of y for x=8.5 is: 17.8630952381
    
    



We can also use the results from the ``ols`` command to make a scatterplot with
the trendline through it.


.. code-block:: python

    fig, ax = plt.subplots()
    ax.set_title('Linear Regression with Fitted Values')
    ax.plot(xv, yv, 'o', label = 'data')
    ax.plot(xv, res.y_predict,'k-', label = 'Linear regression')
    ax.legend(['Data', 'OLS'], loc='best')
    plt.show()
    

.. image:: _static/Slides_Data_Scatter2_1.*
   :width: 12 cm



Example 2: OLS with categorical (dummy variables)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The next example is a bit more involved as we increase the number of
explanatory variables. In addition, some explanatory variables are categorical
variables. In order to use them in our OLS regression we first have to make so
called **dummy** variables (i.e. indicator variables that are either 0 or 1).


.. code-block:: python

    df = pd.read_csv(filepath + 'Lecture_Data_Excel_b.csv')
    
    dummies = pd.get_dummies(df['Race'], prefix = 'd')
    df = df.join(dummies)
    print(df.head())
    

::

       Height  Response  AverageMathSAT  Age  Female  Education  Race
    d_Blk  \
    0     1.1         3             370   20       0          2  Hisp
    0
    1     1.2         4             393   20       0          4   Wht
    0
    2     1.3         4             413   20       0          4   Blk
    1
    3     1.4         5             430   20       0          4   Wht
    0
    4     1.5         3             440   20       0          2   Mex
    0
    
       d_Hisp  d_Mex  d_Oth  d_Wht
    0       1      0      0      0
    1       0      0      0      1
    2       0      0      0      0
    3       0      0      0      1
    4       0      1      0      0
    
    



We next use ``ols`` again to run the regression. At the end we make a
prediction based on some values for the independent variables ``x1``, ``x2``,
etc.


.. code-block:: python

    res = pd.ols(y=df['Height'], x=df[['Age','Education','Female', \
        'd_Blk','d_Hisp','d_Mex','d_Wht']])
    
    # Show coefficient estimates
    print(res)
    print('Parameters = ' + str(res.beta))
    
    # Prediction: size of 8.5 will produce a math score of ...
    betas = res.beta
    print("\nPrediction: " +str(np.sum(betas *
    np.array([1,2.5,22,1,0,0,1,0]))))
    

::

    
    -------------------------Summary of Regression
    Analysis-------------------------
    
    Formula: Y ~ <Age> + <Education> + <Female> + <d_Blk> + <d_Hisp> +
    <d_Mex>
                 + <d_Wht> + <intercept>
    
    Number of Observations:         60
    Number of Degrees of Freedom:   8
    
    R-squared:         0.9437
    Adj R-squared:     0.9361
    
    Rmse:              0.9030
    
    F-stat (7, 52):   124.4348, p-value:     0.0000
    
    Degrees of Freedom: model 7, resid 52
    
    -----------------------Summary of Estimated
    Coefficients------------------------
          Variable       Coef    Std Err     t-stat    p-value    CI 2.5%
    CI 97.5%
    --------------------------------------------------------------------------------
               Age     1.8944     0.1241      15.27     0.0000     1.6512
    2.1375
         Education    -0.0514     0.1265      -0.41     0.6864    -0.2994
    0.1967
            Female     8.7359     0.3077      28.39     0.0000     8.1327
    9.3391
             d_Blk    -0.3784     0.6855      -0.55     0.5834    -1.7220
    0.9653
            d_Hisp    -1.0267     0.7901      -1.30     0.1995    -2.5753
    0.5218
    --------------------------------------------------------------------------------
             d_Mex    -0.5749     0.6863      -0.84     0.4060    -1.9200
    0.7701
             d_Wht     0.0816     0.6808       0.12     0.9050    -1.2528
    1.4161
         intercept   -35.2714     2.7510     -12.82     0.0000   -40.6634
    -29.8794
    ---------------------------------End of
    Summary---------------------------------
    
    Parameters = Age           1.894377
    Education    -0.051375
    Female        8.735906
    d_Blk        -0.378358
    d_Hisp       -1.026743
    d_Mex        -0.574949
    d_Wht         0.081628
    intercept   -35.271435
    dtype: float64
    
    Prediction: 193.659146277
    
    



Homework
-------------------------------------------------------------------------------

:doc:`./Lecture_Data/Homework/Homework_Data`

