Working with data
===============================================================

We will be working with two data sets.
:download:`Lecture_Data_Excel_a.csv <Lecture_Data/Lecture_Data_Excel_a.csv>`.
:download:`Lecture_Data_Excel_b.csv <Lecture_Data/Lecture_Data_Excel_b.csv>`.

Read in small data set from a comma separated (.csv) file
-------------------------------------------------------------

We use the ``Pandas`` package for data work.  We can import the ``Pandas``
library using the ``import`` statement.  ``Pandas`` allows easy organization of
data in the spirit of ``DataFrame`` concept in **R**. You can think of a
**DataFrame** as an array with labels similar to an Excel spreadsheet.

A **DataFrame** can be created in the following ways:

 * From another **DataFrame**.
 * From a ``numpy`` array.
 * From another ``pandas`` data structure called ``series`` which is basically
   a one dimensional **DataFrame**, i.e., a column or row of a matrix.
 * From a file like a CSV file etc.

The first dataset that we are working with is a comma separated file, called
:download:`Lecture_Data_Excel_a.csv <Lecture_Data/Lecture_Data_Excel_a.csv>`.
We first import this file using the ``pd.read_csv`` command from the ``Pandas`` library.


.. code-block:: python

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from ggplot import *
    from scipy import stats as st
    import math as m
    import seaborn as sns
    import time  # Imports system time module to time your script
    
    plt.close('all')  # close all open figures
    




.. code-block:: python

    # Read in small data from .csv file
    # Filepath
    filepath = 'Lecture_Data/'
    # In windows you can also specify the absolute path to your data file
    # filepath =
    'C:/Dropbox/Towson/Teaching/3_ComputationalEconomics/Lectures/Lecture_Data/'
    
    # ------------- Load data --------------------
    df = pd.read_csv(filepath + 'Lecture_Data_Excel_a.csv',
    dtype={'Frequency': float})
    
    # Let's have a look at it, it's a nested list
    print(df)
    

::

                     Area  Frequency  Unnamed: 2  Unnamed: 3
    0          Accounting         73         NaN         NaN
    1             Finance         52         NaN         NaN
    2  General management         36         NaN         NaN
    3     Marketing sales         64         NaN         NaN
    4               other         28         NaN         NaN
    
    




Alternatively we could have use the following command::

    df = pd.read_table(filepath + 'Lecture_Data_Excel_a.csv'), sep=',')

``Pandas`` also has a built in Excel file reader that you can call as follows::

    # Read entire excel spreadsheet
    xl = pd.ExcelFile("Lecture_Data_Excel_a.xlsx")

    # Check how many sheets we have inside the excel file
    xl.sheet_names
    [u'Sheet1', u'Sheet2', u'Sheet3']

    # Pick one sheet and define it as your DataFrame by parsing a sheet
    df = xl.parse("Sheet1")

The **DataFrame** has an index for each row and a column header. We can query a
**DataFrame** as follows:


.. code-block:: python

    print('Shape', df.shape)
    print('-------------------------')
    print('Number of rows', len(df))
    print('-------------------------')
    print('Column headers', df.columns)
    print('-------------------------')
    print('Data types', df.dtypes)
    print('-------------------------')
    print('Index', df.index)
    print('-------------------------')
    

::

    Shape (5, 4)
    -------------------------
    Number of rows 5
    -------------------------
    Column headers Index(['Area', 'Frequency', 'Unnamed: 2', 'Unnamed:
    3'], dtype='object')
    -------------------------
    Data types Area           object
    Frequency     float64
    Unnamed: 2    float64
    Unnamed: 3    float64
    dtype: object
    -------------------------
    Index Int64Index([0, 1, 2, 3, 4], dtype='int64')
    -------------------------
    
    



Drop column 3 and 4 and let's have a look at the data again.


.. code-block:: python

    df = df.drop('Unnamed: 2', 1)
    df = df.drop('Unnamed: 3', 1)
    # Let's have a look at it, it's a nested list
    print(df)
    

::

                     Area  Frequency
    0          Accounting         73
    1             Finance         52
    2  General management         36
    3     Marketing sales         64
    4               other         28
    
    



Sometimes column headers have white spaces in them. It is probably a good idea
to remove all white spaces from header names. You can use the ``.strip()``
command to accomplish this:


.. code-block:: python

    df = df.rename(columns=lambda x: x.strip())
    



Here we use the ``lambda`` command which is an inline function. Basically it is
a shortcut so that we don't have to write a separate function using the ``def``
keyword etc. It's a shortcut for functions that you want to define 'on the fly.'


We next generate a new variable called ``relFrequency`` that contains the relative
frequencies.


.. code-block:: python

    # Make new column with relative frequency
    df['relFrequency'] = df['Frequency']/df['Frequency'].sum()
    # Let's have a look at it, it's a nested list
    print(df)
    

::

                     Area  Frequency  relFrequency
    0          Accounting         73      0.288538
    1             Finance         52      0.205534
    2  General management         36      0.142292
    3     Marketing sales         64      0.252964
    4               other         28      0.110672
    
    



Now we grab the relative frequencies out of the DataFrame into a numpy array.


.. code-block:: python

    xv = df['relFrequency'].values
    print('Array xv is:', xv)
    

::

    Array xv is: [ 0.28853755  0.2055336   0.14229249  0.25296443
    0.11067194]
    
    




.. code-block:: python

    # Let us make additional columns
    df['random'] = df['Frequency']/df['Frequency'].sum()
    # Let's have a look at it, it's a nested list
    print(df)
    

::

                     Area  Frequency  relFrequency    random
    0          Accounting         73      0.288538  0.288538
    1             Finance         52      0.205534  0.205534
    2  General management         36      0.142292  0.142292
    3     Marketing sales         64      0.252964  0.252964
    4               other         28      0.110672  0.110672
    
    



Some Pandas tricks
-------------------------------------------------------------------------------

Renaming variables
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

For renaming column names we use a dictionary as follows.


.. code-block:: python

    df = df.rename(columns={'Area': 'area',
        'Frequency': 'absFrequency'})
    
    print(df.head(3))
    

::

                     area  absFrequency  relFrequency    random
    0          Accounting            73      0.288538  0.288538
    1             Finance            52      0.205534  0.205534
    2  General management            36      0.142292  0.142292
    
    



The ``head(x)`` method allows us to print ``x`` rows from the top of the
**DataFrame**, whereas the ``tail(x)`` method does the same from the bottom of
the DataFrame.


Adding a new column
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


.. code-block:: python

    # Drop a column first so everything fits
    df = df.drop('random', 1)
    
    # Create a new column with some missing values
    df['team'] = pd.Series([2,4, np.NaN,6,10], index=df.index)
    
    # or insert new column at specific location
    df.insert(loc=2, column='position', value=[np.NaN,1, np.NaN,1,3])
    
    print(df)
    

::

                     area  absFrequency  position  relFrequency  team
    0          Accounting            73       NaN      0.288538     2
    1             Finance            52         1      0.205534     4
    2  General management            36       NaN      0.142292   NaN
    3     Marketing sales            64         1      0.252964     6
    4               other            28         3      0.110672    10
    
    



Missing values of NaN's
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

**Count NaNs**

Count the number of rows with missing values.


.. code-block:: python

    nans = df.shape[0] - df.dropna().shape[0]
    print('%d rows have missing values' % nans)
    

::

    2 rows have missing values
    
    



**Select rows with NaNs or overwrite NaNs**

If you want to select the rows with missing values you can index them with the
``isnull()`` method.


.. code-block:: python

    print(df[df.isnull().any(axis=1)])
    

::

                     area  absFrequency  position  relFrequency  team
    0          Accounting            73       NaN      0.288538     2
    2  General management            36       NaN      0.142292   NaN
    
    



Or if you want to search specific columns for missing values you can do


.. code-block:: python

    print(df[df['team'].isnull()])
    

::

                     area  absFrequency  position  relFrequency  team
    2  General management            36       NaN      0.142292   NaN
    
    



Or if you want to grab all the rows without missing observations you can index
the dataframe with the ``notnull()`` method.


.. code-block:: python

    print(df[df['team'].notnull()])
    

::

                  area  absFrequency  position  relFrequency  team
    0       Accounting            73       NaN      0.288538     2
    1          Finance            52         1      0.205534     4
    3  Marketing sales            64         1      0.252964     6
    4            other            28         3      0.110672    10
    
    



**Delete rows with NaNs**


.. code-block:: python

    print(df.dropna())
    

::

                  area  absFrequency  position  relFrequency  team
    1          Finance            52         1      0.205534     4
    3  Marketing sales            64         1      0.252964     6
    4            other            28         3      0.110672    10
    
    



You can also reassign it to the orignal DataFrame or assign
a new one.


.. code-block:: python

    df2 = df.dropna()
    print(df2)
    

::

                  area  absFrequency  position  relFrequency  team
    1          Finance            52         1      0.205534     4
    3  Marketing sales            64         1      0.252964     6
    4            other            28         3      0.110672    10
    
    



**Overwriting NaN with specific values**
If we want to overwrite the ``NaN`` entries with values we can use the
``fillna()`` method. In this example we replace all missing values with zeros.


.. code-block:: python

    df.fillna(value=0, inplace=True)
    print(df)
    

::

                     area  absFrequency  position  relFrequency  team
    0          Accounting            73         0      0.288538     2
    1             Finance            52         1      0.205534     4
    2  General management            36         0      0.142292     0
    3     Marketing sales            64         1      0.252964     6
    4               other            28         3      0.110672    10
    
    



Add rows to the DataFrame
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We can use the ``append()`` method to add rows to the DataFrame.


.. code-block:: python

    df = df.append(pd.Series(
                    [np.nan]*len(df.columns), # Fill cells with NaNs
                    index=df.columns),
                    ignore_index=True)
    
    print(df.tail(3))
    

::

                  area  absFrequency  position  relFrequency  team
    3  Marketing sales            64         1      0.252964     6
    4            other            28         3      0.110672    10
    5              NaN           NaN       NaN           NaN   NaN
    
    



We can then fill this empty row with data.


.. code-block:: python

    df.loc[df.index[-1], 'area'] = 'Economics'
    df.loc[df.index[-1], 'absFrequency'] = 25
    print(df)
    

::

                     area  absFrequency  position  relFrequency  team
    0          Accounting            73         0      0.288538     2
    1             Finance            52         1      0.205534     4
    2  General management            36         0      0.142292     0
    3     Marketing sales            64         1      0.252964     6
    4               other            28         3      0.110672    10
    5           Economics            25       NaN           NaN   NaN
    
    



Sorting and reindexing DataFrames
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We next sort the DataFrame by a certain column (from highest to lowest) using
the ``sort()`` DataFrame method.


.. code-block:: python

    df.sort('absFrequency', ascending=False, inplace=True)
    print(df.head())
    

::

                     area  absFrequency  position  relFrequency  team
    0          Accounting            73         0      0.288538     2
    3     Marketing sales            64         1      0.252964     6
    1             Finance            52         1      0.205534     4
    2  General management            36         0      0.142292     0
    4               other            28         3      0.110672    10
    
    



The ``inplace=True`` option will immediately overwrite the DataFrame ``df``
with the new, sorted, one. If you set ``inplace=False`` you would have to
assign a new DataFrame in order to see the changes::

    df1 = df.sort('absFrequency', ascending=False, inplace=True)

Where ``df1`` would now be the sorted DataFrame and the original ``df``
DataFrame would still be unsorted. However, we now have two objects with
identical data in it which could become a memory problem if the DataFrames are
very large. Setting ``inplace=True`` is probably a good default setting.

If we want we could then reindex the DataFrame according to the new sort order.


.. code-block:: python

    df.index = range(1,len(df.index)+1)
    print(df.head())
    

::

                     area  absFrequency  position  relFrequency  team
    1          Accounting            73         0      0.288538     2
    2     Marketing sales            64         1      0.252964     6
    3             Finance            52         1      0.205534     4
    4  General management            36         0      0.142292     0
    5               other            28         3      0.110672    10
    
    



Merging DataFrames or adding columns
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Let us create a new **DataFrame** that has the variable ``team`` in common with the previous
DataFrame. This new DataFrame contains additional team information that we
would like to merge into the original DataFrame.

First we define the new DataFrame and add some values to it:


.. code-block:: python

    df_team = pd.DataFrame({'team': [3,4,5,6,7,8,9,10], \
                            'sales': [500,300,250,450,345,123,432,890]})
    
    print(df_team)
    

::

       sales  team
    0    500     3
    1    300     4
    2    250     5
    3    450     6
    4    345     7
    5    123     8
    6    432     9
    7    890    10
    
    



This new DataFrame contains some sales information for each team. We next merge
this info into the main DataFrame using the ``team`` column as merge key or key
variable. Before we merge in the new info we drop the ``area`` column so that
the DataFrame fits on the screen.


.. code-block:: python

    df = df.drop('area', 1)
    print(pd.merge(df, df_team, on='team', how='inner'))
    

::

       absFrequency  position  relFrequency  team  sales
    0            64         1      0.252964     6    450
    1            52         1      0.205534     4    300
    2            28         3      0.110672    10    890
    
    



The inner merge only keeps observations that are present in both DataFrames.
This is often too restrictive as many observations will be dropped.

The next one, ``left``, keeps the original (or main DataFrame) and adds the new
information on the left. When it cannot find a team number in the main
DataFrame it simply drops the info from the new DataFrame.


.. code-block:: python

    print(pd.merge(df, df_team, on='team', how='left'))
    

::

       absFrequency  position  relFrequency  team  sales
    0            73         0      0.288538     2    NaN
    1            64         1      0.252964     6    450
    2            52         1      0.205534     4    300
    3            36         0      0.142292     0    NaN
    4            28         3      0.110672    10    890
    5            25       NaN           NaN   NaN    NaN
    
    



The next method only keeps info from the new DataFrame and adds the info of
the original main DataFrame.


.. code-block:: python

    print(pd.merge(df, df_team, on='team', how='right'))
    

::

       absFrequency  position  relFrequency  team  sales
    0            64         1      0.252964     6    450
    1            52         1      0.205534     4    300
    2            28         3      0.110672    10    890
    3           NaN       NaN           NaN     3    500
    4           NaN       NaN           NaN     5    250
    5           NaN       NaN           NaN     7    345
    6           NaN       NaN           NaN     8    123
    7           NaN       NaN           NaN     9    432
    
    




The final merge method merges by team whenever possible but keeps the info from both
DataFrames, even for observations where no merge/overlap occurred.


.. code-block:: python

    print(pd.merge(df, df_team, on='team', how='outer'))
    

::

        absFrequency  position  relFrequency  team  sales
    0             73         0      0.288538     2    NaN
    1             64         1      0.252964     6    450
    2             52         1      0.205534     4    300
    3             36         0      0.142292     0    NaN
    4             28         3      0.110672    10    890
    5             25       NaN           NaN   NaN    NaN
    6            NaN       NaN           NaN     3    500
    7            NaN       NaN           NaN     5    250
    8            NaN       NaN           NaN     7    345
    9            NaN       NaN           NaN     8    123
    10           NaN       NaN           NaN     9    432
    
    




Converting Column Types
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

When converting text into a DataFrame to conduct statistical analysis it is
sometimes necessary to convert a numbers column that is still a string, into
floats, so that we can do math. Let's drop some more columns first so the DataFrame
fits nicely in the output window.


.. code-block:: python

    df = df.drop('absFrequency', 1)
    # Let's have a look at it, it's a nested list
    print(df)
    

::

       position  relFrequency  team
    1         0      0.288538     2
    2         1      0.252964     6
    3         1      0.205534     4
    4         0      0.142292     0
    5         3      0.110672    10
    6       NaN           NaN   NaN
    
    



We now generate a column of "words" or "strings" that happen do be numbers.
Since they are defined as words, we cannot run statistical analysis on these
numbers yet.


.. code-block:: python

    df['string'] = pd.Series(['2','40','34','6','10','200'],
    index=df.index)
    
    # Print DataFrame
    print(df)
    
    # Print summary statistics
    print(df.describe())
    

::

       position  relFrequency  team string
    1         0      0.288538     2      2
    2         1      0.252964     6     40
    3         1      0.205534     4     34
    4         0      0.142292     0      6
    5         3      0.110672    10     10
    6       NaN           NaN   NaN    200
           position  relFrequency       team
    count  5.000000      5.000000   5.000000
    mean   1.000000      0.200000   4.400000
    std    1.224745      0.074136   3.847077
    min    0.000000      0.110672   0.000000
    25%    0.000000      0.142292   2.000000
    50%    1.000000      0.205534   4.000000
    75%    1.000000      0.252964   6.000000
    max    3.000000      0.288538  10.000000
    
    



We first need to reassign the data type of the ``string`` column. We then
rename the column and print summary statistics.


.. code-block:: python

    # Transform strings into floats, i.e., words into numbers
    df['string'] = df['string'].astype(float)
    
    # Rename the column
    df = df.rename(columns={'string': 'salary'})
    
    # Print summary statistics
    print(df.describe())
    

::

           position  relFrequency       team      salary
    count  5.000000      5.000000   5.000000    6.000000
    mean   1.000000      0.200000   4.400000   48.666667
    std    1.224745      0.074136   3.847077   75.743427
    min    0.000000      0.110672   0.000000    2.000000
    25%    0.000000      0.142292   2.000000    7.000000
    50%    1.000000      0.205534   4.000000   22.000000
    75%    1.000000      0.252964   6.000000   38.500000
    max    3.000000      0.288538  10.000000  200.000000
    
    



Replacing values in a DataFrame conditional on criteria
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

If we need to replace values in a **DataFrame** based on certain criteria it is
often more efficient to use **indexing** as opposed to loops. Let us first
create a DataFrame with some random values:


.. code-block:: python

    df1 = pd.DataFrame(np.random.rand(10,5)*100)
    print(df1)
    

::

               0          1          2          3          4
    0   5.964016  65.099692   2.604514  27.413797  46.420845
    1  82.723986  70.756959  31.697136  27.963670  38.688859
    2   8.138214  31.956749  17.797434  24.692932  13.191135
    3  93.073017  34.759564  83.609647  33.406344  68.554443
    4  93.582556   9.808928  18.585683  49.850321  84.468746
    5  35.194378   8.149723  22.791774  77.412394   8.300174
    6  78.389353  73.610111  88.914548   2.292844  44.233570
    7  18.151162   1.696217  36.552867  94.816007  45.594009
    8  15.146534  36.671286  27.829229  13.903670  85.953901
    9  58.472523  62.087970  57.742129   8.962014  96.047687
    
    



We next replace all the values in column ``2`` that are smaller than 30 with
the string `low`.


.. code-block:: python

    df1[1][(df1[1]<30)] = 'Low'
    print(df1)
    

::

               0         1          2          3          4
    0   5.964016  65.09969   2.604514  27.413797  46.420845
    1  82.723986  70.75696  31.697136  27.963670  38.688859
    2   8.138214  31.95675  17.797434  24.692932  13.191135
    3  93.073017  34.75956  83.609647  33.406344  68.554443
    4  93.582556       Low  18.585683  49.850321  84.468746
    5  35.194378       Low  22.791774  77.412394   8.300174
    6  78.389353  73.61011  88.914548   2.292844  44.233570
    7  18.151162       Low  36.552867  94.816007  45.594009
    8  15.146534  36.67129  27.829229  13.903670  85.953901
    9  58.472523  62.08797  57.742129   8.962014  96.047687
    
    



We next replace all the values of column ``4`` that are larger than 70 with the
value 1000.


.. code-block:: python

    df1[3][(df1[3]>70)] = 1000
    print(df1)
    

::

               0         1          2            3          4
    0   5.964016  65.09969   2.604514    27.413797  46.420845
    1  82.723986  70.75696  31.697136    27.963670  38.688859
    2   8.138214  31.95675  17.797434    24.692932  13.191135
    3  93.073017  34.75956  83.609647    33.406344  68.554443
    4  93.582556       Low  18.585683    49.850321  84.468746
    5  35.194378       Low  22.791774  1000.000000   8.300174
    6  78.389353  73.61011  88.914548     2.292844  44.233570
    7  18.151162       Low  36.552867  1000.000000  45.594009
    8  15.146534  36.67129  27.829229    13.903670  85.953901
    9  58.472523  62.08797  57.742129     8.962014  96.047687
    
    



And finally we can combine logical statements and replace values on a
combination of conditions. So let's replace all the values in column ``5`` that
are between 20 and 80 with the string `middle`.


.. code-block:: python

    df1[4][(df1[4]>20) & (df1[4]<80)] = 'Middle'
    print(df1)
    

::

               0         1          2            3         4
    0   5.964016  65.09969   2.604514    27.413797    Middle
    1  82.723986  70.75696  31.697136    27.963670    Middle
    2   8.138214  31.95675  17.797434    24.692932  13.19114
    3  93.073017  34.75956  83.609647    33.406344    Middle
    4  93.582556       Low  18.585683    49.850321  84.46875
    5  35.194378       Low  22.791774  1000.000000  8.300174
    6  78.389353  73.61011  88.914548     2.292844    Middle
    7  18.151162       Low  36.552867  1000.000000    Middle
    8  15.146534  36.67129  27.829229    13.903670   85.9539
    9  58.472523  62.08797  57.742129     8.962014  96.04769
    
    




Making simple graphs from our data
-------------------------------------------------------------------------------

Bar chart
++++++++++++++++

We first make a bar chart of the absolute frequencies.


.. code-block:: python

    fig, ax = plt.subplots()
    #
    ax.bar(1. + np.arange(len(xv)), xv, align='center')
    # Annotate with text
    ax.set_xticks(1. + np.arange(len(xv)))
    for i, val in enumerate(xv):
        ax.text(i+1, val/2, str(round(val, 2)*100)+'%', va='center',
    ha='center', color='black')
    ax.set_ylabel('%')
    ax.set_title('Relative Frequency Barchart')
    plt.show()
    # We can also save this graph as a pdf
    #savefig('./Graphs/fig1.pdf')
    

.. image:: _static/Slides_Data_Barchart_1.*
   :width: 12 cm



This simple command plots the barchart into a window and saves it as
**fig1.pdf** into a subfolder called ``Graphs``. Don't forget to first make
this subfolder, otherwise ``Python`` will throw an error when it can't find the
folder.

Pie chart
++++++++++++++++

We next make a pie chart using the relative frequencies stored in vector ``xv``.


.. code-block:: python

    fig, ax = plt.subplots()
    ax.pie(xv, labels = np.round(xv, 2)*100, shadow=True)
    # Annotate with text
    ax.set_title('Relative Frequency Pie Chart')
    plt.show()
    

.. image:: _static/Slides_Data_Piechart_1.*
   :width: 12 cm



Histogram
++++++++++++++++

Next we use a new data file called
:download:`Lecture_Data_Excel_b.csv <Lecture_Data/Lecture_Data_Excel_b.csv>`.
This data file contains data on height age and other variables. We first make a histogram of
the continuous variable Height.
The actual command for the histogram is ``hist``. It returns three variables
``prob``, ``bins``, and ``patches``. We use them in the following to add
information to the histogram.


.. code-block:: python

    df = pd.read_csv(filepath + 'Lecture_Data_Excel_b.csv')
    df = df.drop('Response' ,1)
    print(df.head())
    

::

       Height  AverageMathSAT  Age  Female  Education  Race
    0     1.1             370   20       0          2  Hisp
    1     1.2             393   20       0          4   Wht
    2     1.3             413   20       0          4   Blk
    3     1.4             430   20       0          4   Wht
    4     1.5             440   20       0          2   Mex
    
    



We next plot a series of histograms.


.. code-block:: python

    heightv = df['Height'].values
    
    # Initialize
    N = len(heightv) # number of obs.
    B = 8           # number of bins in histogram
    
    fig, ax = plt.subplots(2,2)
    plt.subplots_adjust(wspace = 0.8, hspace = 0.8)
    
    prob, bins, patches = ax[0,0].hist(heightv, bins=B, align='mid' )
    # Annotate with text
    for i, p in enumerate(prob):
        percent = int(float(p)/N*100)
        # only annotate non-zero values
        if percent:
            ax[0,0].text(bins[i]*1.5, p/2.0, str(percent)+'%',
    rotation=45, va='bottom', ha='center')
    
    ax[0,0].set_xlabel('Height groups')
    ax[0,0].set_ylabel('Number of obs')
    ax[0,0].set_title('Histogram of Height')
    ax[0,0].set_xlim(min(heightv),max(heightv))
    
    # Using Panda's built in histogram method
    df['Height'].hist(bins = B, ax = ax[0,1], color = 'k', alpha = 0.3)
    ax[0,1].set_title('Histogram of Height')
    ax[0,1].set_xlabel('Height groups')
    ax[0,1].set_ylabel('Number of obs')
    #
    df['AverageMathSAT'].hist(bins = B, ax = ax[1,0], color = 'g', alpha =
    0.3)
    ax[1,0].set_title('Histogram of Math SAT Scores')
    #
    plt.show()
    

.. image:: _static/Slides_Data_Histogram_1.*
   :width: 12 cm



Another way to graph a histogram is by using the powerful ``ggplot`` package
which is a translation of **R's** `ggplot2 <http://ggplot2.org/>`_ package into **Python**. It
follows a completely different graphing philosophy based on the **grammar of
grapics** by:

  Hadley Wickham. A layered grammar of graphics.
  Journal of Computational and Graphical Statistics, vol. 19, no. 1, pp. 3â€“28, 2010.

Or using the ggplot package


.. code-block:: python

    print( ggplot(aes(x='Height'), data = df) + geom_histogram() +
    ggtitle("Histogram of Height using ggplot") + labs("Height", "Freq"))
    

::

    <ggplot: (-9223372036845724149)>
    
    

.. image:: _static/Slides_Data_Boxplot_1.*
   :width: 12 cm




Boxplots
++++++++

A boxplot of height is made as follows:


.. code-block:: python

    fig, ax = plt.subplots()
    ax.boxplot(heightv)
    # Annotate with text
    ax.set_title('Boxplot of Height')
    plt.show()
    

.. image:: _static/Slides_Data_Boxplot_1.*
   :width: 12 cm




Summary statistics
-------------------------------------------------------------------------------

We next go through some basic summary statistics.

Measures of central tendency
+++++++++++++++++++++++++++++++++++

A quick way to produce summary statistics of our data set is to use ``panda's``
command ``describe``.


.. code-block:: python

    print(df.describe())
    

::

              Height  AverageMathSAT        Age     Female  Education
    count  60.000000       60.000000  60.000000  60.000000  60.000000
    mean    6.816667      493.666667  20.933333   0.333333   3.016667
    std     3.571743       34.018274   1.176992   0.475383   1.096863
    min     1.100000      370.000000  20.000000   0.000000   1.000000
    25%     3.950000      478.500000  20.000000   0.000000   2.000000
    50%     6.850000      507.000000  20.000000   0.000000   3.000000
    75%     9.650000      509.250000  22.000000   1.000000   4.000000
    max    12.700000      542.000000  23.000000   1.000000   5.000000
    
    



Or we can produce the same statistics by hand. We first calculate the mean,
median and mode. Note that ``mode`` is part of the ``stats`` package which was
imported as ``st`` using: ``from scipy import stats as st``. Now we have to add
``st`` to the mode command: ``st.mode(heightv)`` in order to call it. The other
commands are part of the ``numpy`` package so they have the prefix ``np.``


.. code-block:: python

    # Number of observations (sample size)
    N = len(heightv)
    
    # Mean - Median - Mode
    print("Mean(height)=  {:.3f}".format(np.mean(heightv)))
    print("Median(height)=  {:.3f}".format(np.median(heightv)))
    
    # Mode (value with highest frequency)
    print("Mode(height)=  {}".format(st.mode(heightv)))
    

::

    Mean(height)=  6.817
    Median(height)=  6.850
    Mode(height)=  (array([ 1.1]), array([ 1.]))
    
    



We can also just summarize a variable using the ``st.describe`` command from
the ``stats`` package.


.. code-block:: python

    # Summary stats
    print("Summary Stats")
    print("-------------")
    print(st.describe(heightv))
    

::

    Summary Stats
    -------------
    DescribeResult(nobs=60, minmax=(1.1000000000000001,
    12.699999999999999), mean=6.8166666666666655,
    variance=12.757344632768362, skewness=0.01909961978127301,
    kurtosis=-1.1611486008105545)
    
    



Measures of dispersion
+++++++++++++++++++++++++++++

We now calculate the range, variance and standard deviations. Remember that for
variance and standard deviation there is a distinction between population and
sample.


.. code-block:: python

    # returns smallest a largest element
    print("Range = {:.3f}".format(max(heightv)-min(heightv)))
    print("Population variance = {:.3f}".\
    
    # population variance
      format(np.sum((heightv-np.mean(heightv))**2)/N))
    
    # sample variance
    print("Sample variance     = {:.3f}".\
      format(np.sum((heightv-np.mean(heightv))**2)/(N-1)))
    
    print("Pop.   standard dev = {:.3f}".\
      format(np.sqrt(np.sum((heightv-np.mean(heightv))**2)/N)))
    
    print("Sample standard dev = {:.3f}". \
      format(np.sqrt(np.sum((heightv-np.mean(heightv))**2)/(N-1))))
    
    #Or simply
    print("Pop.   standard dev = {:.3f}".format(np.std(heightv)))
    

::

    Range = 11.600
    Population variance = 12.545
    Sample variance     = 12.757
    Pop.   standard dev = 3.542
    Sample standard dev = 3.572
    Pop.   standard dev = 3.542
    
    



Measures of relative standing
+++++++++++++++++++++++++++++

Percentiles are calculated as follows:


.. code-block:: python

    print("1 quartile (25th percentile) = {:.3f}".\
      format(st.scoreatpercentile(heightv, 25)))
    print("2 quartile (50th percentile) = {:.3f}".\
      format(st.scoreatpercentile(heightv, 50)))
    print("3 quartile (75th percentile) = {:.3f}".\
      format(st.scoreatpercentile(heightv, 75)))
    
    # Inter quartile rante: Q3-Q1 or P_75-P_25
    print("IQR = P75 - P25              = {:.3f}".\
      format(st.scoreatpercentile(heightv, 75)\
      -st.scoreatpercentile(heightv, 25)))
    

::

    1 quartile (25th percentile) = 3.950
    2 quartile (50th percentile) = 6.850
    3 quartile (75th percentile) = 9.650
    IQR = P75 - P25              = 5.700
    
    



Data aggregation or summary statistics by subgroup
++++++++++++++++++++++++++++++++++++++++++++++++++

We next want to summarize the data by subgroup. Let's assume that we would like
to compare the average ``Height`` by ``Race``. We can use the ``groupby()`` to
accomplish this.


.. code-block:: python

    groupRace = df.groupby('Race')
    print('Summary by Race', groupRace.mean())
    

::

    Summary by Race         Height  AverageMathSAT        Age    Female
    Education
    Race
    Blk   6.278261      491.652174  20.826087  0.304348   3.565217
    Hisp  6.525000      472.750000  21.500000  0.250000   1.750000
    Mex   6.171429      491.571429  20.928571  0.285714   2.428571
    Oth   8.750000      516.000000  21.000000  0.500000   2.500000
    Wht   7.917647      500.411765  20.941176  0.411765   3.117647
    
    



We could also use a different category like gender.


.. code-block:: python

    groupGender = df.groupby('Female')
    print('Summary by Gender', groupGender.mean())
    

::

    Summary by Gender         Height  AverageMathSAT   Age  Education
    Female
    0        4.765           479.7  21.4       2.95
    1       10.920           521.6  20.0       3.15
    
    



Or we can also combine the two or more categories and summarize the data as
follows.


.. code-block:: python

    groupRaceGender = df.groupby(['Race', 'Female'])
    print('Summary by Race and Gender', groupRaceGender.mean())
    

::

    Summary by Race and Gender                 Height  AverageMathSAT
    Age  Education
    Race Female
    Blk  0        4.350000      479.562500  21.1875   3.562500
         1       10.685714      519.285714  20.0000   3.571429
    Hisp 0        5.600000      461.333333  22.0000   1.666667
         1        9.300000      507.000000  20.0000   2.000000
    Mex  0        4.480000      481.900000  21.3000   2.100000
         1       10.400000      515.750000  20.0000   3.250000
    Oth  0        6.300000      506.000000  22.0000   1.000000
         1       11.200000      526.000000  20.0000   4.000000
    Wht  0        5.310000      480.600000  21.6000   3.400000
         1       11.642857      528.714286  20.0000   2.714286
    
    



Be careful and don't forget to add the various categories in a list using the
brackets ``[...]]``.

Measures of linear relationship
---------------------------------------------------------

Covariance
+++++++++++++++++

We first grab two variables ``heightv`` and ``agev`` from the dataframe and
define them as ``numpy`` arrays.


.. code-block:: python

    xv = df['Age'].values
    yv = df['Height'].values
    n = len(xv)
    



We then go ahead and calculate the covariance.


.. code-block:: python

    # Population covariance
    print("Population covariance = {:.3f}".\
      format(np.sum((xv-np.mean(xv))*(yv-np.mean(yv)))/n))
    
    # Sample covariance
    print("Sample covariance     = {:.3f}".\
      format(np.sum((xv-np.mean(xv))*(yv-np.mean(yv)))/(n-1)))
    
    # or simply
    print("Sample covariance     = {:.3f}".format(df.Age.cov(df.Height)))
    

::

    Population covariance = -0.159
    Sample covariance     = -0.162
    Sample covariance     = -0.162
    
    



Correlation coefficient
++++++++++++++++++++++++++++++

We can calculate the correlation coefficient by hand or using the ``pandas``
toolbox command.


.. code-block:: python

    print("Correlation coefficient =  {:.3f}".\
      format((np.sum((xv-np.mean(xv))*(yv-np.mean(yv)))/n)\
      /(np.std(xv)*np.std(yv))))
    
    print("Correlation coefficient =
    {:.3f}".format(df.Age.corr(df.Height)))
    

::

    Correlation coefficient =  -0.038
    Correlation coefficient = -0.038
    
    



Regression line
++++++++++++++++++++++

**Example 1: Simple example**

We first generate some data. A variable ``x`` and a variable ``y``.


.. code-block:: python

    # Define data. 2 vectors xv and yv
    xv = np.arange(1,9,1)
    yv = np.array([6,1,9,5,17,12,14,15])
    



We then "cast" these into a ``pandas`` DataFrame.


.. code-block:: python

    # Define data frame
    df = pd.DataFrame(np.array([xv, yv]).T, columns = ['x', 'y'])
    



We fist make a **scatterplot** with least squares trend line: `y = \beta_0 +
\beta_1 * x + \epsilon`.


.. code-block:: python

    p = np.polyfit(xv, yv, 1)
    print("p = {}".format(p))
    
    # Scatterplot
    fig, ax = plt.subplots()
    ax.set_title('Linear regression with polyfit()')
    ax.plot(xv, yv, 'o', label = 'data')
    ax.plot(xv, np.polyval(p,xv),'-', label = 'Linear regression')
    ax.legend(['Data', 'OLS'], loc='best')
    plt.show()
    

::

    p = [ 1.77380952  1.89285714]
    
    

.. image:: _static/Slides_Data_Scatter1_1.*
   :width: 12 cm



We then run the same regression using a more general method called ``ols``
which is part of ``pandas``. When calling the ``ols`` function
you need to add the module name (``pandas`` was imported as ``pd``)
in front of it: ``pd.ols()``. You then define the independent variable ``y``
and the dependent variables ``x's``.


.. code-block:: python

    # Run OLS regression
    res = pd.ols(y=df['y'], x=df['x'])
    
    # Show coefficient estimates
    print(res)
    print('Parameters:')
    print(res.beta)
    

::

    
    -------------------------Summary of Regression
    Analysis-------------------------
    
    Formula: Y ~ <x> + <intercept>
    
    Number of Observations:         8
    Number of Degrees of Freedom:   2
    
    R-squared:         0.6093
    Adj R-squared:     0.5442
    
    Rmse:              3.7578
    
    F-stat (1, 6):     9.3583, p-value:     0.0222
    
    Degrees of Freedom: model 1, resid 6
    
    -----------------------Summary of Estimated
    Coefficients------------------------
          Variable       Coef    Std Err     t-stat    p-value    CI 2.5%
    CI 97.5%
    --------------------------------------------------------------------------------
                 x     1.7738     0.5798       3.06     0.0222     0.6373
    2.9103
         intercept     1.8929     2.9281       0.65     0.5419    -3.8461
    7.6318
    ---------------------------------End of
    Summary---------------------------------
    
    Parameters:
    x            1.773810
    intercept    1.892857
    dtype: float64
    
    



Finally, we use the model to make a prediction of ``y`` when ``x = 8.5``.


.. code-block:: python

    # Make prediction for x = 8.5
    betas = res.beta
    print("Prediction of y for x = 8.5 is: {:.3f}".\
      format(np.sum(betas * np.array([1,8.5]))))
    

::

    Prediction of y for x = 8.5 is: 17.863
    
    



We can also use the results from the ``ols`` command to make a scatterplot with
the trendline through it.


.. code-block:: python

    fig, ax = plt.subplots()
    ax.set_title('Linear Regression with Fitted Values')
    ax.plot(xv, yv, 'o', label = 'data')
    ax.plot(xv, res.y_predict,'k-', label = 'Linear regression')
    ax.legend(['Data', 'OLS'], loc='best')
    plt.show()
    

.. image:: _static/Slides_Data_Scatter2_1.*
   :width: 12 cm



**Example 2: OLS with categorical (dummy variables)**


The next example is a bit more involved as we increase the number of
explanatory variables. In addition, some explanatory variables are categorical
variables. In order to use them in our OLS regression we first have to make so
called **dummy** variables (i.e. indicator variables that are either 0 or 1).


.. code-block:: python

    df = pd.read_csv(filepath + 'Lecture_Data_Excel_b.csv')
    
    dummies = pd.get_dummies(df['Race'], prefix = 'd')
    df = df.join(dummies)
    print(df.head())
    

::

       Height  Response  AverageMathSAT  Age  Female  Education  Race
    d_Blk  \
    0     1.1         3             370   20       0          2  Hisp
    0
    1     1.2         4             393   20       0          4   Wht
    0
    2     1.3         4             413   20       0          4   Blk
    1
    3     1.4         5             430   20       0          4   Wht
    0
    4     1.5         3             440   20       0          2   Mex
    0
    
       d_Hisp  d_Mex  d_Oth  d_Wht
    0       1      0      0      0
    1       0      0      0      1
    2       0      0      0      0
    3       0      0      0      1
    4       0      1      0      0
    
    



We next use ``ols`` again to run the regression. At the end we make a
prediction based on some values for the independent variables ``x1``, ``x2``,
etc.


.. code-block:: python

    res = pd.ols(y=df['Height'], x=df[['Age','Education','Female', \
        'd_Blk','d_Hisp','d_Mex','d_Wht']])
    
    # Show coefficient estimates
    print(res)
    
    print()
    print('Parameters:')
    print(res.beta)
    

::

    
    -------------------------Summary of Regression
    Analysis-------------------------
    
    Formula: Y ~ <Age> + <Education> + <Female> + <d_Blk> + <d_Hisp> +
    <d_Mex>
                 + <d_Wht> + <intercept>
    
    Number of Observations:         60
    Number of Degrees of Freedom:   8
    
    R-squared:         0.9437
    Adj R-squared:     0.9361
    
    Rmse:              0.9030
    
    F-stat (7, 52):   124.4348, p-value:     0.0000
    
    Degrees of Freedom: model 7, resid 52
    
    -----------------------Summary of Estimated
    Coefficients------------------------
          Variable       Coef    Std Err     t-stat    p-value    CI 2.5%
    CI 97.5%
    --------------------------------------------------------------------------------
               Age     1.8944     0.1241      15.27     0.0000     1.6512
    2.1375
         Education    -0.0514     0.1265      -0.41     0.6864    -0.2994
    0.1967
            Female     8.7359     0.3077      28.39     0.0000     8.1327
    9.3391
             d_Blk    -0.3784     0.6855      -0.55     0.5834    -1.7220
    0.9653
            d_Hisp    -1.0267     0.7901      -1.30     0.1995    -2.5753
    0.5218
    --------------------------------------------------------------------------------
             d_Mex    -0.5749     0.6863      -0.84     0.4060    -1.9200
    0.7701
             d_Wht     0.0816     0.6808       0.12     0.9050    -1.2528
    1.4161
         intercept   -35.2714     2.7510     -12.82     0.0000   -40.6634
    -29.8794
    ---------------------------------End of
    Summary---------------------------------
    
    
    Parameters:
    Age           1.894377
    Education    -0.051375
    Female        8.735906
    d_Blk        -0.378358
    d_Hisp       -1.026743
    d_Mex        -0.574949
    d_Wht         0.081628
    intercept   -35.271435
    dtype: float64
    
    




Prediction: size of 8.5 will produce a math score of ...


.. code-block:: python

    betas = res.beta
    print("Prediction: {}".\
      format(np.sum(betas * np.array([1,2.5,22,1,0,0,1,0]))))
    

::

    Prediction: 193.65914627657307
    
    



