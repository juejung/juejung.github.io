Working with data I: Data cleaning
===============================================================================

We will be working with two data sets.
:download:`Lecture_Data_Excel_a.csv <Lecture_Data/Lecture_Data_Excel_a.csv>`.
:download:`Lecture_Data_Excel_b.csv <Lecture_Data/Lecture_Data_Excel_b.csv>`.

Read in small data set from a comma separated (.csv) file
-------------------------------------------------------------------------------

We use the ``Pandas`` package for data work.  We can import the ``Pandas``
library using the ``import`` statement.  ``Pandas`` allows easy organization of
data in the spirit of ``DataFrame`` concept in **R**. You can think of a
**DataFrame** as an array with labels similar to an Excel spreadsheet.

A **DataFrame** can be created in the following ways:

 * From another **DataFrame**.
 * From a ``numpy`` array.
 * From another ``pandas`` data structure called ``series`` which is basically
   a one dimensional **DataFrame**, i.e., a column or row of a matrix.
 * From a file like a CSV file etc.

The first dataset that we are working with is a comma separated file, called
:download:`Lecture_Data_Excel_a.csv <Lecture_Data/Lecture_Data_Excel_a.csv>`.
We first import this file using the ``pd.read_csv`` command from the ``Pandas`` library.


.. code:: python

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from ggplot import *
    from scipy import stats as st
    import math as m
    import seaborn as sns
    import time  # Imports system time module to time your script
    
    plt.close('all')  # close all open figures
    




.. code:: python

    # Read in small data from .csv file
    # Filepath
    filepath = 'Lecture_Data/'
    # In windows you can also specify the absolute path to your data file
    # filepath =
    'C:/Dropbox/Towson/Teaching/3_ComputationalEconomics/Lectures/Lecture_Data/'
    
    # ------------- Load data --------------------
    df = pd.read_csv(filepath + 'Lecture_Data_Excel_a.csv',
    dtype={'Frequency': float})
    
    # Let's have a look at it, it's a nested list
    print(df)
    

.. code::

                     Area  Frequency  Unnamed: 2  Unnamed: 3
    0          Accounting         73         NaN         NaN
    1             Finance         52         NaN         NaN
    2  General management         36         NaN         NaN
    3     Marketing sales         64         NaN         NaN
    4               other         28         NaN         NaN
    
    




Alternatively we could have use the following command::

    df = pd.read_table(filepath + 'Lecture_Data_Excel_a.csv'), sep=',')

``Pandas`` also has a built in Excel file reader that you can call as follows::

    # Read entire excel spreadsheet
    xl = pd.ExcelFile("Lecture_Data_Excel_a.xlsx")

    # Check how many sheets we have inside the excel file
    xl.sheet_names
    [u'Sheet1', u'Sheet2', u'Sheet3']

    # Pick one sheet and define it as your DataFrame by parsing a sheet
    df = xl.parse("Sheet1")

The **DataFrame** has an index for each row and a column header. We can query a
**DataFrame** as follows:


.. code:: python

    print('Shape', df.shape)
    print('-------------------------')
    print('Number of rows', len(df))
    print('-------------------------')
    print('Column headers', df.columns)
    print('-------------------------')
    print('Data types', df.dtypes)
    print('-------------------------')
    print('Index', df.index)
    print('-------------------------')
    

.. code::

    Shape (5, 4)
    -------------------------
    Number of rows 5
    -------------------------
    Column headers Index(['Area', 'Frequency', 'Unnamed: 2', 'Unnamed:
    3'], dtype='object')
    -------------------------
    Data types Area           object
    Frequency     float64
    Unnamed: 2    float64
    Unnamed: 3    float64
    dtype: object
    -------------------------
    Index Int64Index([0, 1, 2, 3, 4], dtype='int64')
    -------------------------
    
    



Drop column 3 and 4 and let's have a look at the data again.


.. code:: python

    df = df.drop('Unnamed: 2', 1)
    df = df.drop('Unnamed: 3', 1)
    # Let's have a look at it, it's a nested list
    print(df)
    

.. code::

                     Area  Frequency
    0          Accounting         73
    1             Finance         52
    2  General management         36
    3     Marketing sales         64
    4               other         28
    
    



Sometimes column headers have white spaces in them. It is probably a good idea
to remove all white spaces from header names. You can use the ``.strip()``
command to accomplish this:


.. code:: python

    df = df.rename(columns=lambda x: x.strip())
    



Here we use the ``lambda`` command which is an inline function. Basically it is
a shortcut so that we don't have to write a separate function using the ``def``
keyword etc. It's a shortcut for functions that you want to define 'on the fly.'


We next generate a new variable called ``relFrequency`` that contains the relative
frequencies.


.. code:: python

    # Make new column with relative frequency
    df['relFrequency'] = df['Frequency']/df['Frequency'].sum()
    # Let's have a look at it, it's a nested list
    print(df)
    

.. code::

                     Area  Frequency  relFrequency
    0          Accounting         73      0.288538
    1             Finance         52      0.205534
    2  General management         36      0.142292
    3     Marketing sales         64      0.252964
    4               other         28      0.110672
    
    



Now we grab the relative frequencies out of the DataFrame into a numpy array.


.. code:: python

    xv = df['relFrequency'].values
    print('Array xv is:', xv)
    

.. code::

    Array xv is: [ 0.28853755  0.2055336   0.14229249  0.25296443
    0.11067194]
    
    




.. code:: python

    # Let us make additional columns
    df['random'] = df['Frequency']/df['Frequency'].sum()
    # Let's have a look at it, it's a nested list
    print(df)
    

.. code::

                     Area  Frequency  relFrequency    random
    0          Accounting         73      0.288538  0.288538
    1             Finance         52      0.205534  0.205534
    2  General management         36      0.142292  0.142292
    3     Marketing sales         64      0.252964  0.252964
    4               other         28      0.110672  0.110672
    
    



Some Pandas tricks
-------------------------------------------------------------------------------

Renaming variables
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

For renaming column names we use a dictionary as follows.


.. code:: python

    df = df.rename(columns={'Area': 'area',
        'Frequency': 'absFrequency'})
    
    print(df.head(3))
    

.. code::

                     area  absFrequency  relFrequency    random
    0          Accounting            73      0.288538  0.288538
    1             Finance            52      0.205534  0.205534
    2  General management            36      0.142292  0.142292
    
    



The ``head(x)`` method allows us to print ``x`` rows from the top of the
**DataFrame**, whereas the ``tail(x)`` method does the same from the bottom of
the DataFrame.


Adding a new column
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


.. code:: python

    # Drop a column first so everything fits
    df = df.drop('random', 1)
    
    # Create a new column with some missing values
    df['team'] = pd.Series([2,4, np.NaN,6,10], index=df.index)
    
    # or insert new column at specific location
    df.insert(loc=2, column='position', value=[np.NaN,1, np.NaN,1,3])
    
    print(df)
    

.. code::

                     area  absFrequency  position  relFrequency  team
    0          Accounting            73       NaN      0.288538     2
    1             Finance            52         1      0.205534     4
    2  General management            36       NaN      0.142292   NaN
    3     Marketing sales            64         1      0.252964     6
    4               other            28         3      0.110672    10
    
    



Missing values of NaN's
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

**Count NaNs**

Count the number of rows with missing values.


.. code:: python

    nans = df.shape[0] - df.dropna().shape[0]
    print('%d rows have missing values' % nans)
    

.. code::

    2 rows have missing values
    
    



**Select rows with NaNs or overwrite NaNs**

If you want to select the rows with missing values you can index them with the
``isnull()`` method.


.. code:: python

    print(df[df.isnull().any(axis=1)])
    

.. code::

                     area  absFrequency  position  relFrequency  team
    0          Accounting            73       NaN      0.288538     2
    2  General management            36       NaN      0.142292   NaN
    
    



Or if you want to search specific columns for missing values you can do


.. code:: python

    print(df[df['team'].isnull()])
    

.. code::

                     area  absFrequency  position  relFrequency  team
    2  General management            36       NaN      0.142292   NaN
    
    



Or if you want to grab all the rows without missing observations you can index
the dataframe with the ``notnull()`` method.


.. code:: python

    print(df[df['team'].notnull()])
    

.. code::

                  area  absFrequency  position  relFrequency  team
    0       Accounting            73       NaN      0.288538     2
    1          Finance            52         1      0.205534     4
    3  Marketing sales            64         1      0.252964     6
    4            other            28         3      0.110672    10
    
    



**Delete rows with NaNs**


.. code:: python

    print(df.dropna())
    

.. code::

                  area  absFrequency  position  relFrequency  team
    1          Finance            52         1      0.205534     4
    3  Marketing sales            64         1      0.252964     6
    4            other            28         3      0.110672    10
    
    



You can also reassign it to the orignal DataFrame or assign
a new one.


.. code:: python

    df2 = df.dropna()
    print(df2)
    

.. code::

                  area  absFrequency  position  relFrequency  team
    1          Finance            52         1      0.205534     4
    3  Marketing sales            64         1      0.252964     6
    4            other            28         3      0.110672    10
    
    



**Overwriting NaN with specific values**
If we want to overwrite the ``NaN`` entries with values we can use the
``fillna()`` method. In this example we replace all missing values with zeros.


.. code:: python

    df.fillna(value=0, inplace=True)
    print(df)
    

.. code::

                     area  absFrequency  position  relFrequency  team
    0          Accounting            73         0      0.288538     2
    1             Finance            52         1      0.205534     4
    2  General management            36         0      0.142292     0
    3     Marketing sales            64         1      0.252964     6
    4               other            28         3      0.110672    10
    
    



Add rows to the DataFrame
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We can use the ``append()`` method to add rows to the DataFrame.


.. code:: python

    df = df.append(pd.Series(
                    [np.nan]*len(df.columns), # Fill cells with NaNs
                    index=df.columns),
                    ignore_index=True)
    
    print(df.tail(3))
    

.. code::

                  area  absFrequency  position  relFrequency  team
    3  Marketing sales            64         1      0.252964     6
    4            other            28         3      0.110672    10
    5              NaN           NaN       NaN           NaN   NaN
    
    



We can then fill this empty row with data.


.. code:: python

    df.loc[df.index[-1], 'area'] = 'Economics'
    df.loc[df.index[-1], 'absFrequency'] = 25
    print(df)
    

.. code::

                     area  absFrequency  position  relFrequency  team
    0          Accounting            73         0      0.288538     2
    1             Finance            52         1      0.205534     4
    2  General management            36         0      0.142292     0
    3     Marketing sales            64         1      0.252964     6
    4               other            28         3      0.110672    10
    5           Economics            25       NaN           NaN   NaN
    
    



Sorting and reindexing DataFrames
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

We next sort the DataFrame by a certain column (from highest to lowest) using
the ``sort()`` DataFrame method.


.. code:: python

    df.sort('absFrequency', ascending=False, inplace=True)
    print(df.head())
    

.. code::

                     area  absFrequency  position  relFrequency  team
    0          Accounting            73         0      0.288538     2
    3     Marketing sales            64         1      0.252964     6
    1             Finance            52         1      0.205534     4
    2  General management            36         0      0.142292     0
    4               other            28         3      0.110672    10
    
    



The ``inplace=True`` option will immediately overwrite the DataFrame ``df``
with the new, sorted, one. If you set ``inplace=False`` you would have to
assign a new DataFrame in order to see the changes::

    df1 = df.sort('absFrequency', ascending=False, inplace=True)

Where ``df1`` would now be the sorted DataFrame and the original ``df``
DataFrame would still be unsorted. However, we now have two objects with
identical data in it which could become a memory problem if the DataFrames are
very large. Setting ``inplace=True`` is probably a good default setting.

If we want we could then reindex the DataFrame according to the new sort order.


.. code:: python

    df.index = range(1,len(df.index)+1)
    print(df.head())
    

.. code::

                     area  absFrequency  position  relFrequency  team
    1          Accounting            73         0      0.288538     2
    2     Marketing sales            64         1      0.252964     6
    3             Finance            52         1      0.205534     4
    4  General management            36         0      0.142292     0
    5               other            28         3      0.110672    10
    
    



Merging DataFrames or adding columns
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Let us create a new **DataFrame** that has the variable ``team`` in common with the previous
DataFrame. This new DataFrame contains additional team information that we
would like to merge into the original DataFrame.

First we define the new DataFrame and add some values to it:


.. code:: python

    df_team = pd.DataFrame({'team': [3,4,5,6,7,8,9,10], \
                            'sales': [500,300,250,450,345,123,432,890]})
    
    print(df_team)
    

.. code::

       sales  team
    0    500     3
    1    300     4
    2    250     5
    3    450     6
    4    345     7
    5    123     8
    6    432     9
    7    890    10
    
    



This new DataFrame contains some sales information for each team. We next merge
this info into the main DataFrame using the ``team`` column as merge key or key
variable. Before we merge in the new info we drop the ``area`` column so that
the DataFrame fits on the screen.


.. code:: python

    df = df.drop('area', 1)
    print(pd.merge(df, df_team, on='team', how='inner'))
    

.. code::

       absFrequency  position  relFrequency  team  sales
    0            64         1      0.252964     6    450
    1            52         1      0.205534     4    300
    2            28         3      0.110672    10    890
    
    



The inner merge only keeps observations that are present in both DataFrames.
This is often too restrictive as many observations will be dropped.

The next one, ``left``, keeps the original (or main DataFrame) and adds the new
information on the left. When it cannot find a team number in the main
DataFrame it simply drops the info from the new DataFrame.


.. code:: python

    print(pd.merge(df, df_team, on='team', how='left'))
    

.. code::

       absFrequency  position  relFrequency  team  sales
    0            73         0      0.288538     2    NaN
    1            64         1      0.252964     6    450
    2            52         1      0.205534     4    300
    3            36         0      0.142292     0    NaN
    4            28         3      0.110672    10    890
    5            25       NaN           NaN   NaN    NaN
    
    



The next method only keeps info from the new DataFrame and adds the info of
the original main DataFrame.


.. code:: python

    print(pd.merge(df, df_team, on='team', how='right'))
    

.. code::

       absFrequency  position  relFrequency  team  sales
    0            64         1      0.252964     6    450
    1            52         1      0.205534     4    300
    2            28         3      0.110672    10    890
    3           NaN       NaN           NaN     3    500
    4           NaN       NaN           NaN     5    250
    5           NaN       NaN           NaN     7    345
    6           NaN       NaN           NaN     8    123
    7           NaN       NaN           NaN     9    432
    
    




The final merge method merges by team whenever possible but keeps the info from both
DataFrames, even for observations where no merge/overlap occurred.


.. code:: python

    print(pd.merge(df, df_team, on='team', how='outer'))
    

.. code::

        absFrequency  position  relFrequency  team  sales
    0             73         0      0.288538     2    NaN
    1             64         1      0.252964     6    450
    2             52         1      0.205534     4    300
    3             36         0      0.142292     0    NaN
    4             28         3      0.110672    10    890
    5             25       NaN           NaN   NaN    NaN
    6            NaN       NaN           NaN     3    500
    7            NaN       NaN           NaN     5    250
    8            NaN       NaN           NaN     7    345
    9            NaN       NaN           NaN     8    123
    10           NaN       NaN           NaN     9    432
    
    




Converting Column Types
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

When converting text into a DataFrame to conduct statistical analysis it is
sometimes necessary to convert a numbers column that is still a string, into
floats, so that we can do math. Let's drop some more columns first so the DataFrame
fits nicely in the output window.


.. code:: python

    df = df.drop('absFrequency', 1)
    # Let's have a look at it, it's a nested list
    print(df)
    

.. code::

       position  relFrequency  team
    1         0      0.288538     2
    2         1      0.252964     6
    3         1      0.205534     4
    4         0      0.142292     0
    5         3      0.110672    10
    6       NaN           NaN   NaN
    
    



We now generate a column of "words" or "strings" that happen do be numbers.
Since they are defined as words, we cannot run statistical analysis on these
numbers yet.


.. code:: python

    df['string'] = pd.Series(['2','40','34','6','10','200'],
    index=df.index)
    
    # Print DataFrame
    print(df)
    
    # Print summary statistics
    print(df.describe())
    

.. code::

       position  relFrequency  team string
    1         0      0.288538     2      2
    2         1      0.252964     6     40
    3         1      0.205534     4     34
    4         0      0.142292     0      6
    5         3      0.110672    10     10
    6       NaN           NaN   NaN    200
           position  relFrequency       team
    count  5.000000      5.000000   5.000000
    mean   1.000000      0.200000   4.400000
    std    1.224745      0.074136   3.847077
    min    0.000000      0.110672   0.000000
    25%    0.000000      0.142292   2.000000
    50%    1.000000      0.205534   4.000000
    75%    1.000000      0.252964   6.000000
    max    3.000000      0.288538  10.000000
    
    



We first need to reassign the data type of the ``string`` column. We then
rename the column and print summary statistics.


.. code:: python

    # Transform strings into floats, i.e., words into numbers
    df['string'] = df['string'].astype(float)
    
    # Rename the column
    df = df.rename(columns={'string': 'salary'})
    
    # Print summary statistics
    print(df.describe())
    

.. code::

           position  relFrequency       team      salary
    count  5.000000      5.000000   5.000000    6.000000
    mean   1.000000      0.200000   4.400000   48.666667
    std    1.224745      0.074136   3.847077   75.743427
    min    0.000000      0.110672   0.000000    2.000000
    25%    0.000000      0.142292   2.000000    7.000000
    50%    1.000000      0.205534   4.000000   22.000000
    75%    1.000000      0.252964   6.000000   38.500000
    max    3.000000      0.288538  10.000000  200.000000
    
    



Replacing values in a DataFrame conditional on criteria
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

If we need to replace values in a **DataFrame** based on certain criteria it is
often more efficient to use **indexing** as opposed to loops. Let us first
create a DataFrame with some random values:


.. code:: python

    df1 = pd.DataFrame(np.random.rand(10,5)*100)
    print(df1)
    

.. code::

               0          1          2          3          4
    0  26.505276  87.780307  67.574410  10.268601   0.560975
    1  71.010944  45.656933  68.388957  25.351019  35.875104
    2   6.536083  70.311994  12.051071   5.152843  52.730477
    3   8.510390  17.133862  76.785260  56.413375  90.758795
    4  61.718446  26.825269  38.126163  50.482733  92.095157
    5  62.839781  37.309012  36.422392  65.482893   2.883693
    6  98.301998  43.091188  84.424536  57.993706  39.071712
    7   5.032203  96.508764  63.801197   0.232789  27.469405
    8  72.449069  10.116862  48.246507  31.673596  47.865822
    9  44.332694  61.273441  85.173541  77.524371  20.709719
    
    



We next replace all the values in column ``2`` that are smaller than 30 with
the string `low`.


.. code:: python

    df1[1][(df1[1]<30)] = 'Low'
    print(df1)
    

.. code::

               0         1          2          3          4
    0  26.505276  87.78031  67.574410  10.268601   0.560975
    1  71.010944  45.65693  68.388957  25.351019  35.875104
    2   6.536083  70.31199  12.051071   5.152843  52.730477
    3   8.510390       Low  76.785260  56.413375  90.758795
    4  61.718446       Low  38.126163  50.482733  92.095157
    5  62.839781  37.30901  36.422392  65.482893   2.883693
    6  98.301998  43.09119  84.424536  57.993706  39.071712
    7   5.032203  96.50876  63.801197   0.232789  27.469405
    8  72.449069       Low  48.246507  31.673596  47.865822
    9  44.332694  61.27344  85.173541  77.524371  20.709719
    
    



We next replace all the values of column ``4`` that are larger than 70 with the
value 1000.


.. code:: python

    df1[3][(df1[3]>70)] = 1000
    print(df1)
    

.. code::

               0         1          2            3          4
    0  26.505276  87.78031  67.574410    10.268601   0.560975
    1  71.010944  45.65693  68.388957    25.351019  35.875104
    2   6.536083  70.31199  12.051071     5.152843  52.730477
    3   8.510390       Low  76.785260    56.413375  90.758795
    4  61.718446       Low  38.126163    50.482733  92.095157
    5  62.839781  37.30901  36.422392    65.482893   2.883693
    6  98.301998  43.09119  84.424536    57.993706  39.071712
    7   5.032203  96.50876  63.801197     0.232789  27.469405
    8  72.449069       Low  48.246507    31.673596  47.865822
    9  44.332694  61.27344  85.173541  1000.000000  20.709719
    
    



And finally we can combine logical statements and replace values on a
combination of conditions. So let's replace all the values in column ``5`` that
are between 20 and 80 with the string `middle`.


.. code:: python

    df1[4][(df1[4]>20) & (df1[4]<80)] = 'Middle'
    print(df1)
    

.. code::

               0         1          2            3          4
    0  26.505276  87.78031  67.574410    10.268601  0.5609754
    1  71.010944  45.65693  68.388957    25.351019     Middle
    2   6.536083  70.31199  12.051071     5.152843     Middle
    3   8.510390       Low  76.785260    56.413375   90.75879
    4  61.718446       Low  38.126163    50.482733   92.09516
    5  62.839781  37.30901  36.422392    65.482893   2.883693
    6  98.301998  43.09119  84.424536    57.993706     Middle
    7   5.032203  96.50876  63.801197     0.232789     Middle
    8  72.449069       Low  48.246507    31.673596     Middle
    9  44.332694  61.27344  85.173541  1000.000000     Middle
    
    


