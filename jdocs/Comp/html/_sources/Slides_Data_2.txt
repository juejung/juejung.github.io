Working with data II: Statistics
===============================================================================

Importing data
-------------------------------------------------------------------------------
We will be working with two data sets.
:download:`Lecture_Data_Excel_a.csv <Lecture_Data/Lecture_Data_Excel_a.csv>`.
:download:`Lecture_Data_Excel_b.csv <Lecture_Data/Lecture_Data_Excel_b.csv>`.


.. code-block:: python

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from ggplot import *
    from scipy import stats as st
    import math as m
    import seaborn as sns
    import time  # Imports system time module to time your script
    
    plt.close('all')  # close all open figures
    




.. code-block:: python

    # Read in small data from .csv file
    # Filepath
    filepath = 'Lecture_Data/'
    # In windows you can also specify the absolute path to your data file
    # filepath =
    'C:/Dropbox/Towson/Teaching/3_ComputationalEconomics/Lectures/Lecture_Data/'
    
    # ------------- Load data --------------------
    df = pd.read_csv(filepath + 'Lecture_Data_Excel_a.csv',
    dtype={'Frequency': float})
    
    df = df.drop('Unnamed: 2', 1)
    df = df.drop('Unnamed: 3', 1)
    
    # Make new column with relative frequency
    df['relFrequency'] = df['Frequency']/df['Frequency'].sum()
    # Let's have a look at it, it's a nested list
    print(df)
    

::

                     Area  Frequency  relFrequency
    0          Accounting         73      0.288538
    1             Finance         52      0.205534
    2  General management         36      0.142292
    3     Marketing sales         64      0.252964
    4               other         28      0.110672
    
    



Now we grab the relative frequencies out of the DataFrame into a numpy array.


.. code-block:: python

    xv = df['relFrequency'].values
    print('Array xv is:', xv)
    

::

    Array xv is: [ 0.28853755  0.2055336   0.14229249  0.25296443
    0.11067194]
    
    




Making simple graphs from our data
-------------------------------------------------------------------------------

Bar chart
++++++++++++++++

We first make a bar chart of the absolute frequencies.


.. code-block:: python

    fig, ax = plt.subplots()
    #
    ax.bar(1. + np.arange(len(xv)), xv, align='center')
    # Annotate with text
    ax.set_xticks(1. + np.arange(len(xv)))
    for i, val in enumerate(xv):
        ax.text(i+1, val/2, str(round(val, 2)*100)+'%', va='center',
    ha='center', color='black')
    ax.set_ylabel('%')
    ax.set_title('Relative Frequency Barchart')
    plt.show()
    # We can also save this graph as a pdf
    #savefig('./Graphs/fig1.pdf')
    

.. image:: _static/Slides_Data_2_Barchart_1.*
   :width: 12 cm



This simple command plots the barchart into a window and saves it as
**fig1.pdf** into a subfolder called ``Graphs``. Don't forget to first make
this subfolder, otherwise ``Python`` will throw an error when it can't find the
folder.

Pie chart
++++++++++++++++

We next make a pie chart using the relative frequencies stored in vector ``xv``.


.. code-block:: python

    fig, ax = plt.subplots()
    ax.pie(xv, labels = np.round(xv, 2)*100, shadow=True)
    # Annotate with text
    ax.set_title('Relative Frequency Pie Chart')
    plt.show()
    

.. image:: _static/Slides_Data_2_Piechart_1.*
   :width: 12 cm



Histogram
++++++++++++++++

Next we use a new data file called
:download:`Lecture_Data_Excel_b.csv <Lecture_Data/Lecture_Data_Excel_b.csv>`.
This data file contains data on height age and other variables. We first make a histogram of
the continuous variable Height.
The actual command for the histogram is ``hist``. It returns three variables
``prob``, ``bins``, and ``patches``. We use them in the following to add
information to the histogram.


.. code-block:: python

    df = pd.read_csv(filepath + 'Lecture_Data_Excel_b.csv')
    df = df.drop('Response' ,1)
    print(df.head())
    

::

       Height  AverageMathSAT  Age  Female  Education  Race
    0     1.1             370   20       0          2  Hisp
    1     1.2             393   20       0          4   Wht
    2     1.3             413   20       0          4   Blk
    3     1.4             430   20       0          4   Wht
    4     1.5             440   20       0          2   Mex
    
    



We next plot a series of histograms.


.. code-block:: python

    heightv = df['Height'].values
    
    # Initialize
    N = len(heightv) # number of obs.
    B = 8           # number of bins in histogram
    
    fig, ax = plt.subplots(2,2)
    plt.subplots_adjust(wspace = 0.8, hspace = 0.8)
    
    prob, bins, patches = ax[0,0].hist(heightv, bins=B, align='mid' )
    # Annotate with text
    for i, p in enumerate(prob):
        percent = int(float(p)/N*100)
        # only annotate non-zero values
        if percent:
            ax[0,0].text(bins[i]*1.5, p/2.0, str(percent)+'%',
    rotation=45, va='bottom', ha='center')
    
    ax[0,0].set_xlabel('Height groups')
    ax[0,0].set_ylabel('Number of obs')
    ax[0,0].set_title('Histogram of Height')
    ax[0,0].set_xlim(min(heightv),max(heightv))
    
    # Using Panda's built in histogram method
    df['Height'].hist(bins = B, ax = ax[0,1], color = 'k', alpha = 0.3)
    ax[0,1].set_title('Histogram of Height')
    ax[0,1].set_xlabel('Height groups')
    ax[0,1].set_ylabel('Number of obs')
    #
    df['AverageMathSAT'].hist(bins = B, ax = ax[1,0], color = 'g', alpha =
    0.3)
    ax[1,0].set_title('Histogram of Math SAT Scores')
    #
    plt.show()
    

.. image:: _static/Slides_Data_2_Histogram_1.*
   :width: 12 cm



Another way to graph a histogram is by using the powerful ``ggplot`` package
which is a translation of **R's** `ggplot2 <http://ggplot2.org/>`_ package into **Python**. It
follows a completely different graphing philosophy based on the **grammar of
grapics** by:

  Hadley Wickham. A layered grammar of graphics.
  Journal of Computational and Graphical Statistics, vol. 19, no. 1, pp. 3â€“28, 2010.

Or using the ggplot package


.. code-block:: python

    print( ggplot(aes(x='Height'), data = df) + geom_histogram() +
    ggtitle("Histogram of Height using ggplot") + labs("Height", "Freq"))
    

::

    <ggplot: (10232829)>
    
    

.. image:: _static/Slides_Data_2_Boxplot_1.*
   :width: 12 cm




Boxplots
++++++++

A boxplot of height is made as follows:


.. code-block:: python

    fig, ax = plt.subplots()
    ax.boxplot(heightv)
    # Annotate with text
    ax.set_title('Boxplot of Height')
    plt.show()
    

.. image:: _static/Slides_Data_2_Boxplot_1.*
   :width: 12 cm




Summary statistics
-------------------------------------------------------------------------------

We next go through some basic summary statistics.

Measures of central tendency
+++++++++++++++++++++++++++++++++++

A quick way to produce summary statistics of our data set is to use ``panda's``
command ``describe``.


.. code-block:: python

    print(df.describe())
    

::

              Height  AverageMathSAT        Age     Female  Education
    count  60.000000       60.000000  60.000000  60.000000  60.000000
    mean    6.816667      493.666667  20.933333   0.333333   3.016667
    std     3.571743       34.018274   1.176992   0.475383   1.096863
    min     1.100000      370.000000  20.000000   0.000000   1.000000
    25%     3.950000      478.500000  20.000000   0.000000   2.000000
    50%     6.850000      507.000000  20.000000   0.000000   3.000000
    75%     9.650000      509.250000  22.000000   1.000000   4.000000
    max    12.700000      542.000000  23.000000   1.000000   5.000000
    
    



Or we can produce the same statistics by hand. We first calculate the mean,
median and mode. Note that ``mode`` is part of the ``stats`` package which was
imported as ``st`` using: ``from scipy import stats as st``. Now we have to add
``st`` to the mode command: ``st.mode(heightv)`` in order to call it. The other
commands are part of the ``numpy`` package so they have the prefix ``np.``


.. code-block:: python

    # Number of observations (sample size)
    N = len(heightv)
    
    # Mean - Median - Mode
    print("Mean(height)=  {:.3f}".format(np.mean(heightv)))
    print("Median(height)=  {:.3f}".format(np.median(heightv)))
    
    # Mode (value with highest frequency)
    print("Mode(height)=  {}".format(st.mode(heightv)))
    

::

    Mean(height)=  6.817
    Median(height)=  6.850
    Mode(height)=  (array([ 1.1]), array([ 1.]))
    
    



We can also just summarize a variable using the ``st.describe`` command from
the ``stats`` package.


.. code-block:: python

    # Summary stats
    print("Summary Stats")
    print("-------------")
    print(st.describe(heightv))
    

::

    Summary Stats
    -------------
    DescribeResult(nobs=60, minmax=(1.1000000000000001,
    12.699999999999999), mean=6.8166666666666655,
    variance=12.757344632768362, skewness=0.01909961978127301,
    kurtosis=-1.1611486008105545)
    
    



Measures of dispersion
+++++++++++++++++++++++++++++

We now calculate the range, variance and standard deviations. Remember that for
variance and standard deviation there is a distinction between population and
sample.


.. code-block:: python

    # returns smallest a largest element
    print("Range = {:.3f}".format(max(heightv)-min(heightv)))
    print("Population variance = {:.3f}".\
    
    # population variance
      format(np.sum((heightv-np.mean(heightv))**2)/N))
    
    # sample variance
    print("Sample variance     = {:.3f}".\
      format(np.sum((heightv-np.mean(heightv))**2)/(N-1)))
    
    print("Pop.   standard dev = {:.3f}".\
      format(np.sqrt(np.sum((heightv-np.mean(heightv))**2)/N)))
    
    print("Sample standard dev = {:.3f}". \
      format(np.sqrt(np.sum((heightv-np.mean(heightv))**2)/(N-1))))
    
    #Or simply
    print("Pop.   standard dev = {:.3f}".format(np.std(heightv)))
    

::

    Range = 11.600
    Population variance = 12.545
    Sample variance     = 12.757
    Pop.   standard dev = 3.542
    Sample standard dev = 3.572
    Pop.   standard dev = 3.542
    
    



Measures of relative standing
+++++++++++++++++++++++++++++

Percentiles are calculated as follows:


.. code-block:: python

    print("1 quartile (25th percentile) = {:.3f}".\
      format(st.scoreatpercentile(heightv, 25)))
    print("2 quartile (50th percentile) = {:.3f}".\
      format(st.scoreatpercentile(heightv, 50)))
    print("3 quartile (75th percentile) = {:.3f}".\
      format(st.scoreatpercentile(heightv, 75)))
    
    # Inter quartile rante: Q3-Q1 or P_75-P_25
    print("IQR = P75 - P25              = {:.3f}".\
      format(st.scoreatpercentile(heightv, 75)\
      -st.scoreatpercentile(heightv, 25)))
    

::

    1 quartile (25th percentile) = 3.950
    2 quartile (50th percentile) = 6.850
    3 quartile (75th percentile) = 9.650
    IQR = P75 - P25              = 5.700
    
    



Data aggregation or summary statistics by subgroup
++++++++++++++++++++++++++++++++++++++++++++++++++

We next want to summarize the data by subgroup. Let's assume that we would like
to compare the average ``Height`` by ``Race``. We can use the ``groupby()`` to
accomplish this.


.. code-block:: python

    groupRace = df.groupby('Race')
    print('Summary by Race', groupRace.mean())
    

::

    Summary by Race         Height  AverageMathSAT        Age    Female
    Education
    Race
    Blk   6.278261      491.652174  20.826087  0.304348   3.565217
    Hisp  6.525000      472.750000  21.500000  0.250000   1.750000
    Mex   6.171429      491.571429  20.928571  0.285714   2.428571
    Oth   8.750000      516.000000  21.000000  0.500000   2.500000
    Wht   7.917647      500.411765  20.941176  0.411765   3.117647
    
    



We could also use a different category like gender.


.. code-block:: python

    groupGender = df.groupby('Female')
    print('Summary by Gender', groupGender.mean())
    

::

    Summary by Gender         Height  AverageMathSAT   Age  Education
    Female
    0        4.765           479.7  21.4       2.95
    1       10.920           521.6  20.0       3.15
    
    



Or we can also combine the two or more categories and summarize the data as
follows.


.. code-block:: python

    groupRaceGender = df.groupby(['Race', 'Female'])
    print('Summary by Race and Gender', groupRaceGender.mean())
    

::

    Summary by Race and Gender                 Height  AverageMathSAT
    Age  Education
    Race Female
    Blk  0        4.350000      479.562500  21.1875   3.562500
         1       10.685714      519.285714  20.0000   3.571429
    Hisp 0        5.600000      461.333333  22.0000   1.666667
         1        9.300000      507.000000  20.0000   2.000000
    Mex  0        4.480000      481.900000  21.3000   2.100000
         1       10.400000      515.750000  20.0000   3.250000
    Oth  0        6.300000      506.000000  22.0000   1.000000
         1       11.200000      526.000000  20.0000   4.000000
    Wht  0        5.310000      480.600000  21.6000   3.400000
         1       11.642857      528.714286  20.0000   2.714286
    
    



Be careful and don't forget to add the various categories in a list using the
brackets ``[...]]``.

Measures of linear relationship
---------------------------------------------------------

Covariance
+++++++++++++++++

We first grab two variables ``heightv`` and ``agev`` from the dataframe and
define them as ``numpy`` arrays.


.. code-block:: python

    xv = df['Age'].values
    yv = df['Height'].values
    n = len(xv)
    



We then go ahead and calculate the covariance.


.. code-block:: python

    # Population covariance
    print("Population covariance = {:.3f}".\
      format(np.sum((xv-np.mean(xv))*(yv-np.mean(yv)))/n))
    
    # Sample covariance
    print("Sample covariance     = {:.3f}".\
      format(np.sum((xv-np.mean(xv))*(yv-np.mean(yv)))/(n-1)))
    
    # or simply
    print("Sample covariance     = {:.3f}".format(df.Age.cov(df.Height)))
    

::

    Population covariance = -0.159
    Sample covariance     = -0.162
    Sample covariance     = -0.162
    
    



Correlation coefficient
++++++++++++++++++++++++++++++

We can calculate the correlation coefficient by hand or using the ``pandas``
toolbox command.


.. code-block:: python

    print("Correlation coefficient =  {:.3f}".\
      format((np.sum((xv-np.mean(xv))*(yv-np.mean(yv)))/n)\
      /(np.std(xv)*np.std(yv))))
    
    print("Correlation coefficient =
    {:.3f}".format(df.Age.corr(df.Height)))
    

::

    Correlation coefficient =  -0.038
    Correlation coefficient = -0.038
    
    



Regression line
++++++++++++++++++++++

**Example 1: Simple example**

We first generate some data. A variable ``x`` and a variable ``y``.


.. code-block:: python

    # Define data. 2 vectors xv and yv
    xv = np.arange(1,9,1)
    yv = np.array([6,1,9,5,17,12,14,15])
    



We then "cast" these into a ``pandas`` DataFrame.


.. code-block:: python

    # Define data frame
    df = pd.DataFrame(np.array([xv, yv]).T, columns = ['x', 'y'])
    



We fist make a **scatterplot** with least squares trend line: `y = \beta_0 +
\beta_1 * x + \epsilon`.


.. code-block:: python

    p = np.polyfit(xv, yv, 1)
    print("p = {}".format(p))
    
    # Scatterplot
    fig, ax = plt.subplots()
    ax.set_title('Linear regression with polyfit()')
    ax.plot(xv, yv, 'o', label = 'data')
    ax.plot(xv, np.polyval(p,xv),'-', label = 'Linear regression')
    ax.legend(['Data', 'OLS'], loc='best')
    plt.show()
    

::

    p = [ 1.77380952  1.89285714]
    
    

.. image:: _static/Slides_Data_2_Scatter1_1.*
   :width: 12 cm



We then run the same regression using a more general method called ``ols``
which is part of ``pandas``. When calling the ``ols`` function
you need to add the module name (``pandas`` was imported as ``pd``)
in front of it: ``pd.ols()``. You then define the independent variable ``y``
and the dependent variables ``x's``.


.. code-block:: python

    # Run OLS regression
    res = pd.ols(y=df['y'], x=df['x'])
    
    # Show coefficient estimates
    print(res)
    print('Parameters:')
    print(res.beta)
    

::

    
    -------------------------Summary of Regression
    Analysis-------------------------
    
    Formula: Y ~ <x> + <intercept>
    
    Number of Observations:         8
    Number of Degrees of Freedom:   2
    
    R-squared:         0.6093
    Adj R-squared:     0.5442
    
    Rmse:              3.7578
    
    F-stat (1, 6):     9.3583, p-value:     0.0222
    
    Degrees of Freedom: model 1, resid 6
    
    -----------------------Summary of Estimated
    Coefficients------------------------
          Variable       Coef    Std Err     t-stat    p-value    CI 2.5%
    CI 97.5%
    --------------------------------------------------------------------------------
                 x     1.7738     0.5798       3.06     0.0222     0.6373
    2.9103
         intercept     1.8929     2.9281       0.65     0.5419    -3.8461
    7.6318
    ---------------------------------End of
    Summary---------------------------------
    
    Parameters:
    x            1.773810
    intercept    1.892857
    dtype: float64
    
    



Finally, we use the model to make a prediction of ``y`` when ``x = 8.5``.


.. code-block:: python

    # Make prediction for x = 8.5
    betas = res.beta
    print("Prediction of y for x = 8.5 is: {:.3f}".\
      format(np.sum(betas * np.array([1,8.5]))))
    

::

    Prediction of y for x = 8.5 is: 17.863
    
    



We can also use the results from the ``ols`` command to make a scatterplot with
the trendline through it.


.. code-block:: python

    fig, ax = plt.subplots()
    ax.set_title('Linear Regression with Fitted Values')
    ax.plot(xv, yv, 'o', label = 'data')
    ax.plot(xv, res.y_predict,'k-', label = 'Linear regression')
    ax.legend(['Data', 'OLS'], loc='best')
    plt.show()
    

.. image:: _static/Slides_Data_2_Scatter2_1.*
   :width: 12 cm



**Example 2: OLS with categorical (dummy variables)**


The next example is a bit more involved as we increase the number of
explanatory variables. In addition, some explanatory variables are categorical
variables. In order to use them in our OLS regression we first have to make so
called **dummy** variables (i.e. indicator variables that are either 0 or 1).


.. code-block:: python

    df = pd.read_csv(filepath + 'Lecture_Data_Excel_b.csv')
    
    dummies = pd.get_dummies(df['Race'], prefix = 'd')
    df = df.join(dummies)
    print(df.head())
    

::

       Height  Response  AverageMathSAT  Age  Female  Education  Race
    d_Blk  \
    0     1.1         3             370   20       0          2  Hisp
    0
    1     1.2         4             393   20       0          4   Wht
    0
    2     1.3         4             413   20       0          4   Blk
    1
    3     1.4         5             430   20       0          4   Wht
    0
    4     1.5         3             440   20       0          2   Mex
    0
    
       d_Hisp  d_Mex  d_Oth  d_Wht
    0       1      0      0      0
    1       0      0      0      1
    2       0      0      0      0
    3       0      0      0      1
    4       0      1      0      0
    
    



We next use ``ols`` again to run the regression. At the end we make a
prediction based on some values for the independent variables ``x1``, ``x2``,
etc.


.. code-block:: python

    res = pd.ols(y=df['Height'], x=df[['Age','Education','Female', \
        'd_Blk','d_Hisp','d_Mex','d_Wht']])
    
    # Show coefficient estimates
    print(res)
    
    print()
    print('Parameters:')
    print(res.beta)
    

::

    
    -------------------------Summary of Regression
    Analysis-------------------------
    
    Formula: Y ~ <Age> + <Education> + <Female> + <d_Blk> + <d_Hisp> +
    <d_Mex>
                 + <d_Wht> + <intercept>
    
    Number of Observations:         60
    Number of Degrees of Freedom:   8
    
    R-squared:         0.9437
    Adj R-squared:     0.9361
    
    Rmse:              0.9030
    
    F-stat (7, 52):   124.4348, p-value:     0.0000
    
    Degrees of Freedom: model 7, resid 52
    
    -----------------------Summary of Estimated
    Coefficients------------------------
          Variable       Coef    Std Err     t-stat    p-value    CI 2.5%
    CI 97.5%
    --------------------------------------------------------------------------------
               Age     1.8944     0.1241      15.27     0.0000     1.6512
    2.1375
         Education    -0.0514     0.1265      -0.41     0.6864    -0.2994
    0.1967
            Female     8.7359     0.3077      28.39     0.0000     8.1327
    9.3391
             d_Blk    -0.3784     0.6855      -0.55     0.5834    -1.7220
    0.9653
            d_Hisp    -1.0267     0.7901      -1.30     0.1995    -2.5753
    0.5218
    --------------------------------------------------------------------------------
             d_Mex    -0.5749     0.6863      -0.84     0.4060    -1.9200
    0.7701
             d_Wht     0.0816     0.6808       0.12     0.9050    -1.2528
    1.4161
         intercept   -35.2714     2.7510     -12.82     0.0000   -40.6634
    -29.8794
    ---------------------------------End of
    Summary---------------------------------
    
    
    Parameters:
    Age           1.894377
    Education    -0.051375
    Female        8.735906
    d_Blk        -0.378358
    d_Hisp       -1.026743
    d_Mex        -0.574949
    d_Wht         0.081628
    intercept   -35.271435
    dtype: float64
    
    




Prediction: size of 8.5 will produce a math score of ...


.. code-block:: python

    betas = res.beta
    print("Prediction: {}".\
      format(np.sum(betas * np.array([1,2.5,22,1,0,0,1,0]))))
    

::

    Prediction: 193.65914627657307
    
    



